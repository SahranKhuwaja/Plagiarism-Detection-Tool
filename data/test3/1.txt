
Figure 11.6: A DTD for movie stars
The components of an element E are generally other elements. They must 
appear between the tags <E> and < /E > in the order listed. However, there 
are several operators that control the number of times elements appear.
1. A * following an element means that the-element may occur any number 
of times, including zero times.
2. A + following an element means that the element may occur one or more 
times.
3. A ? following an element means that the element may occur either zero 
times or one time, but no more.
4. We can connect a list of options by the “or” symbol I to indicate that exactly one option appears. For example, if <Movie> elements had <Genre> 
subelements, we might declare these by
<!ELEMENT Genre (Comedy I Drama IS ciF i|T een)>
to indicate that each <Genre> element has one of these four subelements.
5. Parentheses can be used to group components. For example, if we declared 
addresses to have the form
<!ELEMENT Address S tr e e t, (C ity |Z ip )>
then <Address> elements would each have a <Street> subelement followed by either a <City> or <Zip> subelement, but not both.
2N ote th a t th e stars-and-m ovies X M L docum ent of Fig. 11.3 is n o t intended to conform 
to th is D T D .
498 CHAPTER 11. THE SEMISTRUCTURED-DATA MODEL
<Stars>
<Star>
<Name>Carrie Fisher</Name>
<Address>
<Street>123 Maple St.</Street>
<City>Hollywood</City>
</Address>
<Address>
<Street>5 Locust Ln.</Street>
<City>Malibu</City>
</Address>
<Movies>
<Movie>
<Title>Star Wars</Title>
<Year>1977</Year>
</Movie>
<Movie>
<Title>Empire Strikes Back</Title>
<Year>1980</Year>
</Hovie>
<Movie>
<Title>Return of the Jedi</Title>
<Year>1983</Year>
</Movie>
</Movies>
</Star>
<Star>
<Name>Mark Hamill</Name>
<Address>
<Street>456 Oak Rd.<Street>
<City>Brentwood</City>
</Address>
<Movies>
<Movie>
<Title>Star Wars</Title>
<Year>1977</Year>
</Movie>
<Movie>
<Title>Empire Strikes Back</Title>
<Year>1980</Year>
</Movie>
<Movie>
<Title>Return of the Jedi</Title>
<Year>1983</Year>
</Movie>
</Movies>
< / S t a r >
</Stars>
Figure 11.7: Example of a document following the DTD of Fig. 11.6
11.3. DOCUMENT TYPE DEFINITIONS 499
11.3.2 Using a DTD
If a document is intended to conform to a certain DTD, we can either:
a) Include the DTD itself as a preamble to the document, or
b) In the opening line, refer to the DTD, which must be stored separately 
in the file system accessible to the application that is processing the document.
E xam ple 11.9: Here is how we might introduce the document of Fig. 11.7 to 
assert that it is intended to conform to the DTD of Fig. 11.6.
<?xml v ersio n = "1.0" encoding = "u tf-8 " standalone = "no"?>
<!DOCTYPE S ta rs SYSTEM " sta r.d td ">
The attribute standalone = "no" says that a DTD is being used. Recall we 
set this attribute to "yes" when we did not wish to specify a DTD for the 
document. The location from which the DTD can be obtained is given in the 
! DOCTYPE clause, where the keyword SYSTEM followed by a file name gives this 
location. □
11.3.3 Attribute Lists
A DTD also lets us specify which attributes an element may have, and what 
the types of these attributes are. A declaration of the form
<!ATTLIST element-name attribute-name type >
says that the named attribute can be an attribute of the named element, and 
that the type of this attribute is the indicated type. Several attributes can be 
defined in one ATTLIST statement, but it is not necessary to do so, and the 
ATTLIST statements can appear in any position in the DTD.
The most common type for attributes is CDATA. This type is essentially 
character-string data with special characters like < escaped as in #PCDATA. Notice that CDATA does not take a pound sign as #PCDATA does. Another option 
is an enumerated type, which is a list of possible strings, surrounded by parentheses and separated by | ’s. Following the data type there can be a keyword 
#REQUIRED or #IMPLIED, which means that the attribute must be present, or is 
optional, respectively.
E xam ple 11.10: Instead of having the title and year be subelements of a 
<Movie> element, we could make these be attributes instead. Figure 11.8 shows 
possible attribute-list declarations. Notice that Movie is now an empty element. 
We have given it three attributes: t i t l e , year, and genre. The first two are 
CDATA, while the genre has values from an enumerated type. Note that in the 
document, the values, such as comedy, appear with quotes. Thus,
<Movie t i t l e = "S tar Wars" year = "1977" genre = "sc iF i" />
is a possible movie element in a document that conforms to this DTD. □
500 CHAPTER 11. THE SEMISTRUCTURED-DATA MODEL
<!ELEMENT Movie EMPTY>
<!ATTLIST Movie
t i t l e CDATA #REQUIRED 
year CDATA #REQUIRED
genre (comedy I drama I s c iF i I teen ) #IMPLIED
>
Figure 11.8: Data about movies will appear as attributes
<!D0CTYPE StarMovieData [
<!ELEMENT StarMovieData (S tar* , Movie*)>
<!ELEMENT S ta r (Name, Address+)>
<!ATTLIST S ta r
s ta r ld ID #REQUIRED 
s ta rr e d ln IDREFS #IMPLIED
>
<!ELEMENT Name (#PCDATA)>
<!ELEMENT Address (S tre e t, C ity)>
<!ELEMENT S tre e t (#PCDATA)>
<!ELEMENT C ity (#PCDATA)>
<!ELEMENT Movie ( T itle , Year)>
<!ATTLIST Movie
movield ID #REQUIRED 
starsO f IDREFS #IMPLIED
>
<!ELEMENT T itle (#PCDATA)>
<!ELEMENT Year (#PCDATA)>
]>
Figure 11.9: A DTD for stars and movies, using ID’s and IDREF’s
11.3.4 Identifiers and References
Recall from Section 11.2.5 that certain attributes can be used as identifiers for 
elements. In a DTD, we give these attributes the type ID. Other attributes 
have values that are references to these element ID’s; these attributes may be 
declared to have type IDREF. The value of an IDREF attribute must also be the 
value of some ID attribute of some element, so the IDREF is in effect a pointer 
to the ID. An alternative is to give an attribute the type IDREFS. In that case, 
the value of the attribute is a string consisting of a list of ID’s, separated by 
whitespace. The effect is that an IDREFS attribute links its element to a set of 
elements — the elements identified by the ID’s on the list.
11.3. DOCUMENT TYPE DEFINITIONS 501
E xam ple 11.11: Figure 11.9 shows a DTD in which stars and movies are 
given equal status, and the ID-IDREFS correspondence is used to describe the 
many-many relationship between movies and stars that was suggested in the 
semistructured data of Fig. 11.1. The structure differs from that of the DTD 
in Fig. 11.6, in that stars and movies have equal status; both are subelements 
of the root element. That is, the name of the root element for this DTD is 
StarMovieData, and its elements are a sequence of stars followed by a sequence 
of movies.
A star no longer has a set of movies as subelements, as was the case for the 
DTD of Fig. 11.6. Rather, its only subelements are a name and address, and in 
the beginning <Star> tag we shall find an attribute s ta rre d ln of type IDREFS, 
whose value is a list of ID’s for the movies of the star.
<? xml v ersio n = "1.0" encoding = "u tf-8 " standalone = "yes" ?> 
<StarMovieData>
<Star starlD = "cf" s ta rre d ln = "sw">
<Name>Carrie Fisher</Name>
<Address>
<Street>123 Maple S t.< /S tre e t> 
<City>Hollywood</City>
</Address>
<Address>
<Street>5 Locust L n.< /Street>
<City>Malibu</City>
</Address>
</Star>
<Star starID = "mh" s ta rre d ln = "sw">
<Name>Mark Hamill</Name>
<Address>
<Street>456 Oak R d.< /Street>
<City>Brentwood</City>
</Address>
</Star>
<Movie movielD = "sw" starsO f = "cf mh">
< T itle> S tar W ars</Title>
<Year>1977</Year>
</Movie>
</StarMovieData>
Figure 11.10: Adding stars-in information to our XML document
A <Star> element also has an attribute stair Id. Since it is declared to be 
of type ID, the value of s ta r ld may be referenced by <Movie> elements to 
indicate the stars of the movie. That is, when we look at the attribute list for 
Movie in Fig. 11.9, we see that it has an attribute movield of type ID; these
502 CHAPTER 11. THE SEMISTRUCTURED-DATA MODEL
are the ID’s that will appear on lists that are the values of starred ln . elements. 
Symmetrically, the attribute starsO f of Movie is an IDREFS, a list of ID’s for 
stars. □
11.3.5 Exercises for Section 11.3
E xercise 11.3.1: Add to the document of Fig. 11.10 the following facts:
a) Carrie Fisher and Mark Hamill also starred in The Empire Strikes Back
(1980) and Return of the Jedi (1983).
b) Harrison Ford also starred in Star Wars, in the two movies mentioned in
(a), and the movie Firewall (2006).
c) Carrie Fisher also starred in Hannah and Her Sisters (1985).
d) M att Damon starred in The Bourne Identity (2002).
E xercise 11.3.2: Suggest how typical data about banks and customers, as 
was described in Exercise 4.1.1, could be represented as a DTD.
E xercise 11.3.3: Suggest how typical data about players, teams, and fans, as 
was described in Exercise 4.1.3, could be represented as a DTD.
E xercise 11.3.4: Suggest how typical data about a genealogy, as was described in Exercise 4.1.6, could be represented as a DTD.
! E xercise 11.3.5: Using your representation from Exercise 11.2.2, devise an 
algorithm that will take any relation schema (a relation name and a list of 
attribute names) and produce a DTD describing a document that represents 
that relation.
11.4 XML Schema
XML Schema is an alternative way to provide a schema for XML documents. 
It is more powerful than DTD’s, giving the schema designer extra capabilities. 
For instance, XML Schema allows arbitrary restrictions on the number of occurrences of subelements. It allows us to declare types, such as integer or float, 
for simple elements, and it gives us the ability to declare keys and foreign keys.
11.4.1 The Form of an XML Schema
An XML Schema description of a schema is itself an XML document. It uses 
the namespace at the URL:
h t t p ://www.w3. org/2001/XMLSchema
11.4. XM L SCHEMA 503
that is provided by the World-Wide-Web Consortium. Each XML-Schema document thus has the form:
<? xml v ersio n = "1.0" encoding = "u tf-8 " ?>
<xs: schema xmlns:xs="h t t p ://www.w3. org/2001/XMLSchema">
< /x s: schema>
The first line indicates XML, and uses the special brackets <? amd ?>. The 
second line is the root tag for the document that is the schema. The attribute 
xmlns (XML namespace) makes the variable xs stand for the namespace for 
XML Schema that was mentioned above. It is this namespace that causes 
the tag <xs: schema> to be interpreted as schema in the namespace for XML 
Schema. As discussed in Section 11.2.6, qualifying each XML-Schema term we 
use with the prefix x s : will cause each such tag to be interpreted according 
to the rules for XML Schema. Between the opening <xs: schema> tag and its 
matched closing tag < /x s: schema> will appear a schema. In what follows, we 
shall learn the most important tags from the XML-Schema namespace and what 
they mean.
11.4.2 Elements
An important component of schemas is the element, which is similar to an 
element definition in a DTD. In the discussion that follows, you should be alert 
to the fact that, because XML-Schema definitions are XML documents, these 
schemas are themselves composed of “elements.” However, the elements of 
the schema itself, each of which has a tag that begins with x s :, are not the 
elements being defined by the schema.3 The form of an element definition in 
XML Schema is:
<xs: element name = element name type = element type >
constraints and/or structure information
</xs:elem ent>
The element name is the chosen tag for these elements in the schema being 
defined. The type can be either a simple type or a complex type. Simple 
types include the common primitive types, such as x s:in te g e r, x s :s trin g , 
and x s : boolean. There can be no subelements for an element of a simple type.
E xam ple 11.12: Here are title and year elements defined in XML Schema:
<xs:elem ent name = "T itle " type = " x s:s trin g " />
<xs:elem ent name = "Year" type = "x s:in te g e r" />
3To fu rth e r assist in th e distin ctio n betw een tag s th a t are p a rt of a schem a definition and 
th e tag s of th e schem a being defined, we shall begin each of th e la tte r w ith a cap ital letter.
504 CHAPTER 11. THE SEMISTRUCTURED-DATA MODEL
Each of these <xs: elem ent> elements is itself empty, so it can be closed by /> 
with no matched closing tag. The first defined element has name T itle and is 
of string type. The second element is named Year and is of type integer. In 
documents (perhaps talking about movies) with <Title> and <Year> elements, 
these elements will not be empty, but rather will be followed by a string (the 
title) or integer (the yeax), and a matched closing tag, < /T itle> or </Year>, 
respectively. □
11.4.3 Complex Types
A complex type in XML Schema can have several forms, but the most common is a sequence of elements. These elements are required to occur in the 
sequence given, but the number of repetitions of each element can be controlled 
by attributes minOccurs and maxOccurs, that appear in the element definitions 
themselves. The meanings of these attributes are as expected; no fewer than 
minOccurs occurrences of each element may appear in the sequence, and no 
more than maxOccurs occurrences may appeax. If there is more than one occurrence, they must all appear consecutively. The default, if one or both of 
these attributes are missing, is one occurrence. To say that there is no upper 
limit on occurrences, use the value "unbounded" for maxOccurs.
<xs: complexType name = type name >
<xs:sequence>
list of element definitions
< /x s: sequence>
< /x s: complexType>
Figure 11.11: Defining a complex type that is a sequence of elements
The form of a definition for a complex-type that is a sequence of elements is 
shown in Fig. 11.11. The name for the complex type is optional, but is needed if 
we are going to use this complex type as the type of one or more elements of the 
schema being defined. An alternative is to place the complex-type definition 
between an opening <xs:element> tag and its matched closing tag, to make 
that complex type be the type of the element.
E xam ple 11.13: Let us write a complete XML-Schema document that defines 
a very simple schema for movies. The root element for movie documents will 
be <Movies>, and the root will have zero or more <Movie> subelements. Each 
<Movie> element will have two subelements: a title and year, in that order. 
The XML-Schema document is shown in Fig. 11.12.
Lines (1) and (2) are a typical preamble to an XML-Schema definition. In 
lines (3) through (8), we define a complex type, whose name is movieType. 
This type consists of a sequence of two elements named T itle and Year; they 
are the elements we saw in Example 11.12. The type definition itself does not
11.4. XML SCHEMA 505
1) <? xml v ersio n = "1.0" encoding = "u tf-8 " ?>
2) <xs:schema xmlns:xs = "http://www.w3.org/2001/XMLSchema">
3) <xs: complexType name = "movieType">
4) <xs:sequence>
5) <xs:elem ent name = "T itle " type = " x s:strin g " />
6) <xs:elem ent name = "Year" type = "x s: in teg er" />
7) < /x s: sequence>
8) < /x s: complexType>
9) <xs:elem ent name = "Movies">
10) <xs:complexType>
11) <xs: sequence>
12) <xs:elem ent name = "Movie" type = "movieType"
minOccurs = "0" maxOccurs = "unbounded" />
13) < /x s: sequence>
14) </xs:complexType>
15) </xs:elem ent>
16) </xs:schema>
Figure 11.12: A schema for movies in XML Schema
create any elements, but notice how the name movieType is used in line (12) 
to make this type be the type of Movie elements.
Lines (9) through (15) define the element Movies. Although we could have 
created a complex type for this element, as we did for Movie, we have chosen to 
include the type in the element definition itself. Thus, we put no type attribute 
in line (9). Rather, between the opening <xs:element> tag at line (9) and 
its matched closing tag at line (15) appears a complex-type definition for the 
element Movies. This complex type has no name, but it is defined at line (11) 
to be a sequence. In this case, the sequence has only one kind of element, Movie, 
as indicated by line (12). This element is defined to have type movieType — 
the complex type we defined at lines (3) through (8). It is also defined to have 
between zero and infinity occurrences. Thus, the schema of Fig. 11.12 says the 
same thing as the DTD we show in Fig. 11.13. □
There are several other ways we can construct a complex type.
• In place of x s : sequence we could use x s : a ll, which means that each of 
the elements between the opening <xs: a ll> tag and its matched closing 
tag must occur, in any order, exactly once each.
• Alternatively, we could replace xs:sequence by xs:choice. Then, exactly one of the elements found between the opening <xs:choice> tag
506 CHAPTER 11. THE SEMISTRUCTURED-DATA MODEL
<!DOCTYPE Movies [
<!ELEMENT Movies (Movie*)>
<!ELEMENT Movie (T itle , Year)>
<!ELEMENT T itle (#PCDATA)>
<!ELEMENT Year (#PCDATA)>
]>
Figure 11.13: A DTD for movies
and its matched closing tag will appear.
The elements inside a sequence or choice can have minOccurs and maxOccurs 
attributes to govern how many times they can appear. In the case of a choice, 
only one of the elements can appear at all, but it can appear more than once if 
it has a value of maxOccurs greater than 1. The rules for x s : a l l are different. 
It is not permitted to have a maxOccurs value other than 1, but minOccurs can 
be either 0 or 1. In the former case, the element might not appear at all.
11.4.4 Attributes
A complex type can have attributes. That is, when we define a complex type 
T, we can include instances of element <xs: a t t r i b u t e d When we use T as 
the type of an element E, then E can have (or must have) an instance of this 
attribute. The form of an attribute definition is:
<xs: a ttr ib u te name = attribute name type = type name
other information about the attribute />
The “other information” may include information such as a default value and 
usage (required or optional — the latter is the default).
E xam ple 11.14: The notation
< x s :a ttrib u te name = "year" type = " x s:in te g e r" 
d e fa u lt = "0" />
defines year to be an attribute of type integer. We do not know of what 
element year is an attribute; it depends where the above definition is placed. 
The default value of year is 0, meaning that if an element without a value for 
attribute year occurs in a document, then the value of year is taken to be 0. 
As another instance:
<xs:a ttr ib u te name = "year" type = " x s:in te g e r" 
use = "required" />
11.4. XM L SCHEMA 507
is another definition of the attribute year. However, setting use to re q u ired 
means that any element of the type being defined must have a value for attribute 
year. □
Attribute definitions are placed within a complex-type definition. In the 
next example, we rework Example 11.13 by making the type movieType have 
attributes for the title and year, rather than subelements for that information.
1) <? xml v ersio n = "1.0" encoding = "u tf-8 " ?>
2) <xs:schema xmlns:xs = "http://www.w3.org/2001/XMLSchema">
3) <xs: complexType name = "movieType">
4) <xs:a ttr ib u te name = " t i t l e " type = " x s:s trin g "
use = "required" />
5) <xs: a ttr ib u te name = "year" type = "x s:in te g e r"
use = "required" />
6) < /x s: complexType>
7) <xs:elem ent name = "Movies">
8) <xs: complexType>
9) <xs: sequence>
10) <xs:elem ent name = "Movie" type = "movieType"
minOccurs = "0" maxOccurs = "unbounded" />
11) < /x s: sequence>
12) </xs:complexType>
13) </xs:elem ent>
14) </xs:schema>
Figure 11.14: Using attributes in place of simple elements
E xam ple 11.15: Figure 11.14 shows the revised XML Schema definition. At 
lines (4) and (5), the attributes t i t l e and year are defined to be required 
attributes for elements of type movieType. When element Movie is given that 
type at line (10), we know that every <Movie> element must have values for 
t i t l e and year. Figure 11.15 shows the DTD resembling Fig. 11.14. □
11.4.5 Restricted Simple Types
It is possible to create a restricted version of a simple type such as integer or 
string by limiting the values the type can take. These types can then be used as 
the type of an attribute or element. We shall consider two kinds of restrictions 
here:
508 CHAPTER 11. THE SEMISTRUCTURED-DATA MODEL
<!DOCTYPE Movies [
<!ELEMENT Movies (Movie*)>
<!ELEMENT Movie EMPTY>
<!ATTLIST Movie
t i t l e CDATA #REQUIRED 
year CDATA #REQUIRED
>
]>
Figure 11.15: DTD equivalent for Fig. 11.14
1. Restricting numerical values by using m inlnclusive to state the lower 
bound, m axlnclusive to state the upper bound.4
2. Restricting values to an enumerated type.
The form of a range restriction is shown in Fig. 11.16. The restriction has a 
base, which may be a primitive type (e.g., x s :s tr in g ) or another simple type.
<xs: simpleType name = type name >
< x s :r e s tr ic tio n base = base type >
upper and/or lower bounds
< /x s :re s tric tio n >
< /x s: simpleType>
Figure 11.16: Form of a range restriction
E xam ple 11.16: Suppose we want to restrict the year of a movie to be no 
earlier than 1915. Instead of using x s : in te g e r as the type for element Year in 
line (6) of Fig. 11.12 or for the attribute year in line (5) of Fig. 11.14, we could 
define a new simple type as in Fig. 11.17. The type movieYearType would then 
be used in place of x s : in te g e r in the two lines cited above. □
Our second way to restrict a simple type is to provide an enumeration of 
values. The form of a single enumerated value is:
<xs: enum eration value = some value />
A restriction can consist of any number of these values.
4T h e “inclusive” m eans th a t th e range of values includes th e given b ound. A n altern ativ e 
is to replace I n c lu s iv e by E x c lu siv e , m eaning th a t th e s ta te d b o unds are ju s t o utside th e 
p e rm itte d range.
11.4. XML SCHEMA 509
<xs:simpleType name = "movieYearType">
< x s :re s tric tio n base = "x s:in teg er">
< xs:m inlnclusive value = "1915" />
< /x s :re s tric tio n >
<xs:simpleType>
Figure 11.17: A type that restricts integer values to be 1915 or greater
E xam ple 11.17: Let us design a simple type suitable for the genre of movies. 
In our running example, we have supposed that there are only four possible 
genres: comedy, drama, sciFi, and teen. Figure 11.18 shows how to define 
a type genreType that could serve as the type for an element or attribute 
representing our genres of movies. □
<xs:simpleType name = "genreType">
< x s :re s tric tio n base = "x s:strin g ">
<xs:enum eration value = "comedy" />
<xs:enum eration value = "drama" />
<xs:enum eration value = "sc iF i" />
<xs:enum eration value = "teen" />
< /x s:r e s t r i c t i o n 
<xs: simpleType>
Figure 11.18: A enumerated type in XML Schema
11.4.6 Keys in XML Schema
An element can have a key declaration, which says that when we look at a 
certain class C of elements, values of one or more given fields within those 
elements are unique. The concept of “field” is actually quite general, but the 
most common case is for a field to be either a subelement or an attribute. 
The class C of elements is defined by a “selector.” Like fields, selectors can be 
complex, but the most common case is a sequence of one or more element names, 
each a subelement of the one before it. In terms of a tree of semistructured 
data, the class is all those nodes reachable from a given node by following a 
particular sequence of arc labels.
E xam ple 11.18: Suppose we want to say, about the semistructured data in 
Fig. 11.1, that among all the nodes we can reach from the root by following 
a star label, what we find following a further name label leads us to a unique 
value. Then the “selector” would be star and the “field” would be name. The 
implication of asserting this key is that within the root element shown, there
510 CHAPTER 11. THE SEMISTRUCTURED-DATA MODEL
cannot be two stars with the same name. If movies had names instead of titles, 
then the key assertion would not prevent a movie and a star from having the 
same name. Moreover, if there were actually many elements like the tree of 
Fig. 11.1 found in one document (e.g., each of the objects we called “Root” in 
that figure were actually a single movie and its stars), then different trees could 
have the same star name without violating the key constraint. □
The form of a key declaration is
<xs:key name = key name >
<xs: s e le c to r xpath = path description >
< x s :fie ld xpath = path description >
</xs:key>
There can be more than one line with an x s : f ie ld element, in case several fields 
are needed to form the key. An alternative is to use the element xs:unique in 
place of xs:key. The difference is that if “key” is used, then the fields must 
exist for each element defined by the selector. However, if “unique” is used, 
then they might not exist, and the constraint is only that they are unique if 
they exist.
The selector path can be any sequence of elements, each of which is a subelement of the previous. The element names are separated by slashes. The field 
can be any subelement of the last element on the selector path, or it can be 
an attribute of that element. If it is an attribute, then it is preceded by the 
“at-sign.” There are other options, and in fact, the selector and field can be 
any XPath expressions; we take up the XPath query language in Section 12.1.
E xam ple 11.19: In Fig. 11.19 we see an elaboration of Fig. 11.12. We have 
added the element Genre to the definition of movieType, in order to have a 
nonkey subelement for a movie. Lines (3) through (10) define genreType as in 
Example 11.17. The Genre subelement of movieType is added at line (15).
The definition of the Movies element has been changed in lines (24) through 
(28) by the addition of a key. The name of the key is movieKey; this name will 
be used if it is referenced by a foreign key, as we shall discuss in Section 11.4.7. 
Otherwise, the name is irrelevant. The selector path is just Movie, and there 
are two fields, T itle and Year. The meaning of this key declaration is that, 
within any Movies element, among all its Movie subelements, no two can have 
both the same title and the same year, nor can any of these values be missing. 
Note that because of the way movieType was defined at lines (13) and (14), 
with no values for minOccurs or maxOccurs for T itle or Year, the defaults, 1, 
apply, and there must be exactly one occurrence of each. □
11.4.7 Foreign Keys in XML Schema
We can also declare that an element has, perhaps deeply nested within it, a 
field or fields that serve as a reference to the key for some other element. This
11.4. XM L SCHEMA 511
1) <? xml v ersio n = "1.0" encoding = "u tf-8 " ?>
2) <xs:schema xm lns:xs = "http://www.w3.org/2001/XMLSchema">
3) <xs:simpleType name = "genreType">
4) <xs:r e s tr ic tio n base = "x s:strin g ">
5) <xs:enum eration value = "comedy" />
6) <xs:enum eration value = "drama" />
7) <xs:enum eration value = "sc iF i" />
8) <xs:enum eration value = "teen" />
9) < /x s :re s tric tio n >
10) <xs:simpleType>
11) <xs: complexType name = "movieType">
12) <xs:sequence>
13) <xs:elem ent name = " T itle " type = " x s:s trin g " />
14) <xs:elem ent name = "Year" type = " x s:in te g e r" />
15) <xs:elem ent name = "Genre" type = "genreType"
minOccurs = "0" maxOccurs = "1" />
16) </xs:sequence>
17) </xs:complexType>
18) <xs:elem ent name = "Movies">
19) <xs:complexType>
20) <xs: sequence>
21) <xs:elem ent name = "Movie" type = "movieType"
minOccurs = "0" maxOccurs = "unbounded" />
22) < /x s: sequence>
23) < /x s: complexType>
24) <xs:key name = "movieKey">
25) < x s:se le c to r xpath = "Movie" />
26) < x s :fie ld xpath = "T itle " />
27) < x s :fie ld xpath = "Year" />
28) </xs:key>
29) </xs:elem ent>
30) < /x s: schema>
Figure 11.19: A schema for movies in XML Schema
512 CHAPTER 11. THE SEMISTRUCTURED-DATA MODEL
capability is similar to what we get with ID’s and IDREF’s in a DTD (see 
Section 11.3.4). However, the latter are untyped references, while references in 
XML Schema are to particular types of elements. The form of a foreign-key 
definition in XML Schema is:
<xs:keyref name = foreign-key name r e f e r = key name >
<xs: s e le c to r xpath = path description >
<xs: f ie ld xpath = path description >
</xs:keyref>
The schema element is xs:keyref. The foreign-key itself has a name, and it 
refers to the name of some key or unique value. The selector and field(s) are as 
for keys.
E xam ple 11.20: Figure 11.20 shows the definition of an element <Stars>. 
We have used the style of XML Schema where each complex type is defined 
within the element that uses it. Thus, we see at lines (4) through (6) that a 
<Stars> element consists of one or more <Star> subelements.
At lines (7) through (11), we see that each <Star> element has three kinds 
of subelements. There is exactly one <Name> and one <Address> subelement, 
and any number of <StarredIn> subelements. In lines (12) through (15), we 
find that a <StarredIn> element has no subelements, but it does have two 
attributes, t i t l e and year.
Lines (22) through (26) define a foreign key. In line (22) we see that the 
name of this foreign-key constraint is movieRef and that it refers to the key 
movieKey that was defined in Fig. 11.19. Notice that this foreign key is defined 
within the <Stars> definition. The selector is S ta r/S ta rre d ln . That is, it says 
we should look at every <StarredIn> subelement of every <Star> subelement 
of a <Stars> element. From that <StarredIn> element, we extract the two 
fields t i t l e and year. The @ indicates that these axe attributes rather than 
subelements. The assertion made by this foreign-key constraint is that any 
title-year pair we find in this way will appear in some <Movie> element as the 
pair of values for its subelements <Title> and <Year>. □
11.4.8 Exercises for Section 11.4
E xercise 11.4.1: Give an example of a document that conforms to the XML 
Schema definition of Fig. 11.12 and an example of one that has all the elements 
mentioned, but does not conform to the definition.
Exercise 11.4.2: Rewrite Fig. 11.12 so that there is a named complex type 
for Movies, but no named type for Movie.
Exercise 11.4.3: Write the XML Schema definitions of Fig. 11.19 and 11.20 
as a DTD.
11.4. XM L SCHEMA 513
1) <? xml v ersio n = "1.0" encoding = "u tf-8 " ?>
2) <xs:schema xmlns:xs = "http://wwu.w3.org/2001/XMLSchema">
3) <xs:elem ent name = "Stars">
4) <xs: complexType>
5) <xs:sequence>
6) <xs:elem ent name = "Star" minOccurs = "1"
maxOccurs = "unbounded">
7) <xs: complexType>
8) <xs:sequence>
9) <xs:elem ent name = "Name"
type = " x s;s trin g " />
10) <xs:elem ent name = "Address"
type = " x s:s trin g " />
11) <xs:elem ent name = "S tarred ln "
minOccurs = "0" 
maxOccurs = "unbounded">
12) <xs:complexType>
13) <xs: a ttr ib u te name = " t i t l e "
type = " x s:s trin g " />
14) <xs:a ttr ib u te name = "year"
type = "x s:in te g e r" />
15) </xs:complexType>
16) </xs:elem ent>
17) </xs:sequence>
18) < /x s:complexType>
19) </xs:elem ent>
20) < /x s: sequence>
21) </xs:complexType>
22) <xs:keyref name = "movieRef" re f e r s = "movieKey">
23) < x s:se le c to r xpath = "S ta r/S ta rre d ln " />
24) < x s :fie ld xpath = " O title " />
25) < x s :fie ld xpath = "Syear" />
26) </xs:keyref>
27) </xs:elem ent>
Figure 11.20: Stars with a foreign key
514 CHAPTER 11. THE SEMISTRUCTURED-DATA MODEL
11.5 Summary of Chapter 11
♦ Semistructured Data: In this model, data is represented by a graph. 
Nodes are like objects or values of attributes, and labeled axes connect an 
object to both the values of its attributes and to other objects to which 
it is connected by a relationship.
♦ XML: The Extensible Markup Language is a World-Wide-Web Consortium standard that representes semistructured data linearly.
♦ XML Elements: Elements consist of an opening tag <Foo>, a matched 
closing tag </Foo>, and everything between them. What appears can be 
text, or it can be subelements, nested to any depth.
♦ XML Attributes: Tags can have attribute-value pairs within them. These 
attributes provide additional information about the element with which 
they are associated.
♦ Document Type Definitions: The DTD is a simple, grammatical form 
of defining elements and attributes of XML, thus providing a rudimentary schema for those XML documents that use the DTD. An element is 
defined to have a sequence of subelements, and these elements can be required to appear exactly once, at most once, at least once, or any number 
of times. An element can also be defined to have a list of required and/or 
optional attributes.
♦ Identifiers and References in D TD ’s: To represent graphs that are not 
trees, a DTD allows us to declare attributes of type ID and IDREF(S). An 
element can thus be given an identifier, and that identifier can be referred 
to by other elements from which we would like to establish a link.
♦ XML Schema: This notation is another way to define a schema for certain XML documents. XML Schema definitions are themselves written in 
XML, using a set of tags in a namespace that is provided by the WorldWide-Web Consortium.
♦ Simple Types in XML Schema: The usual sorts of primitive types, such as 
integers and strings, are provided. Additional simple types can be defined 
by restricting a simple type, such as by providing a range for values or by 
giving an enumeration of permitted values.
♦ Complex Types in XML Schema: Structured types for elements may be 
defined to be sequences of elements, each with a minimum and maximum 
number of occurrences. Attributes of an element may also be defined in 
its complex type.
♦ Keys and Foreign Keys in XML Schema: A set of elements and/or attributes may be defined to have a unique value within the scope of some
11.6. REFERENCES FOR CHAPTER 11 515
enclosing element. Other sets of elements and/or attributes may be defined to have a value that appears as a key within some other kind of 
element.
11.6 References for Chapter 11
Semistructured data as a data model was first studied in [5] and [4]. LOREL, 
the prototypical query language for this model is described in [3]. Surveys of 
work on semistructured data include [1], [7], and the book [2].
XML is a standard developed by the World-Wide-Web Consortium. The 
home page for information about XML is [9]. References on DTD’s and XML 
Schema are also found there. For XML parsers, the definition of DOM is in [8] 
and for SAX it is [6]. A useful place to go for quick tutorials on many of these 
subjects is [10].
1. S. Abiteboul, “Querying semi-structured data,” Proc. Intl. Conf. on Database Theory (1997), Lecture Notes in Computer Science 1187 (F. Afrati 
and P. Kolaitis, eds.), Springer-Verlag, Berlin, pp. 1-18.
2. S. Abiteboul, D. Suciu, and P. Buneman, Data on the Web: From Relations to Semistructured Data and XML, Morgan-Kaufmann, San Francisco, 1999.
3. S. Abiteboul, D. Quass, J. McHugh, J. Widom, and J. L. Weiner, “The 
LOREL query language for semistructured data,” In J. Digital Libraries
1:1, 1997.
4. P. Buneman, S. B. Davidson, and D. Suciu, “Programming constructs for 
unstructured data,” Proceedings of the Fifth International Workshop on
Database Programming Languages, Gubbio, Italy, Sept., 1995.
5. Y. Papakonstantinou, H. Garcia-Molina, and J. Widom, “Object exchange across heterogeneous information sources,” IEEE Intl. Conf. on
Data Engineering, pp. 251-260, March 1995.
6. Sax Project, h t t p : //www. s a x p ro je c t. org/
7. D. Suciu (ed.) Special issue on management of semistructured data, SIGMOD Record 26:4 (1997).
8. World-Wide-Web Consortium, http://www.w3.org/D0M/
9. World-Wide-Web Consortium, http://www.w3.org/XML/
10. W3 Schools, http://www.w3schools.com

Chapter 12
Programming Languages
for XML
We now turn to programming languages for semistructured data. All the widely 
used languages of this type apply to XML data, and might be used for semistructured data represented in other ways as well. In this chapter, we shall study 
three such languages. The first, XPath, is a simple language for describing sets 
of similar paths in a graph of semistructured data. XQuery is an extension 
of XPath that adopts something of the style of SQL. It allows iterations over 
sets, subqueries, and many other features that will be familiar from the study 
of SQL.
The third topic of this chapter is XSLT. This language was developed originally as a transformation language, capable of restructuring XML documents 
or turning them into printable (HTML) documents. However, its expressive 
power is actually quite similar to that of XQuery, and it is capable of producing 
XML results. Thus, it can serve as a query language for XML.
12.1 XPath
In this section, we introduce XPath. We begin with a discussion of the data 
model used in the most recent version of XPath, called XPath 2.0; this model 
is used in XQuery as well. This model plays a role analogous to the “bag of 
tuples of primitive-type components” that is used in the relational model as the 
value of a relation.
In later sections, we learn about XPath path expressions and their meaning. 
In general, these expressions allow us to move from elements of a document to 
some or all of their subelements. Using “axes,” we are also able to move within 
documents in a variety of ways, and to obtain the attributes of elements.
517
518 CHAPTER 12. PROGRAMMING LANGUAGES FOR XML
12.1.1 The XPath Data Model
As in the relational model, XPath assumes that all values — those it produces 
and those constructed in intermediate steps — have the same general “shape.” 
In the relational model, this “shape” is a bag of tuples. Tuples in a given 
bag all have the same number of components, and the components each have 
a primitive type, e.g., integer or string. In XPath, the analogous “shape” is 
sequence of items. An item is either:
1. A value of primitive type: integer, real, boolean, or string, for example.
2. A node. There are many kinds of nodes, but in our introduction, we shall 
only talk about three kinds:
(a) Documents. These are files containing an XML document, perhaps 
denoted by their local path name or a URL.
(b) Elements. These are XML elements, including their opening tags, 
their matched closing tag if there is one, and everything in between 
(i.e., below them in the tree of semistructured data that an XML 
document represents).
(c) Attributes. These are found inside opening tags, as we discussed in 
several places in Chapter 11.
The items in a sequence need not be all of the same type, although often they 
will be.
E xam ple 12.1: Figure 12.1 is a sequence of four items. The first is the integer 
10; the second is a string, and the third is a real. These are all items of primitive 
type.
10
"ten"
10.0
<Number base = "8">
<Digit>l</Digit>
<Digit>2</Digit>
</Number>
@val="10"
Figure 12.1: A sequence of five items
The fourth item is a node, and this node’s type is “element.” Notice that 
the element has tag Number with an attribute and two subelements with tag 
D igit. The last item is an attribute node. □
12.1. XPATH 519
1 2 .1.2 Document Nodes
While the documents to which XPath is applied can come from various sources, 
it is common to apply XPath to documents that are files. We can make a 
document node from a file by applying the function:
doc (file name)
The named file should be an XML document. We can name a file either by 
giving its local name or a URL if it is remote. Thus, examples of document 
nodes include:
doc("m ovies. xml")
doc( " /u s r/s a lly /d a ta /m o v ie s . xml")
doc(" in fo la b . S tan fo rd . edu/~hector/m ovies. xml")
Every XPath query refers to a document. In many cases, this document will be 
apparent from the context. For example, recall our discussion of XML-Schema 
keys in Section 11.4.6. We used XPath expressions to denote the selector and 
field(s) for a key. In that context, the document was “whatever document the 
schema definition is being applied to.”
12.1.3 Path Expressions
Typically, an XPath expression starts at the root of a document and gives a sequence of tags and slashes (/), say /7 i/T 2/ • • • /T n. We evaluate this expression 
by starting with a sequence of items consisting of one node: the document. We 
then process each of T i,T 2, ... in turn. To process Tj, consider the sequence 
of items that results from processing the previous tags, if any. Examine those 
items, in order, and find for each all its subelements whose tag is Tj. Those 
items are appended to the output sequence, in the order in which they appear 
in the document.
As a special case, the root tag T\ for the document is considered a “subelement” of the document node. Thus, the expression /T i produces a sequence 
of one item, which is an element node consisting of the entire contents of the 
document. The difference may appear subtle; before we applied the expression 
/Ti, we had a document node representing the file, and after applying /Ti to 
that node we have an element node representing the text in the file.
E xam ple 12.2: Suppose our document is a file containing the XML text 
of Fig. 11.5, which we reproduce here as Fig. 12.2. The path expression 
/StarM ovieData produces the sequence of one element. This element has tag 
<StarMovieData>, of course, and it consists of everything in Fig. 12.2 except 
for line (1).
Now, consider the path expression
/StarM ovieData/Star/Name
520 CHAPTER 12. PROGRAMMING LANGUAGES FOR XML
1) <? xml version= "1.0" encoding="utf-8" standalone="yes" ?>
2) <StarMovieData>
3) <Star starID = "cf" s ta rr e d ln = "sw">
4) <Name>Carrie Fisher</Name>
5) <Address>
6) <Street>123 Maple S t.< /S tre e t>
7) <City>Hollywood</City>
8) </Address>
9) <Address>
10) <Street>5 Locust L n.< /S treet>
11) <City>Malibu</City>
12) </Address>
13) </Star>
14) <Star sta rlD = "mh" s ta rr e d ln = "sw">
15) <Name>Mark Hamill</Name>
16) <Street>456 Oak R d.< /S treet>
17) <City>Brentwood</City>
18) </Star>
19) <Movie movielD = "sw" starsO f = " c f" , "mh">
20) < T itle> S tar W ars</Title>
21) <Year>1977</Year>
22) </Movie>
23) </StarMovieData>
Figure 12.2: An XML document for applying path expressions
When we apply the StarMovieData tag to the sequence consisting of the document, we get the sequence consisting of the root element, as discussed above. 
Next, we apply to this sequence the tag S tar. There are two subelements of 
the StarMovieData element that have tag S tar. These are lines (3) through 
(12) for star Carrie Fisher and lines (14) through (18) for star Mark Hamill. 
Thus, the result of the path expression /StarM ovieD ata/Star is the sequence 
of these two elements, in that order.
Finally, we apply to this sequence the tag Name. The first element has one 
Name subelement, at line (4). The second element also has one Name subelement, 
at line (15). Thus, the sequence
<Name>Carrie Fisher</Name>
<Name>Mark Hamill</Name>
is the result of applying the path expression /StarM ovieData/Star/Nam e to 
the document of Fig. 12.2. □
12.1. XPATH 521
12.1.4 Relative Path Expressions
In several contexts, we shall use XPath expressions that are relative to the 
current node or sequence of nodes.
• In Section 11.4.6 we talked about selector and field values that were really 
XPath expressions relative to a node or sequence of nodes for which we 
were defining a key.
• In Example 12.2 we talked about applying the XPath expression S ta r to 
the element consisting of the entire document, or the expression Name to 
a sequence of S ta r elements.
Relative expressions do not start with a slash. Each such expression must be 
applied in some context, which will be clear from its use. The similarity to the 
way files and directories are designated in a UNIX file system is not accidental.
12.1.5 Attributes in Path Expressions
Path expressions allow us to find all the elements within a document that are 
reached from the root along a particular kind of path (a sequence of tags). 
Sometimes, we want to find not these elements but rather the values of an 
attribute of those elements. If so, we can end the path expression by an attribute name preceded by an at-sign. That is, the path-expression form is 
/T 1/T 2/ - - - /T n/@A.
The result of this expression is computed by first applying the path expression /T i/T 2/ • • • /T n to get a sequence of elements. We then look at the opening 
tag of each element, in turn, to find an attribute A. If there is one, then the 
value of that attribute is appended to the sequence that forms the result.
E xam ple 12.3: The path expression
/StarM ovieD ata/Star/® starID
applied to the document of Fig. 12.2 finds the two S ta r elements and looks into 
their opening tags at lines (3) and (14) to find the values of their starlD attributes. Both elements have this attribute, so the result sequence is "cf " "mh".
□
12.1.6 Axes
So far, we have only navigated through semistructured-data graphs in two ways: 
from a node to its children or to an attribute. XPath in fact provides a large 
number of axes, which are modes of navigation. Two of these axes are child
(the default axis) and attribute, for which @ is really a shorthand. At each step 
in a path expression, we can prefix a tag or attribute name by an axis name 
and a double-colon. For example,
522 CHAPTER 12. PROGRAMMING LANGUAGES FOR XM L
/StarMovieData/Star/OstarlD
is really shorthand for:
/child::StarMovieData/child::Star/attribute::starID
Some of the other axes are parent, ancestor (really a proper ancestor), descendant (a proper descendant), next-sibling (any sibling to the right), previoussibling (any sibling to the left), self, and descendant-or-self. The latter has a 
shorthand / / and takes us from a sequence of elements to those elements and 
all their subelements, at any level of nesting.
E xam ple 12.4: It might look hard to find, in the document of Fig. 12.2, all 
the cities where stars live. The problem is that Mark Hamill’s city is not nested 
within an Address element, so it is not reached along the same paths as Carrie 
Fisher’s cities. However, the path expression
//City
finds all the C ity subelements, at any level of nesting, and returns them in the 
order in which they appear in the document. That is, the result of this path 
expression is the sequence:
<City>Hollywood</City>
<City>Malibu</City>
<City>Brentwood</City>
which we obtain from lines (7), (11), and (17), respectively.
We could also use the / / axis within the path expression. For example, 
should the document contain city information that wasn’t about stars (e.g., 
studios and their addresses), then we could restrict the paths that we consider 
to make sure that the city was a subelement of a S ta r element. For the given 
document, the path expression
/S tarM o v ieD ata/S tar//C ity
produces the same three C ity elements as a result. □
Some of the other axes have shorthands as well. For example, .. stands for 
parent, and . for self. We have already seen @ for attribute and / for child.
12.1.7 Context of Expressions
In order to understand the meaning of an axis like parent, we need to explore 
further the view of data in XPath. Results of expressions are sequences of 
elements or primitive values. However, XPath expressions and their results do 
not exist in isolation; if they did, it would not make sense to ask for the “parent” 
of an element. Rather, there is normally a context in which the expression is
12.1. XPATH 523
evaluated. In all our examples, there is a single document from which elements 
are extracted. If we think of an element in the result of some XPath expression 
as a reference to the element in the document, then it makes sense to apply 
axes like parent, ancestor, or next-sibling to the element in the sequence.
For example, we mentioned in Section 11.4.6 that keys in XML Schema are 
defined by a pair of XPath expressions. Key constraints apply to XML documents that obey the schema that includes the constraint. Each such document 
provides the context for the XPath expressions in the schema itself. Thus, it is 
permitted to use all the XPath axes in these expressions.
12.1.8 Wildcards
Instead of specifying a tag along every step of a path, we can use a * to say 
“any tag.” Likewise, instead of specifying an attribute, @* says “any attribute.”
E xam ple 12.5 : Consider the path expression
/StarMovieData/*/@*
applied to the document of Fig. 12.2. First, /StarM ovieData/* takes us to 
every subelement of the root element. There are three: two stars and a movie. 
Thus, the result of this path expression is the sequence of elements in lines (3) 
through (13), (14) through (18), and (19) through (22).
However, the expression asks for the values of all the attributes of these 
elements. We therefore look for attributes among the outermost tags of each 
of these elements, and return their values in the order in which they appear in 
the document. Thus, the sequence
"cf" "sw" "mh" "sw" "sw" "cf" "mh"
is the result of the XPath query.
A subtle point is that the value of the starsO f attribute in line (19) is itself 
a sequence of items — strings " c f" and "mh". XPath expands sequences that 
are part of other sequences, so all items are at the “top level,” as we showed 
above. That is, a sequence of items is not itself an item. □
12.1.9 Conditions in Path Expressions
As we evaluate a path expression, we can restrict ourselves to follow only a 
subset of the paths whose tags match the tags in the expression. To do so, we 
follow a tag by a condition, surrounded by square brackets. This condition can 
be anything that has a boolean value. Values can be compared by comparison 
operators such as = or >=. “Not equal” is represented as in C, by !=. A compound condition can be constructed by connecting comparisons with operators 
or or and.
The values compared can be path expressions, in which case we are comparing the sequences returned by the expressions. Comparisons have an implied
524 CHAPTER 12. PROGRAMMING LANGUAGES FOR XML
“there exists” sense; two sequences are related if any pair of items, one from 
each sequence, are related by the given comparison operator. An example 
should make this concept clear.
E xam ple 12.6: The following path expression:
/S tarM o v ieD ata/S tar[//C ity = "Malibu"]/Name
returns the names of the movie stars who have at least one home in Malibu. To 
begin, the path expression /StarM ovieD ata/Star returns a sequence of all the 
S ta r elements. For each of these elements, we need to evaluate the truth of the 
condition //C ity = "Malibu". Here, //C ity is a path expression, but it, like 
any path expression in a condition, is evaluated relative to the element to which 
the condition is applied. That is, we interpret the expression assuming that the 
element were the entire document to which the path expression is applied.
We start with the element for Carrie Fisher, lines (3) through (13) of 
Fig. 12.2. The expression //C ity causes us to look for all subelements, nested 
zero or more levels deep, that have a C ity tag. There are two, at lines (7) and 
(11). The result of the path expression //C ity applied to the Carrie-Fisher 
element is thus the sequence:
<City>Hollywood</City>
<City>Malibu</City>
Each item in this sequence is compared with the value "Malibu". An element 
whose type is a primitive value such as a string can be equated to that string, 
so the second item passes the test. As a result, the entire S ta r element of lines 
(3) through (13) satisfies the condition.
When we apply the condition to the second item, lines (14) through (18) 
for Mark Hamill, we find a C ity subelement, but its value does not match 
"Malibu" and this element fails the condition. Thus, only the Carrie-Fisher 
element is in the result of the path expression
/S tarM o v ieD ata/S tar[//C ity = "Malibu"]
We have still to finish the XPath query by applying to this sequence of 
one element the continuation of the path expression, /Name. At this stage, 
we search for a Name subelement of the Carrie-Fisher element and find it 
at line (4). Consequently, the query result is the sequence of one element, 
<Name>Carrie Fisher</Name>. □
Several other useful forms of condition are:
• An integer [j] by itself is true only when applied the ith child of its parent.
• A tag [T] by itself is true only for elements that have one or more subelements with tag T.
12.1. XPATH 525
• Similarly, an attribute [A] by itself is true only for elements that have a 
value for the attribute A.
Example 12.7: Figure 12.3 is a variant of our running movie example, in 
which we have grouped all the movies with a common title as one Movie element, 
with subelements that have tag Version. The title is an attribute of the movie, 
and the year is an attribute of the version. Versions have Star subelements. 
Consider the XPath query, applied to this document:
/Movies/Movie/Version[1]/@year
It asks for the year in which the first version of each movie was made, and the 
result is the sequence "1933" "1984".
1) <? xml version="1.0" encoding="utf-8" standalone="yes" ?>
2) <Movies>
3) <Movie t i t l e = "King Kong">
4) cV ersion year = "1933">
5) <Star>Fay Wray</Star>
6) </Version>
7) <Version year = "1976">
8) < S tar> Jeff B ridges</Star>
9) < S tar> Jessica Lange</Star>
10) </Version>
11) CVersion year = "2005" />
12) </Movie>
13) <Movie t i t l e = "Footloose">
14) <Version year = "1984">
15) <Star>Kevin Bacon</Star>
16) <Star>John Lithgow</Star>
17) <Star>Sarah J e s s ic a Parker</Star>
18) </Version>
19) </Movie>
20) </Movies>
Figure 12.3: An XML document for applying path expressions
In more detail, there are four Version elements that match the path
/Movies/Movie/Version
These are at lines (4) through (6), (7) through (10), line (11), and lines (14) 
through (18), respectively. Of these, the first and last are the first children of 
their respective parents. The year attributes for these versions are 1933 and 
1984, respectively. □
526 CHAPTER 12. PROGRAMMING LANGUAGES FOR XM L
E xam ple 12.8: The XPath query:
/M ovies/M ovie/V ersion[Star]
applied to the document of Fig. 12.3 returns three V ersion elements. The 
condition [S tar] is interpreted as “has at least one S ta r subelement.” That 
condition is true for the V ersion elements of lines (4) through (6), (7) through 
(10), and (14) through (18); it is false for the element of line (11). □
<Products>
<Maker name = "A">
<PC model = "1001" p ric e = "2114">
<Speed>2. 66</Speed>
<RAM>1024</RAM>
<HardDisk>250</HardDisk>
</PC>
<PC model = "1002" p ric e = "995">
<Speed>2. 10</Speed>
<RAM>512</RAM>
<HardDisk>250</HardDisk>
</PC>
<Laptop model = "2004" p ric e = "1150">
<Speed>2. 00</Speed>
<RAM>512</RAM>
<HardDisk>60</HardDisk>
<Screen>13. 3</Screen>
</Laptop>
<Laptop model = "2005" p ric e = "2500">
<Speed>2. 16</Speed>
<RAM>1024</RAM>
<HardD i sk>120</HardD i sk>
<Screen>17.0</Screen>
</Laptop>
</Maker>
Figure 12.4: XML document with product data — beginning
12.1.10 Exercises for Section 12.1
E xercise 12.1.1: Figures 12.4 and 12.5 are the beginning and end, respectively, of an XML document that contains some of the data from our running 
products exercise. Write the following XPath queries. What is the result of 
each?
12.1. XPATH 527
<Maker name = "E">
<PC model = "1011" p ric e = "959"> 
<Speed>l. 86</Speed> 
<RAM>2048</RAM> 
<HardDisk>160</HardDisk>
</PC>
<PC model = "1012" p ric e = "649"> 
<Speed>2. 80</Speed> 
<RAM>1024</RAM> 
<HardDisk>160</HardDisk>
</PC>
<Laptop model = "2001" p ric e = "3673"> 
<Speed>2. 00</Speed> 
<RAM>2048</RAM> 
<HardDisk>240</HardDisk> 
<Screen>20.1</Screen>
</Laptop>
< P rin ter model = "3002" p ric e = "239"> 
<Color>false</Color> 
<Type>laser</Type>
< /P rin ter>
<Maker name = "H">
< P rin ter model = "3006" p ric e = "100"> 
<Color>true</Color> 
<Type>ink-jet</Type>
< /P rin ter>
< P rin ter model = "3007" p ric e = "200"> 
<C olor>true</Color> 
<Type>laser</Type>
< /P rin ter>
</Maker>
</Products>
Figure 12.5: XML document with product data — end
528 CHAPTER 12. PROGRAMMING LANGUAGES FOR XM L
a) Find the amount of RAM on each PC.
b) Find the price of each product of any kind.
c) Find all the printer elements.
! d) Find the makers of laser printers.
! e) Find the makers of PC ’s and/or laptops.
f) Find the model numbers of PC ’s with a hard disk of at least 200 gigabytes. 
!! g) Find the makers of at least two PC’s.
E xercise 12.1.2: The document of Fig. 12.6 contains data similar to that 
used in our running battleships exercise. In this document, data about ships is 
nested within their class element, and information about battles appears inside 
each ship element. Write the following queries in XPath. W hat is the result of 
each?
a) Find the names of all ships.
b) Find all the C lass elements for classes with a displacement larger than 
35000.
c) Find all the Ship elements for ships that were launched before 1917.
d) Find the names of the ships that were sunk.
! e) Find the years in which ships having the same name as their class were 
launched.
! f) Find the names of all ships that were in battles.
!! g) Find the Ship elements for all ships that fought in two or more battles.
12.2 XQuery
XQuery is an extension of XPath that has become a standard for high-level 
querying of databases containing data in XML form. This section will introduce 
some of the important capabilities of XQuery.
12.2. XQUERY 529
<Ships>
<Class name = "Kongo" type = "be" country = "Japan"
numGuns = "8" bore = "14" displacem ent = "32000"> 
<Ship name = "Kongo" launched = "1913" />
<Ship name = "H iei" launched = "1914" />
<Ship name = "K irishim a" launched = "1915">
< B attle outcome = "sunk">Guadalcanal</Battle> 
</Ship>
<Ship name = "Haruna" launched = "1915" />
</Class>
<Class name = "North C arolina" type = "bb" country = "USA" 
numGuns = "9" bore = "16" displacem ent = "37000"> 
<Ship name = "North C arolina" launched = "1941" />
<Ship name = "Washington" launched = "1941">
<B attle outcome = "ok">Guadalcanal</Battle>
</Ship>
</Class>
<Class name = "Tennessee" type = "bb" country = "USA"
numGuns = "12" bore = "14" displacem ent = "32000"> 
<Ship name = "Tennessee" launched = "1920">
< B attle outcome = "ok">Surigao S tra it< /B a ttle > 
</Ship>
<Ship name = "C alifo rn ia" launched = "1921">
< B attle outcome = "ok">Surigao S tra it< /B a ttle >
</Class>
<Class name = "King George V" type = "bb" 
country = "Great B rita in "
numGuns = "10" bore = "14" displacem ent = "32000"> 
<Ship name = "King George V" launched = "1940" />
<Ship name = "Prince of Wales" launched = "1941">
< B attle outcome = "damaged">Denmark S tra it< /B a ttle > 
< B attle outcome = "sunk">M alaya</Battle>
</Ship>
<Ship name = "Duke of York" launched = "1941">
< B attle outcome = "ok">North Cape</Battle>
</Ship>
<Ship name = "Howe" launched = "1942" />
<Ship name = "Anson" launched = "1942" />
</Class>
</Ships>
Figure 12.6: XML document containing battleship data
530 CHAPTER 12. PROGRAMMING LANGUAGES FOR XM L
Case Sensitivity of XQuery
XQuery is case sensitive. Thus, keywords such as l e t or fo r need to be 
written in lower case, just like keywords in C or Java.
12.2.1 XQuery Basics
XQuery uses the same model for values that we introduced for XPath in Section 12.1.1. That is, all values produced by XQuery expressions are sequences 
of items. Items are either primitive values or nodes of various types, including 
elements, attributes, and documents. Elements in a sequence are assumed to 
exist in the context of some document, as discussed in Section 12.1.7.
XQuery is a functional language, which implies that any XQuery expression 
can be used in any place that an expression is expected. This property is a very 
strong one. SQL, for example, allows subqueries in many places; but SQL does 
not permit, for example, any subquery to be any operand of any comparison in 
a where-clause. The functional property is a double-edged sword. It requires 
every operator of XQuery to make sense when applied to lists of more than one 
item, leading to some unexpected consequences.
To start, every XPath expression is an XQuery expression. There is, however, much more to XQuery, including FLWR (pronounced “flower”) expressions, which are in some sense analogous to SQL select-from-where expressions.
12.2.2 FLWR Expressions
Beyond XPath expressions, the most important form of XQuery expression 
involves clauses of four types, called for-, let-, where-, and return- (FLWR) 
clauses.1 We shall introduce each type of clause in turn. However, we should 
be aware that there are options in the order and occurrences of these clauses.
1. The query begins with zero or more for- and let-clauses. There can be 
more than one of each kind, and they can be interlaced in any order, e.g., 
for, for, let, for, let.
2. Then comes an optional where-clause.
3. Finally, there is exactly one return-clause.
E xam ple 12.9: Perhaps the simplest FLWR expression is:
re tu rn <Greeting>Hello World</Greeting>
It examines no data, and produces a value that is a simple XML element. □
1 T h ere is also an order-by clause th a t we shall in tro d u ce in Section 12.2.10. For th a t 
reason, F L W R is a less com m on acronym for th e p rincipal form of X Q uery query th a n is 
FLW O R .
12.2. XQUERY 531
L et C lauses
The simple form of a let-clause is:
l e t variable := expression
The intent of this clause is that the expression is evaluated and assigned to 
the variable for the remainder of the FLWR expression. Variables in XQuery 
must begin with a dollar-sign. Notice that the assignment symbol is :=, not 
an equal sign (which is used, as in XPath, in comparisons). More generally, 
a comma-separated list of assignments to variables can appear where we have 
shown one.
E xam ple 12.10: One use of let-clauses is to assign a variable to refer to one 
of the documents whose data is used by the query. For example, if we want to 
query a document in file s ta r s. xml, we can start our query with:
l e t $ s ta rs := d o c("stars.x m l")
In what follows, the value of $ s ta rs is a single doc node. It can be used in front 
of an XPath expression, and that expression will apply to the XML document 
contained in the file s t a r s . xml. □
For C lauses
The simple form of a for-clause is:
fo r variable in expression
The intent is that the expression is evaluated. The result of any expression 
is a sequence of items. The variable is assigned to each item, in turn, and 
what follows this for-clause in the query is executed once for each value of the 
variable. You will not be much deceived if you draw an analogy between an 
XQuery for-clause and a C for-statement. More generally, several variables may 
be set ranging over different sequences of items in one for-clause.
E xam ple 12.11: We shall use the data suggested in Fig. 12.7 for a number of examples in this section. The data consists of two files, stairs.xm l in 
Fig. 12.7(a) and movies .xml in Fig. 12.7(b). Each of these files has data similar 
to what we used in Section 12.1, but the intent is that what is shown is just a 
small sample of the actual contents of these files.
Suppose we start a query:
l e t $movies := doc("movies.xml") 
fo r $m in $movies/Movies/Movie
. . . something done w ith each Movie element
532 CHAPTER 12. PROGRAMMING LANGUAGES FOR XM L
1) <? xml version= "1.0" encoding="utf-8" standalone="yes" ?>
2) <Stars>
3) <Star>
4) <Name>Carrie Fisher</Name>
5) <Address>
6) <Street>123 Maple S t.< /S tre e t>
7) <City>Hollywood</City>
8) </Address>
9) <Address>
10) <Street>5 Locust L n.< /S treet>
11) <City>Malibu</City>
12) </Address>
13) </Star>
. . . more s ta r s
14) < /S tars>
(a) Document sta rs.x m l
15) <? xml version= "1.0" encoding="utf-8" standalone="yes" ?>
16) <Movies>
17) <Movie t i t l e = "King Kong">
18) <Version year = "1933">
19) <Star>Fay Wray</Star>
20) </Version>
21) <Version year = "1976">
22) < S tar> Jeff B ridges</Star>
23) < S tar> Jessica Lange</Star>
24) </Version>
25) <Version year = "2005" />
26) </Movie>
27) <Movie t i t l e = "Footloose">
28) <Version year = "1984">
29) <Star>Kevin Bacon</Star>
30) <Star>John Lithgow</Star>
31) <Star>Sarah J e s s ic a P arker< /Star>
32) </Version>
33) </Movie>
. . . more movies
34) </Movies>
(b) Document m ovies. xml
Figure 12.7: Data for XQuery examples
12.2. XQUERY 533
Boolean Values in XQuery
A comparision like $x = 10 evaluates to true or false (strictly speaking, 
to one of the names x s :tru e or x s :fa ls e from the namespace for XML 
Schema). However, several other types of expressions can be interpreted as 
true or false, and so can serve as the value of a condition in a where-clause. 
The important coercions to remember are:
1. If the value is a sequence of items, then the empty sequence is interpreted as false and nonempty sequences as true.
2. Among numbers, 0 and NaN (“not a number,” in essence an infinite 
number) are false, and other numbers are true.
3. Among strings, the empty string is false and other strings are true.
Notice that $movies/Movies/Movie is an XPath expression that tells us to start 
with the document in file m ovies. xml, then go to the root Movies element, and 
then form the sequence of all Movie subelements. The body of the “for-loop” 
will be executed first with $m equal to the element of lines (17) through (26) 
of Fig. 12.7, then with $m equal to the element of lines (27) through (33), and 
then with each of the remaining Movie elements in the document. □
T he W here C lause
The form of a where-clause is:
where condition
This clause is applied to an item, and the condition, which is an expression, 
evaluates to true or false. If the value is true, then the return-clause is applied to 
the current values of any vaxiables in the query. Otherwise, nothing is produced 
for the current values of variables.
T he R etu rn C lause
The form of this clause is:
re tu rn expression
The result of a FLWR expression, like that of any expression in XQuery, is 
a sequence of items. The sequence of items produced by the expression in 
the return-clause is appended to the sequence of items produced so far. Note 
that although there is only one return-clause, this clause may be executed many
534 CHAPTER 12. PROGRAMMING LANGUAGES FOR XM L
times inside “for-loops,” so the result of the query may be constructed in stages. 
We should not think of the return-clause as a “return-statement,” since it does 
not end processing of the query.
E xam ple 12.12: Let us complete the query we started in Example 12.11 by 
asking for a list of all the star elements found among the versions of all movies. 
The query is:
l e t $movies := docC'movies.xml") 
fo r $m in $movies/Movies/Movie 
re tu rn $m /V ersion/Star
The first value of $min the “for-loop” is the element of lines (17) through (26) 
of Fig. 12.7. From that Movie element, the XPath expression /V e rsio n /S tar 
produces a sequence of the three S ta r elements at lines (19), (22), and (23). 
That sequence begins the result of the query.
<Star>Fay Wray</Star>
< S tar> Jeff B ridges</Star>
< S tar> Jessica Lange</Star>
<Star>Kevin Bacon</Star>
<Star>John Lithgow</Star>
<Star>Sarah J e s s ic a Parker< /Star>
Figure 12.8: Beginning of the result sequence for the query of Example 12.12
The next value of $m is the element of lines (27) through (33). Now, the 
result of the expression in the return-clause is the sequence of elements in lines 
(29), (30), and (31). Thus the beginning of the result sequence looks like that 
in Fig. 12.8. □
12.2.3 Replacement of Variables by Their Values
Let us consider a modification to the query of Example 12.12. Here, we want to 
produce not just a sequence of <Star> elements, but rather a sequence of Movie 
elements, each containing all the stars of movies with a given title, regardless 
of which version they starred in. The title will be an attribute of the Movie 
element.
Figure 12.9 shows an attem pt that seems right, but in fact is not correct.
The expression we return for each value of $m seems to be an opening <Movie> 
tag followed by the sequence of S ta r elements for that movie, and finally a 
closing </Movie> tag. The <Movie> tag has a t i t l e attribute that is a copy of 
the same attribute from the Movie element in file movies.xml. However, when 
we execute this program, what appears is:
12.2. XQUERY 535
Sequences of Sequences
We should remind the reader that sequences of items can have no internal 
structure. Thus, in Fig. 12.8, there is no separator between Jessica Lange 
and Kevin Bacon, or any grouping of the first three stars and the last 
three, even though these groups were produced by different executions of 
the return-clause.
l e t $movies := doc("movies.xml") 
fo r $m in $movies/Movies/Movie
re tu rn <Movie t i t l e = $m /Stitle>$m /Version/Star</M ovie>
Figure 12.9: Erroneous attempt to produce Movie elements
<Movie t i t l e = "$m/Qtitle">$m/Version/Star</M ovie>
<Movie t i t l e = "$m/@ title">$m/Version/Star</M ovie>
The problem is that, between tags, or as the value of an attribute, any text 
string is permissible. This return statement looks no different, to the XQuery 
processor, than the return of Example 12.9, where we really were producing text 
inside matching tags. In order to get text interpreted as XQuery expressions 
inside tags, we need to surround the text by curly braces.
l e t $movies := doc("movies.xml") 
fo r $m in $movies/Movies/Movie
re tu rn <Movie t i t l e = {$m/@title}>{$m/Version/Star}</Movie>
Figure 12.10: Adding curly braces fixes the problem
The proper way to meet our goal is shown in Fig. 12.10. In this query, 
the expressions $ m /title and $m /V ersion/Star inside the braces are properly 
interpreted as XPath expressions. The first is replaced by a text string, and 
the second is replaced by a sequence of S ta r elements, as intended.
E xam ple 12.13: This example not only further illustrates the use of curly 
braces to force interpretation of expressions, but also emphasizes how any 
XQuery expression can be used wherever an expression of any kind is permitted. Our goal is to duplicate the result of Example 12.12, where we got a 
sequence of S ta r elements, but to make the entire sequence of stars be within 
a S ta rs element. We cannot use the trick of Fig. 12.10 with S ta rs in place 
of S tar, because that would place many S tars tags around separate groups of 
stars.
536 CHAPTER 12. PROGRAMMING LANGUAGES FOR XM L
l e t $starS eq := (
l e t $movies := doc("m ovies.xml") 
fo r $m in $movies/Movies/Movie 
re tu rn $m /V ersion/Star
)
re tu rn <Stars>{$starSeq}</Stars>
Figure 12.11: Putting tags around a sequence
Figure 12.11 does the job. We assign the sequence of S ta r elements that 
results from the query of Example 12.12 to a local variable $starSeq. We then 
return that sequence, surrounded by tags, being careful to enclose the variable 
in braces so it is evaluated and not treated literally. □
12.2.4 Joins in XQuery
We can join two or more documents in XQuery in much the same way as we 
join two or more relations in SQL. In each case we need variables, each of 
which ranges over elements of one of the documents or tuples of one of the 
relations, respectively. In SQL, we use a from-clause to introduce the needed 
tuple variables (which may just be the table name itself); in XQuery we use a 
for-clause.
However, we must be very careful how we do comparisons in a join. First, 
there is the m atter of comparison operators such as = or < operating on sequences with the meaning of “there exist elements that compare” as discussed 
in Section 12.1.9. We shall take up this point again in Section 12.2.5. Additionally, equality of elements is by “element identity” (analogous to “object 
identity”). That is, an element is not equal to a different element, even if it 
looks the same, character-by-character. Fortunately, we usually do not want to 
compare elements, but really the primitive values such as strings and integers 
that appear as values of their attributes and subelements. The comparison 
operators work as expected on primitive values; < is “precedes in lexicographic 
order” for strings.
There is a built-in function data(E ) that extracts the value of an element 
E. We can use this function to extract the text from an element that is a string 
with matching tags.
E xam ple 12.14: Suppose we want to find the cities in which stars mentioned 
in the movies.xml file of Fig. 12.7(b) live. We need to consult the sta rs.x m l 
file of Fig. 12.7(a) to get that information. Thus, we set up a variable ranging 
over the S ta r elements of m ovies. xml and another variable ranging over the 
S ta r elements of s ta r s .xml. When the data in a S ta r element of movies .xml 
matches the data in the Name subelement of a S ta r element of sta irs. xml, then 
we have a match, and we extract the C ity element of the latter.
12.2. XQUERY 537
Figure 12.12 shows a solution. The let-clause introduces variables to stand 
for the two documents. As before, this shorthand is not necessary, and we could 
have used the document nodes themselves in the XPath expressions of the next 
two lines. The for-clause introduces a doubly nested loop. Variable $ sl ranges 
over each S ta r element of movies.xml and $s2 does the same for stars.x m l.
l e t $movies := doc("m ovies.xm l"),
$ s ta rs := d o c("stars.x m l") 
fo r $ sl in $m ovies/M ovies/M ovie/V ersion/Star,
$s2 in $ s ta rs /S ta rs /S ta r 
where d a ta ($ sl) = data($s2/Name) 
re tu rn $s2/A ddress/C ity
Figure 12.12: Finding the cities of stars
The where-clause uses the built-in function d ata to extract the strings that 
are the values of the elements $ sl and $s2. Finally, the return-clause produces 
a C ity element. □
12.2.5 XQuery Comparison Operators
We shall now consider another puzzle where things don’t quite work as expected. 
Our goal is to find the stars in s t a r s . xml of Fig. 12.7(a) that live at 123 Maple 
St., Malibu. Our first attempt is in Fig. 12.13.
l e t $ s ta rs := d o c("stars.x m l") 
fo r $s in $ s ta rs /S ta rs /S ta r 
where $ s/A d d ress/S treet = "123 Maple S t." and 
$s/A ddress/C ity = "Malibu" 
re tu rn $s/Name
Figure 12.13: An erroneous attempt to find who lives at 123 Maple St., Malibu
In the where-clause, we compare S tre e t elements and C ity elements with 
strings, but that works as expected, because an element whose value is a string 
is coerced to that string, and the comparison will succeed when expected. The 
problem is seen when $s takes the S ta r element of lines (3) through (13) of 
Fig. 12.7 as its value. Then, XPath expression $ s/A d d ress/S treet produces 
the sequence of two elements of lines (6) and (10) as its value. Since the = 
operator returns true if any pair of items, one from each side, equate, the value 
of the first condition is true; line (6), after coercion, is equal to the string 
"123 Maple S t." . Similarly, the second condition compares the list of two 
C ity elements of lines (7) and (11) with the string "Malibu", and equality is 
found for line (11). As a result, the Name element for Carrie Fisher [line (4)] is 
returned.
538 CHAPTER 12. PROGRAMMING LANGUAGES FOR XML
But Carrie Fisher doesn’t live at 123 Maple St., Malibu. She lives at 123 
Maple St., Hollywood, and elsewhere in Malibu. The existential nature of 
comparisons has caused us to fail to notice that we were getting a street and 
city from different addresses.
XQuery provides a set of comparison operators that only compare sequences 
consisting of a single item, and fail if either operand is a sequence of more than 
one item. These operators are two-letter abbreviations for the comparisons: eq, 
ne, I t , g t, le , and ge. We could use eq in place of = to catch the case where 
we are actually comparing a string with several streets or cities. The revised 
query is shown in Fig. 12.14.
l e t $ s ta rs := doc ("stairs, xml") 
fo r $s in $ s ta r s /S ta rs /S ta r
where $ s/A d d ress/S tree t eq "123 Maple S t." and 
$ s/A ddress/C ity eq "Malibu" 
re tu rn $s/Name
Figure 12.14: A second erroneous attem pt to find who lives at 123 Maple St., 
Malibu
This query does not allow the Carrie-Fisher element to pass the test of the 
where-clause, because the left sides of the eq operator are not single items, and 
therefore the comparison fails. Unfortunately, it will not report any star with 
two or more addresses, even if one of those addresses is 123 Maple St., Malibu. 
Writing a correct query is tricky, regardless of which version of the comparison 
operators we use, and we leave a correct query as an exercise.
12.2.6 Elimination of Duplicates
XQuery allows us to eliminate duplicates in sequences of any kind, by applying 
the built-in function d is tin c t-v a lu e s . There is a subtlely that must be noted, 
however. Strictly speaking, distinct-values applies to primitive types. It will 
strip the tags from an element that is a tagged text-string, but it won’t put 
them back. Thus, the input to d is tin c t- v a lu e s can be a list of elements and 
the result a list of strings.
E xam ple 12.15: Figure 12.11 gathered all the S ta r elements from all the 
movies and returned them as a sequence. However, a star that appeared 
in several movies would appear several times in the sequence. By applying 
d is tin c t-v a lu e s to the result of the subquery that becomes the value of variable $ starse q , we can eliminate all but one copy of each S ta r element. The 
new query is shown in Fig. 12.15.
Notice, however, that what is produced is a list of the names of the stars 
surrounded by the S ta rs tags, as:
<Stars>"Fay Wray" " Je ff B ridges" </S tars>
12.2. XQUERY 539
l e t $starS eq := d is tin c t- v a lu e s (
l e t $movies := doc("movies.xml") 
fo r $m in $movies/Movies/Movie 
re tu rn $m /V ersion/Star
)
re tu rn <Stars>{$starSeq}</Stars>
Figure 12.15: Eliminating duplicate stars
In comparison, the version in Fig. 12.11 produced
<Stars><Star>Fay Wray</Star> <S tar> Jeff B ridges</Star> • • •
</Stars>
but might produce duplicates. □
12.2.7 Quantification in XQuery
There are expressions that say, in effect, “for all” and “there exists.” Their 
forms, respectively, are:
every variable in expressionl s a t i s f i e s expressions
some variable in expressionl s a t i s f i e s expressions
Here, expressionl produces a sequence of items, and the variable takes on each 
item, in turn, as its value. For each such value, expressions (which normally 
involves the variable) is evaluated, and should produce a boolean value.
In the “every” version, the result of the entire expression is false if some item 
produced by expressionl makes expressions false; the result is true otherwise. 
In the “some” version, the result of the entire expression is true if some item 
produced by expressionl makes expressions true; the result is false otherwise.
l e t $ s ta rs := docO 'stars.xm l") 
fo r $s in $ s ta rs /S ta rs /S ta r 
where every $c in $s/A ddress/C ity s a tis f ie s 
$c = "Hollywood" 
re tu rn $s/Name
Figure 12.16: Finding the stars who only live in Hollywood
E xam ple 12.16: Using the data in the file s ta r s , xml of Fig. 12.7(a), we want 
to find those stars who live in Hollywood and nowhere else. That is, no matter 
how many addresses they have, they all have city Hollywood. Figure 12.16 
shows how to write this query. Notice that $s/A ddress/C ity produces the
540 CHAPTER 12. PROGRAMMING LANGUAGES FOR XML
sequence of C ity elements of the star $s. The where-clause is thus satisfied if 
and only if every element on that list is <City>Hollywood</City>.
Incidentally, we could change the “every” to “some” and find the stars that 
have at least one home in Hollywood. However, it is rarely necessary to use the 
“some” version, since most tests in XQuery are existentially quantified anyway. 
For instance,
l e t $ s ta rs := d o c("stars.x m l") 
fo r $s in $ s ta r s /S ta r s /S ta r 
where $s/A ddress/C ity = "Hollywood" 
re tu rn $s/Name
produces the stars with a home in Hollywood, without using a “some” expression. Recall our discussion in Section 12.2.5 of how a comparision such as =, 
with a sequence of more than one item on either or both sides, is true if we can 
match any items from the two sides. □
12.2.8 Aggregations
XQuery provides built-in functions to compute the usual aggregations such as 
count, sum, or max. They take any sequence as argument; that is, they can be 
applied to the result of any XQuery expression.
l e t $movies := doc("m ovies.xml") 
fo r $m in $movies/Movies/Movie 
where count($m /Version) > 1 
re tu rn $m
Figure 12.17: Finding the movies with multiple versions
E xam ple 12.17: Let us examine the data in file movies.xml of Fig. 12.7(b) 
and produce those Movie elements that have more than one version. Figure
12.17 does the job. The XPath expression $m/Version produces the sequence 
of V ersion elements for the movie $m. The number of items in the sequence is 
counted. If that count exceeds 1, the where-clause is satisfied, and the movie 
element $m is appended to the result. □
12.2.9 Branching in XQuery Expressions
There is an if-then-else expression in XQuery of the form
i f ( expressionl) then expression2 e ls e expressions
To evaluate this expression, first evaluate expressionl; if it is true, evaluate 
expression2, which becomes the result of the whole expression. If expressionl
is false, the result of the whole expression is expressions.
12.2. XQUERY 541
This expression is not a statement — there are no statements in XQuery, 
only expressions. Thus, the analog in C is the ?: expression, not the if-then-else 
statement. Like the expression in C, there is no way to omit the “else” part. 
However, we can use as expressions the empty sequence, which is denoted (). 
This choice makes the conditional expression produce the empty sequence when 
the test-condition is not satisfied.
E xam ple 12.18: Our goal in this example is to produce each of the versions 
of King Kong, tagging the most recent version L atest and the earlier versions 
Old. In line (1), we set variable $kk to be the Movie element for King Kong. 
Notice that we have used an XPath condition in this line, to make sure that we 
produce only that one element. Of course, if there were several Movie elements 
that had the title King Kong, then all of them would be on the sequence of items 
that is the value of $kk, and the query would make no sense. However, we are 
assuming title is a key for movies in this structure, since we have explicitly 
grouped versions of movies with the same title.
1) l e t $kk :=
doc("m ovies.xm l")/M ovies/M ovie[@ title = "King Kong"]
2) fo r $v in $kk/Version
3) re tu rn
4) i f ($v/@year = max($kk/Version/@year))
5) then <Latest>{$v}</Latest>
6) e ls e <01d>{$v}</01d>
Figure 12.18: Tagging the versions of King Kong
Line (2) causes $v to iterate over all versions of King Kong. For each such 
version, we return one of two elements. To tell which, we evaluate the condition 
of line (4). On the right of the equal-sign is the maximum year of any of the 
King-Kong versions, and on the left is the year of the version $v. If they are 
equal, then $v is the latest version, and we produce the element of line (5). If 
not, then $v is an old version, and we produce the element of line (6). □
12.2.10 Ordering the Result of a Query
It is possible to sort the results as part of a FLWR query, if we add an orderclause before the return-clause. In fact, the query form we have been concentrating on here is usually called FLWOR (but still pronounced “flower”), to 
acknowledge the optional presence of an order-clause. The form of this clause 
is:
order list of expressions
The sort is based on the value of the first expression, ties are broken by the 
value of the second expression, and so on. The default order is ascending, but 
the keyword descending following an expression reverses the order.
542 CHAPTER 12. PROGRAMMING LANGUAGES FOR XM L
What happens when an order is present is analogous to what happens in 
SQL. Just before we reach the stage in query processing where the output is 
assembled (the SELECT clause in SQL; the return-clause in XQuery), the result 
of previous clauses is assembled and sorted. In the case of SQL, the intermediate 
result is a set of bindings of tuples to the tuple variables that range over each of 
the relations in the FROM clause. Specifically, it is all those bindings that pass 
the test of the WHERE clause.
In XQuery, we should think of the intermediate result as a sequence of 
bindings of variables to values. The variables are those defined in the for- and 
let-clauses that precede the order-clause, and the sequence consists of all those 
bindings that pass the test of the where-clause. These bindings are each used to 
evaluate the expressions in the order-clause, and the values of those expressions 
govern the position of the binding in the order of all the bindings. Once we 
have the order of bindings, we use them, in turn, to evaluate the expression in 
the return-clause.
E xam ple 12.19: Let us consider all versions of all movies, order them by year, 
and produce a sequence of Movie elements with the title and year as attributes. 
The data comes from file movies.xml in Fig. 12.7(b), as usual. The query is 
shown in Fig. 12.19.
l e t $movies := doc("m ovies.xml") 
fo r $m in $movies/Movies/Movie,
$v in $m/Version 
order $v/@year
re tu rn <Movie t i t l e = "{$m /© title}" year = "{$v/@year}" />
Figure 12.19: Construct the sequence of title-year pairs, ordered by year
When we reach the order-clause, bindings provide values for the three variables $movies, $m, and $v. The value doc ("movies .xml) is bound to $movies 
in every one of these bindings. However, the values of $m and $v vary; for each 
pair consisting of a movie and a version of that movie, there will be one binding 
for the two variables. For instance, the first such binding associates with $m the 
element in lines (17) through (26) of Fig. 12.7(b) and associates with $v the 
element of lines (18) through (20).
The bindings are sorted according to the value of attribute year in the 
element to which $v is bound. There may be many movies with the same year, 
and the ordering does not specify how these are to be ordered. As a result, all 
we know is that the movie-version pairs with a given year will appear together 
in some order, and the groups for each year will be in the ascending order of 
year. If we wanted to specify a total ordering of the bindings, we could, for 
example, add a second term to the list in the order-clause, such as:
o rder $v/®year, $m /@ title
to break ties alphabetically by title.
After sorting the bindings, each binding is passed to the return-clause, in 
the order chosen. By substituting for the variables in the return-clause, we 
produce from each binding a single Movie element. □
12.2.11 Exercises for Section 12.2
E xercise 1 2 .2 .1 : Using the product data from Figs. 12.4 and 12.5, write the 
following in XQuery.
a) Find the P rin te r elements with a price less than 100.
b) Find the P rin te r elements with a price less than 100, and produce the 
sequence of these elements surrounded by a tag <CheapPrinters>.
! c) Find the names of the makers of both printers and laptops.
! d) Find the names of the makers that produce at least two PC’s with a speed 
of 3.00 or more.
! e) Find the makers such that every PC they produce has a price no more 
than 1000.
!! f) Produce a sequence of elements of the form
<LaptopXModel>a;</Model><Maker>2/</Maker></Laptop>
where x is the model number and y is the name of the maker of the laptop.
E xercise 12.2.2: Using the battleships data of Fig. 12.6, write the following 
in XQuery.
a) Find the names of the classes that had at least 10 guns.
b) Find the names of the ships that had at least 10 guns.
c) Find the names of the ships that were sunk.
d) Find the names of the classes with at least 3 ships.
! e) Find the names of the classes such that no ship of that class was in a 
battle.
!! f) Find the names of the classes that had at least two ships launched in the 
same year.
!! g) Produce a sequence of items of the form
< B attle name = arXShip name = y />• • • </B attle>
12.2. XQUERY 543
544 CHAPTER 12. PROGRAMMING LANGUAGES FOR XM L
where x is the name of a battle and y the name of a ship in the battle. 
There may be more than one Ship element in the sequence.
! Exercise 12.2.3: Solve the problem of Section 12.2.5; write a query that finds 
the star(s) living at a given address, even if they have several addresses, without 
finding stars that do not live at that address.
! Exercise 12.2.4: Do there exist expressions E and F such that the expression every $x in E s a t i s f i e s F is true, but some $x in E s a t i s f i e s F
is false? Either give an example or explain why it is impossible.
12.3 Extensible Stylesheet Language
XSLT (Extensible Stylesheet Language for Transformations) is a standard of 
the World-Wide-Web Consortium. Its original purpose was to allow XML documents to be transformed into HTML or similar forms that allowed the document to be viewed or printed. However, in practice, XSLT is another query 
language for XML. Like XPath or XQuery, we can use XSLT to extract data 
from documents or turn one document form into another form.
12.3.1 XSLT Basics
Like XML Schema, XSLT specifications are XML documents; these specifications are usually called stylesheets. The tags used in XSLT are found in a 
namespace, which is http://www.w3.org/1999/XSL/Transform . Thus, at the 
highest level, a stylesheet looks like Fig. 12.20.
<? xml v ersio n = "1.0" encoding = " u tf-8 " ?>
< x sl: s ty le s h e e t xm lns:xsl =
"h t t p : / / www.w3. org/1999/XSL/Transf orm">
< /x s l:sty lesh eet>
Figure 12.20: The form of an XSLT stylesheet
12.3.2 Templates
A stylesheet will have one or more templates. To apply a stylesheet to an XML 
document, we go down the list of templates until we find one that matches 
the root. As processing proceeds, we often need to find matching templates 
for elements nested within the document. If so, we again search the list of 
templates for a match according to matching rules that we shall learn in this 
section. The simplest form of a template tag is:
12.3. EXTENSIBLE STYLESHEET LANGUAGE 545
< x sl:tem plate match = "XPath expression">
The XPath expression, which can be either rooted (beginning with a slash) or 
relative, describes the elements of an XML document to which this template is 
applied. If the expression is rooted, then the template is applied to every element of the document that matches the path. Relative expressions are applied 
when a template T has within it a tag <xsl:apply-tem plates> . In that case, 
we look among the children of the elements to which T is applied. In that way, 
we can traverse an XML document’s tree in a depth-first manner, performing 
complicated transformations on the document.
The simplest content of a template is text, typically HTML. When a template matches a document, the text inside that document is produced as output. 
Within the text can be calls to apply templates to the children and/or obtain 
values from the document itself, e.g., from attributes of the current element.
1) <? xml v ersio n = "1.0" encoding = "u tf-8 " ?>
2) < x sl:sty le sh e e t xm lns:xsl =
3) "h t t p : //www.w 3 . org/1999/XSL/Transform">
4) < x sl:tem p late match = "/">
5) <HTML>
6) <B0DY>
7) <B>This is a document</b>
8) </body>
9) </html>
10) < /xsl:tem plate>
11) < /x sl:sty le sh e e t>
Figure 12.21: Printing output for any document
E xam ple 12.20: In Fig. 12.21 is an exceedingly simple stylesheet. It applies 
to any document and produces the same HTML document, regardless of its 
input. This HTML document says “This is a document” in boldface.
Line (4) introduces the one template in the stylesheet. The value of the 
match attribute is "/", which matches only the root. The body of the template, 
lines (5) through (9), is simple HTML. When these lines are produced as output, 
the resulting file can be treated as HTML and displayed by a browser or other 
HTML processor. □
12.3.3 Obtaining Values From XML Data
It is unusual that the document we produce does not depend in any way on the 
input to the transformation, as was the case in Example 12.20. The simplest 
way to extract data from the input is with the value-of tag. The form of this 
tag is:
546 CHAPTER 12. PROGRAMMING LANGUAGES FOR XM L
<? xml version="1.0" encoding="utf-8" standalone="yes" ?> 
<Movies>
<Movie title = "King Kong">
<Version year = "1933">
<Star>Fay Wray</Star>
</Version>
<Version year = "1976">
<Star>Jeff Bridges</Star>
<Star>Jessica Lange</Star>
</Version>
CVersion year = "2005" />
</Movie>
<Movie title = "Footloose">
CVersion year = "1984">
<Star>Kevin Bacon</Star>
<Star>John Lith.gow</Star>
<Star>Sarah Jessica Parker</Star>
</Version>
</Movie>
... more movies
</Movies>
Figure 12.22: The file m ovies. xml
<xsl:value-of select = "expression" />
The expression is an XPath expression that should produce a string as value. 
Other values, such as elements containing text, are coerced into strings in the 
obvious way.
E xam ple 12.21: In Fig. 12.22 we reproduce the file movies.xml that was 
used in Section 12.2 as a running example. In this example of a stylesheet, we 
shall use v alu e-o f to obtain all the titles of movies and print them, one to a 
line. The stylesheet is shown in Fig. 12.23.
At line (4), we see that the template matches every Movie element, so we 
process them one at a time. Line (5) applies the v alu e-o f operation with an 
XPath expression O title . That is, we go to the t i t l e attribute of each Movie 
element and take the value of that attribute. This value is produced as output, 
and followed at line (6) by the HTML break tag, so the next movie title will be 
printed on the next line. □
12.3.4 Recursive Use of Templates
The most interesting and powerful transformations require recursive application 
of templates at various elements of the input. Having selected a template to
12.3. EXTENSIBLE STYLESH EET LANGUAGE 547
1) <? xml version = "1.0" encoding = "utf-8" ?>
2) <xsl:stylesheet xmlns:xsl =
3) "http://www.w3.org/1999/XSL/Transform">
4) <xsl:template match = "/Movies/Movie">
5) <xsl:value-of select = "Stitle" />
6) <BR/>
7) </xsl:template>
8) </xsl:stylesheet>
Figure 12.23: Printing the titles of movies
apply to the root of the input document, we can ask that a template be applied 
to each of its subelements, by using the apply-tem plates tag. If we want to 
apply a certain template to only some subset of the subelements, e.g., those 
with a certain tag, we can use a s e le c t expression, as:
< x sl:ap p ly -tem p lates s e le c t = "expression" />
When we encounter such a tag within a template, we find the set of matching 
subelements of the current element (the element to which the template is being 
applied). For each subelement, we find the first template that matches and 
apply it to the subelement.
E xam ple 12.22: In this example, we shall use XSLT to transform an XML 
document into another XML document, rather than into an HTML document. 
Let us examine Fig. 12.24. There are four templates, and together they process 
movie data in the form of Fig. 12.22. The first template, lines (4) through (8), 
matches the root. It says to output the text <Movies> and then apply templates 
to the children of the root element. We could have specified that templates were 
to be applied only to children that are tagged <Movie>, but since we expect no 
other tags among the children, we did not specify:
6) < x sl:ap p ly -tem p lates s e le c t = "Movie" />
Notice that after applying templates to the <Movie> children (which will 
result in the printing of many elements), we close the <Movies> element in the 
output with the appropriate closing tag at line (7). Also observe that we can tell 
the difference between tags that are output text, such as lines (5) and (7), from 
tags that are XSLT, because all XSLT tags must be from the x s l namespace.
Now, let us see what applying templates to the <Movie> elements does. The 
first (and only) template that matches these elements is the second, at lines (9) 
through (15). This template begins by outputting the text <Movie t i t l e = " 
at line (10). Then, line (11) obtains the title of the movie and emits it to the 
output. Line (12) finishes the quoted attribute value and the <Movie> tag in 
the output. Line (13) applies templates to all the children of the movie, which 
should be versions. Finally, line (14) emits the matching </Movie> ending tag.
548 CHAPTER 12. PROGRAMMING LANGUAGES FOR XML
1) <? xml version = "1.0" encoding = "utf-8" ?>
2) <xsl:stylesheet xmlns:xsl =
3) "http://www.w3.org/1999/XSL/Transform">
4) <xsl:template match = "/Movies">
5) <Movies>
6) <xsl:apply-templates />
7) </Movies>
8) </xsl:template>
9) <xsl:template match = "Movie">
10) <Movie title = "
11) <xsl:value-of select = "fititle” />
12) ">
13) <xsl:apply-templates />
14) </Movie>
15) </xsl:template>
16) <xsl:template match = "Version">
17) <xsl:apply-templates />
18) </xsl:template>
19) <xsl:template match = "Star">
20) <Star name = "
21) <xsl:value-of select = "." />
22) " />
23) </xsl:template>
24) </xsl:stylesheet>
Figure 12.24: Transforming the m ovies. xml file
When line (13) calls for templates to be applied to all the versions of a 
movie, the only matching template is that of lines (16) through (18), which 
does nothing but apply templates to the children of the version, which should 
be <Star> elements. Thus, what gets generated between each opening <Movie> 
tag and its matched closing tag is determined by the last template of lines (19) 
through (23). This template is applied to each <Star> element.
Star elements from the input are transformed in the output. Instead of the 
star’s name being text, as it is in Fig. 12.22, the template starting at line (19) 
produces a <Star> element with the name as an attribute. Line (21) says to 
select the <Star> element itself (the dot represents the .“self” axis as an XPath 
expression) as a value for the output. However, all output is text, so the tags 
of the element are not part of the output. That result is exactly what we want,
12.3. EXTENSIBLE STYLESH EET LANGUAGE 549
since the value of the attribute name should be a string, not an element. The 
empty <Star> element is completed on line (22). For instance, given the input 
of Fig. 12.22, the output would be as shown in Fig. 12.25. □
<Movies>
<Movie t i t l e = "King Kong">
<Star name = "Fay Wray" />
<Star name = " Je ff B ridges" />
<Star name = " Je ssic a Lange" /> 
</Movie>
<Movie t i t l e = "Footloose">
<Star name = "Kevin Bacon" />
<Star name = "John Lithgow" />
<Star name = "Sarah J e s s ic a Parker" /> 
</Movie>
. . . more movies
</Movies>
Figure 12.25: Output of the transform of Fig. 12.24
12.3.5 Iteration in XSLT
We can put a loop within a template that gives us freedom over the order in 
which we visit certain subelements of the element to which the template is being 
applied. The f or-each tag creates the loop, with a form:
< x sl:fo r-e a ch s e le c t = "expression">
The expression is an XPath expression whose value is a sequence of items. 
Whatever is between the opening <for-each> tag and its matched closing tag 
is executed for each item, in turn.
E xam ple 12.23: In Fig. 12.26 is a copy of our document s ta r s .xml; we wish 
to transform it to an HTML list of all the names of stars followed by an HTML 
list of all the cities in which stars live. Figure 12.27 has a template that does 
the job.
There is one template, which matches the root. The first thing that happens 
is at line (5), where the HTML tag <0L> is emitted to start an ordered list. 
Then, line (6) starts a loop, which iterates over each <Star> subelement. At 
lines (7) through (9), a list item with the name of that star is emitted. Line (11) 
ends the list of names and begins a list of cities. The second loop, lines (12) 
through (16), runs through each <Address> element and emits a list item for 
the city. Line (17) closes the second list. □
550 CHAPTER 12. PROGRAMMING LANGUAGES FOR XM L
<? xml versions"1.0" encoding="utf-8" standalone="yes" ?> 
<Stars>
<Star>
<Name>Carrie Fisher</Name>
<Address>
<Street>123 Maple St.</Street> 
<City>Hollywood</City>
</Address>
<Address>
<Street>5 Locust Ln.</Street> 
<City>Malibu</City>
</Address>
</Star>
... more stars
</Stars>
Figure 12.26: Document stars.x m l
1) <? xml version = "1.0" encoding = "utf-8" ?>
2) <xsl:stylesheet xmlns:xsl =
3) "http://www.w 3 .org/1999/XSL/Transform">
4) <xsl:template match = "/">
5) <0L>
6) <xsl:for-each select = "Stars/Star" />
7) <LI>
8) <xsl:value-of select = "Name">
9) </li>
10) </xsl:for-each>
11) </ol><P/xOL>
12) <xsl:for-each select = "Stars/Star/Addre
13) <LI>
14) <xsl:value-of select = "City">
15) </li>
16) </xsl:for-each>
17) </ol>
18) </xsl:template>
19) </xsl:stylesheet>
Figure 12.27: Printing names and cities of stars
12.3. EXTENSIBLE STYLESHEET LANGUAGE 551
12.3.6 Conditionals in XSLT
We can introduce branching into our templates by using an i f tag. The form 
of this tag is:
<xsl:if test = "boolean expression">
Whatever appears between this tag and its matched closing tag is executed if 
and only if the boolean expression is true. There is no else-clause, but we can 
follow this expression by another i f that has the opposite test condition should 
we wish.
1) <? xml version = "1.0" encoding = "utf-8" ?>
2) <xsl:stylesheet xmlns:xsl =
3) "http://www.w3.org/1999/XSL/Transform">
4) <xsl:template match = "/">
5) <TABLE border = "5"><TR><TH>Stars</thx/tr>
6) <xsl:for-each select = "Stars/Star" />
7) <xsl:if test = "Address/City = ’Hollywood’">
8) <TRXTD>
9) <xsl:value-of select = "Name" />
10) </tdx/tr>
11) </xsl:if>
12) </xsl:for-each>
13) </table>
14) </xsl:template>
15) </xsl:stylesheet>
Figure 12.28: Finding the names of the stars who live in Hollywood
E xam ple 12.24: Figure 12.28 is a stylesheet that prints a one-column table, 
with header “Stars.” There is one template, which matches the root. The first 
thing this template does is print the header row at line (5). The for-each loop 
of lines (6) through (12) iterates over each star. The conditional of line (7) 
tests whether the star has at least one home in Hollywood. Remember that 
the equal-sign represents a comparison is true if any item on the left equals any 
item on the right. That is what we want, since we asked whether any of the 
homes a star has is in Hollywood. Lines (8) through (10) print a row of the 
table. □
12.3.7 Exercises for Section 12.3
E xercise 12.3.1: Suppose our input XML document has the form of the product data of Figs. 12.4 and 12.5. Write XSLT stylesheets to produce each of the 
following documents.
552 CHAPTER 12. PROGRAMMING LANGUAGES FOR XM L
a) An HTML file consisting of a header “Manufacturers” followed by an 
enumerated list of the names of all the makers of products listed in the 
input.
b) An HTML file consisting of a table with headers “Model” and “Price,” 
with a row for each PC. That row should have the proper model and price 
for the PC.
! c) An HTML file consisting of a table whose headers are “Model,” “Price,” 
“Speed,” and “Ram” for all Laptops, followed by another table with the 
same headers for PC ’s.
d) An XML file with root tag <PCs> and subelements having tag <PC>. This 
tag has attributes model, p ric e , speed, and ram. In the output, there 
should be one <PC> element for each <PC> element of the input file, and 
the values of the attributes should be taken from the corresponding input 
element.
!! e) An XML file with root tag <Products> whose subelements are <Product> 
elements. Each <Product> element has attributes type, maker, model, 
and p ric e , where the type is one of "PC", "Laptop", or " P rin te r". There 
should be one <Product> element in the output for every PC, laptop, 
and printer in the input file, and the output values should be chosen 
appropriately from the input data.
! f) Repeat part (b), but make the output file a Latex file.
E xercise 12.3.2: Suppose our input XML document has the form of the product data of Fig. 12.6. Write XSLT stylesheets to produce each of the following
documents.
a) An HTML file with a header for each class. Under each header is a table 
with column-headers “Name” and “Launched” with the appropriate entry 
for each ship of the class.
b) An HTML file with root tag <Losers> and subelements <Ship>, each of 
whose values is the name of one of the ships that were sunk.
! c) An XML file with root tag <Ships> and subelements <Ship> for each 
ship. These elements each should have attributes name, c la ss, country 
and numGuns with the appropriate values taken from the input file.
! d) Repeat (c), but only list those ships that were in at least one battle.
e) An XML file identical to the input, except that <B attle> elements should 
be empty, with the outcome and name of the battle as two attributes.
12.4. SUMMARY OF CHAPTER 12 553
12.4 Summary of Chapter 12
♦ XPath: This language is a simple way to express many queries about XML 
data. You describe paths from the root of the document by sequences of 
tags. The path may end at an attribute rather than an element.
♦ The XPath Data Model: All XPath values are sequences of items. An 
item is either a primitive value or an element. An element is an opening 
XML tag, its matched closing tag, and everything in between.
♦ Axes: Instead of proceeding down the tree in a path, one can follow 
another axis, including jumps to any descendant, a parent, or a sibling.
♦ XPath Conditions: Any step in a path can be constrained by a condition, 
which is a boolean-valued expression. This expression appears in square 
brackets.
♦ XQuery: This language is a more advanced form of query language for 
XML documents. It uses the same data model as XPath. XQuery is a 
functional language.
♦ FLWR Expressions: Many queries in XQuery consist of let-, for-, whereand return-clauses. “Let” introduces temporary definitions of variables; 
“for” creates loops; “where” supplies conditions to be tested, and “return” 
defines the result of the query.
♦ Comparison Operators in XQuery and XPath: The conventional comparison operators such as < apply to sequences of items, and have a “thereexists” meaning. They are true if the stated relation holds between any 
pair of items, one from each of the lists. To be assured that single items 
are being compared, we can use letter codes for the operators, such as I t 
for “less than.”
♦ Other XQuery Expressions: XQuery has many operations that resemble 
those in SQL. These operators include existential and universal quantification, aggregation, duplicate-elimination, and sorting of results.
♦ XSLT: This language is designed for transformations of XML documents, 
although it also can be used as a query language. A “program” in this 
language has the form of an XML document, with a special namespace 
that allows us to use tags to describe a transformation.
♦ Templates: The heart of XSLT is a template, which matches certain elements of the input document. The template describes output text, and 
can extract values from the input document for inclusion in the output. 
A template can also call for templates to be applied recursively to the 
children of an element.
554 CHAPTER 12. PROGRAMMING LANGUAGES FOR XML
♦ XSLT Programming Constructs: A template can also include XSLT constructs that behave like an iterative programming language. These constructs include for-loops and if-statements.
12.5 References for Chapter 12
The World-Wide-Web Consortium site for the definition of XPath is [2], The 
site for XQuery is [3], and for XSLT it is [4].
[1] is an introduction to the XQuery language. There are tutorials for XPath, 
XQuery, and XSLT at [5].
1. D. D. Chamberlin, “XQuery: an XML Query Language,” IBM Systems
Journal 41:4 (2002), pp. 597-615. See also
www.research.ibm.com/j ournal/s j/4 1 4 /chamberlin.pdf
2. World-Wide-Web Consortium h t t p : //www. w3. org/TR/xpath
3. World-Wide-Web Consortium http://www.w3.org/TR/xquery
4. World-Wide-Web Consortium h t t p : / / www. w3. org/T R /xslt
5. W3 Schools, http://www.w3schools.com
Part IV
Database System
Implementation
555

Chapter 13
Secondary Storage
Management
Database systems always involve secondary storage — the disks and other devices that store large amounts of data that persists over time. This chapter 
summarizes what we need to know about how a typical computer system manages storage. We review the memory hierarchy of devices with progressively 
slower access but larger capacity. We examine disks in particular and see how 
the speed of data access is affected by how we organize our data on the disk. 
We also study mechanisms for making disks more reliable.
Then, we turn to how data is represented. We discuss the way tuples of a 
relation or similar records or objects are stored. Efficiency, as always, is the 
key issue. We cover ways to find records quickly, and how to manage insertions 
and deletions of records, as well as records whose sizes grow and shrink.
13.1 The Memory Hierarchy
We begin this section by examining the memory hierarchy of a computer system. 
We then focus on disks, by far the most common device at the “secondarystorage” level of the hierarchy. We give the rough parameters that determine 
the speed of access and look at the transfer of data from disks to the lower 
levels of the memory hierarchy.
13.1.1 The Memory Hierarchy
A typical computer system has several different components in which data may 
be stored. These components have data capacities ranging over at least seven 
orders of magnitude and also have access speeds ranging over seven or more 
orders of magnitude. The cost per byte of these components also varies, but 
more slowly, with perhaps three orders of magnitude between the cheapest and
557
558 CHAPTER 13. SECONDARY STO RAG E M ANAGEM ENT
most expensive forms of storage. Not surprisingly, the devices with smallest 
capacity also offer the fastest access speed and have the highest cost per byte. 
A schematic of the memory hierarchy is shown in Fig. 13.1.
DBMS
Nonvolatile
t
Volatile
I
Figure 13.1: The memory hierarchy
Here are brief descriptions of the levels, from the lowest, or fastest-smallest 
level, up.
1. Cache. A typical machine has a megabyte or more of cache storage. 
On-board cache is found on the same chip as the microprocessor itself, 
and additional level-2 cache is found on another chip. Data and instructions are moved to cache from main memory when they are needed by 
the processor. Cached data can be accessed by the processor in a few 
nanoseconds.
2. Main Memory. In the center of the action is the computer’s main memory.
We may think of everything that happens in the computer — instruction 
executions and data manipulations — as working on information that is 
resident in main memory (although in practice, it is normal for what is 
used to migrate to the cache). A typical machine in 2008 is configured 
with about a gigabyte of main memory, although much larger main memories are possible. Typical times to move data from main memory to the 
processor or cache are in the 10-100 nanosecond range.
3. Secondary Storage. Secondary storage is typically magnetic disk, a device 
we shall consider in detail in Section 13.2. In 2008, single disk units 
have capacities of up to a terabyte, and one machine can have several 
disk units. The time to transfer a single byte between disk and main
13.1. THE MEM ORY HIERARCHY 559
Computer Quantities are Powers of 2
It is conventional to talk of sizes or capacities of computer components 
as if they were powers of 10: megabytes, gigabytes, and so on. In reality, 
since it is most efficient to design components such as memory chips to 
hold a number of bits that is a power of 2, all these numbers are really 
shorthands for nearby powers of 2. Since 210 = 1024 is very close to a 
thousand, we often maintain the fiction that 210 = 1000, and talk about 
210 with the prefix “kilo,” 220 as “mega,” 230 as “giga,” 240 as “tera,” and 
250 as “peta,” even though these prefixes in scientific parlance refer to 103, 
106, 109, 1012 and 1015, respectively. The discrepancy grows as we talk of 
larger numbers. A “gigabyte” is really 1.074 x 109 bytes.
We use the standard abbreviations for these numbers: K, M, G, T, and 
P for kilo, mega, giga, tera, and peta, respectively. Thus, 16Gb is sixteen 
gigabytes, or strictly speaking 234 bytes. Since we sometimes want to talk 
about numbers that are the conventional powers of 10, we shall reserve for 
these the traditional numbers, without the prefixes “kilo,” “mega,” and 
so on. For example, “one million bytes” is 1,000,000 bytes, while “one 
megabyte” is 1,048,576 bytes.
A recent trend is to use “kilobyte,” “megabyte,” and so on for exact 
powers of ten, and to replace the third and fourth letters by “bi” to represent the similar powers of two. Thus, “kibibyte” is 1024 bytes, “mebibyte” 
is 1,048,576 bytes, and so on. We shall not use this convention.
memory is around 10 miliseconds. However, large numbers of bytes can 
be transferred at one time, so the m atter of how fast data moves from 
and to disk is somewhat complex.
4. Tertiary Storage. As capacious as a collection of disk units can be, there
are databases much larger than what can be stored on the disk(s) of a
single machine, or even several machines. To serve such needs, tertiary
storage devices have been developed to hold data volumes measured in terabytes. Tertiary storage is characterized by significantly higher read/write
times than secondary storage, but also by much larger capacities and 
smaller cost per byte than is available from magnetic disks. Many tertiary devices involve robotic arms or conveyors that bring storage media 
such as magnetic tape or optical disks (e.g., DVD’s) to a reading device. 
Retrieval takes seconds or minutes, but capacities in the petabyte range 
are possible.
560 CHAPTER 13. SECONDARY STO RAG E M ANAGEM ENT
13.1.2 Transfer of Data Between Levels
Normally, data moves between adjacent levels of the hierarchy. At the secondary 
and tertiary levels, accessing the desired data or finding the desired place to 
store data takes a great deal of time, so each level is organized to transfer 
large amounts of data to or from the level below, whenever any data at all is 
needed. Especially important for understanding the operation of a database 
system is the fact that the disk is organized into disk blocks (or just blocks, or 
as in operating systems, pages) of perhaps 4-64 kilobytes. Entire blocks axe 
moved to or from a continuous section of main memory called a buffer. Thus, 
a key technique for speeding up database operations is to arrange data so that 
when one piece of a disk block is needed, it is likely that other data on the same 
block will also be needed at about the same time.
The same idea applies to other hierarchy levels. If we use tertiary storage, 
we try to arrange so that when we select a unit such as a DVD to read, we 
need much of what is on that DVD. At a lower level, movement between main 
memory and cache is by units of cache lines, typically 32 consecutive bytes. 
The hope is that entire cache lines will be used together. For example, if a 
cache line stores consecutive instructions of a program, we hope that when 
the first instruction is needed, the next few instructions will also be executed 
immediately thereafter.
13.1.3 Volatile and Nonvolatile Storage
An additional distinction among storage devices is whether they are volatile or 
nonvolatile. A volatile device “forgets” what is stored in it when the power goes 
off. A nonvolatile device, on the other hand, is expected to keep its contents 
intact even for long periods when the device is turned off or there is a power 
failure. The question of volatility is important, because one of the characteristic 
capabilities of a DBMS is the ability to retain its data even in the presence of 
errors such as power failures.
Magnetic and optical materials hold their data in the absence of power. 
Thus, essentially all secondary and tertiary storage devices are nonvolatile. On 
the other hand, main memory is generally volatile (although certain types of 
more expensive memory chips, such as flash memory, can hold their data after 
a power failure). A significant part of the complexity in a DBMS comes from 
the requirement that no change to the database can be considered final until it 
has migrated to nonvolatile, secondary storage.
13.1.4 Virtual MemoryTypical software executes in virtual-memory, an address space that is typically 
32 bits; i.e., there are 232 bytes, or 4 gigabytes, in a virtual memory. The 
operating system manages virtual memory, keeping some of it in main memory 
and the rest on disk. Transfer between memory and disk is in units of disk
13.1. THE MEMORY HIERARCHY 561
M oore’s Law
Gordon Moore observed many years ago that integrated circuits were improving in many ways, following an exponential curve that doubles about 
every 18 months. Some of these parameters that follow “Moore’s law” are:
1. The number of instructions per second that can be executed for unit 
cost. Until about 2005, the improvement was achieved by making 
processor chips faster, while keeping the cost fixed. After that year, 
the improvement has been maintained by putting progressively more 
processors on a single, fixed-cost chip.
2. The number of memory bits that can be bought for unit cost and 
the number of bits that can be put on one chip.
3. The number of bytes per unit cost on a disk and the capacity of the 
largest disks.
On the other hand, there are some other important parameters that 
do not follow Moore’s law; they grow slowly if at all. Among these slowly 
growing parameters are the speed of accessing data in main memory and 
the speed at which disks rotate. Because they grow slowly, “latency” 
becomes progressively larger. That is, the time to move data between 
levels of the memory hierarchy appears enormous today, and will only get 
worse.
blocks (pages). Virtual memory is an artifact of the operating system and its 
use of the machine’s hardware, and it is not a level of the memory hierarchy.
The path in Fig. 13.1 involving virtual memory represents the treatment 
of conventional programs and applications. It does not represent the typical 
way data in a database is managed, since a DBMS manages the data itself. 
However, there is increasing interest in main-memory database systems, which 
do indeed manage their data through virtual memory, relying on the operating 
system to bring needed data into main memory through the paging mechanism. 
Main-memory database systems, like most applications, are most useful when 
the data is small enough to remain in main memory without being swapped 
out by the operating system.
13.1.5 Exercises for Section 13.1
Exercise 13.1.1: Suppose that in 2008 the typical computer has a processor 
chip with two processors (“cores”) that each run at 3 gigahertz, has a disk of 
250 gigabytes, and a main memory of 1 gigabyte. Assume that Moore’s law 
(these factors double every 18 months) holds into the indefinite future.
562 CHAPTER 13. SECONDARY STO RAG E M ANAGEM ENT
a) When will petabyte disks be common?
b) When will terabyte main memories be common?
c) When will terahertz processor chips be common (i.e., the total number of 
cycles per second of all the cores on a chip will be approximately 1012?
d) W hat will be a typical configuration (processor, disk, memory) in the year 
2015?
! Exercise 13.1.2: Commander Data, the android from the 24th century on 
Star Trek: The Next Generation once proudly announced that his processor 
runs at “12 teraops.” While an operation and a cycle may not be the same, let 
us suppose they are, and that Moore’s law continues to hold for the next 300 
years. If so, what would Data’s true processor speed be?
13.2 Disks
The use of secondary storage is one of the important characteristics of a DBMS, 
and secondary storage is almost exclusively based on magnetic disks. Thus, to 
motivate many of the ideas used in DBMS implementation, we must examine 
the operation of disks in detail.
13.2.1 Mechanics of Disks
The two principal moving pieces of a disk drive are shown in Fig. 13.2; they 
are a disk assembly and a head assembly. The disk assembly consists of one 
or more circular platters that rotate around a central spindle. The upper and 
lower surfaces of the platters are covered with a thin layer of magnetic material, 
on which bits are stored. 0’s and l ’s are represented by different patterns in the 
magnetic material. A common diameter for disk platters is 3.5 inches, although 
disks with diameters from an inch to several feet have been built.
The disk is organized into tracks, which are concentric circles on a single 
platter. The tracks that are at a fixed radius from the center, among all the 
surfaces, form one cylinder. Tracks occupy most of a surface, except for the 
region closest to the spindle, as can be seen in the top view of Fig. 13.3. The 
density of data is much greater along a track than radially. In 2008, a typical 
disk has about 100,000 tracks per inch but stores about a million bits per inch 
along the tracks.
Tracks are organized into sectors, which are segments of the circle separated 
by gaps that are not magnetized to represent either 0’s or l ’s.1 The sector is an 
indivisible unit, as far as reading and writing the disk is concerned. It is also 
indivisible as far as errors are concerned. Should a portion of the magnetic layer
1 W e show each tra c k w ith th e sam e n u m b er of sectors in F ig. 13.3. However, th e n u m b er 
of sectors p e r tra c k no rm ally varies, w ith th e o u te r track s having m ore sectors th a n inn er 
tracks.
13.2. DISKS 563
Figure 13.2: A typical disk
be corrupted in some way, so that it cannot store information, then the entire 
sector containing this portion cannot be used. Gaps often represent about 10% 
of the total track and are used to help identify the beginnings of sectors. As we 
mentioned in Section 13.1.2, blocks are logical units of data that are transferred 
between disk and main memory; blocks consist of one or more sectors.
Figure 13.3: Top view of a disk surface
The second movable piece shown in Fig. 13.2, the head assembly, holds the 
disk heads. For each surface there is one head, riding extremely close to the 
surface but never touching it (or else a “head crash” occurs and the disk is 
destroyed). A head reads the magnetism passing under it, and can also alter 
the magnetism to write information on the disk. The heads are each attached 
to an arm, and the arms for all the surfaces move in and out together, being 
part of the rigid head assembly.
E xam ple 13.1: The Megatron 7^7 disk has the following characteristics, which
564 CHAPTER 13. SECONDARY STO RAG E M ANAGEM ENT
are typical of a large vintage-2008 disk drive.
• There are eight platters providing sixteen surfaces.
• There are 216, or 65,536, tracks per surface.
• There are (on average) 28 = 256 sectors per track.
• There are 212 = 4096 bytes per sector.
The capacity of the disk is the product of 16 surfaces, times 65,536 tracks, 
times 256 sectors, times 4096 bytes, or 240 bytes. The Megatron 747 is thus a 
terabyte disk. A single track holds 256 x 4096 bytes, or 1 megabyte. If blocks 
are 214, or 16,384 bytes, then one block uses 4 consecutive sectors, and there 
are (on average) 256/4 = 32 blocks on a track. □
13.2.2 The Disk Controller
One or more disk drives are controlled by a disk controller, which is a small 
processor capable of:
1. Controlling the mechanical actuator that moves the head assembly, to 
position the heads at a particular radius, i.e., so that any track of one 
particular cylinder can be read or written.
2. Selecting a sector from among all those in the cylinder at which the heads 
are positioned. The controller is also responsible for knowing when the rotating spindle has reached the point where the desired sector is beginning 
to move under the head.
3. Transferring bits between the desired sector and the computer’s main 
memory.
4. Possibly, buffering an entire track or more in local memory of the disk 
controller, hoping that many sectors of this track will be read soon, and 
additional accesses to the disk can be avoided.
Figure 13.4 shows a simple, single-processor computer. The processor communicates via a data bus with the main memory and the disk controller. A 
disk controller can control several disks; we show three disks in this example.
13.2.3 Disk Access Characteristics
Accessing (reading or writing) a block requires three steps, and each step has 
an associated delay.
1. The disk controller positions the head assembly at the cylinder containing 
the track on which the block is located. The time to do so is the seek time.
13.2. DISKS 565
Disks
Figure 13.4: Schematic of a simple computer system
2. The disk controller waits while the first sector of the block moves under 
the head. This time is called the rotational latency.
3. All the sectors and the gaps between them pass under the head, while the 
disk controller reads or writes data in these sectors. This delay is called 
the transfer time.
The sum of the seek time, rotational latency, and transfer time is the latency
of the disk.
The seek time for a typical disk depends on the distance the heads have to 
travel from where they are currently located. If they are already at the desired 
cylinder, the seek time is 0. However, it takes roughly a millisecond to start 
the disk heads moving, and perhaps 10 milliseconds to move them across all 
the tracks.
A typical disk rotates once in roughly 10 milliseconds. Thus, rotational 
latency ranges from 0 to 10 milliseconds, and the average is 5. TYansfer times 
tend to be much smaller, since there are often many blocks on a track. Thus, 
transfer times are in the sub-millisecond range. When you add all three delays, 
the typical average latency is about 10 milliseconds, and the maximum latency 
about twice that.
E xam ple 13.2: Let us examine the time it takes to read a 16,384-byte block 
from the Megatron 747 disk. First, we need to know some timing properties of 
the disk:
• The disk rotates at 7200 rpm; i.e., it makes one rotation in 8.33 milliseconds.
• To move the head assembly between cylinders takes one millisecond to 
start and stop, plus one additional millisecond for every 4000 cylinders
566 CHAPTER 13. SECONDARY STORAGE M ANAGEM ENT
traveled. Thus, the heads move one track in 1.00025 milliseconds and 
move from the innermost to the outermost track, a distance of 65,536 
tracks, in about 17.38 milliseconds.
• Gaps occupy 10% of the space around a track.
Let us calculate the minimum, maximum, and average times to read that
16,384-byte block. The minimum time is just the transfer time. That is, the 
block might be on a track over which the head is positioned already, and the 
first sector of the block might be about to pass under the head.
Since there are 4096 bytes per sector on the Megatron 747 (see Example 13.1 
for the physical specifications of the disk), the block occupies four sectors. The 
heads must therefore pass over four sectors and the three gaps between them. 
We assume that gaps represent 10% of the circle and sectors the remaining 90%. 
There are 256 gaps and 256 sectors around the circle. Since the gaps together 
cover 36 degrees of arc and sectors the remaining 324 degrees, the total degrees 
of arc covered by 3 gaps and 4 sectors is 36 x 3/256 + 324 x 4/256 = 5.48 
degrees. The transfer time is thus (5.48/360) x 0.00833 = .00013 seconds. That 
is, 5.48/360 is the fraction of a rotation needed to read the entire block, and 
.00833 seconds is the amount of time for a 360-degree rotation.
Now, let us look at the maximum possible time to read the block. In the 
worst case, the heads are positioned at the innermost cylinder, and the block 
we want to read is on the outermost cylinder (or vice versa). Thus, the first 
thing the controller must do is move the heads. As we observed above, the time 
it takes to move the Megatron 747 heads across all cylinders is about 17.38 
milliseconds. This quantity is the seek time for the read.
The worst thing that can happen when the heads arrive at the correct cylinder is that the beginning of the desired block has just passed under the head. 
Assuming we must read the block starting at the beginning, we have to wait 
essentially a full rotation, or 8.33 milliseconds, for the beginning of the block 
to reach the head again. Once that happens, we have only to wait an amount 
equal to the transfer time, 0.13 milliseconds, to read the entire block. Thus, 
the worst-case latency is 17.38 + 8.33 + 0.13 = 25.84 milliseconds.
Last, let us compute the average latency. Two of the components of the 
latency are easy to compute: the transfer time is always 0.13 milliseconds, and 
the average rotational latency is the time to rotate the disk half way around, or
4.17 milliseconds. We might suppose that the average seek time is just the time 
to move across half the tracks. However, that is not quite right, since typically, 
the heads are initially somewhere near the middle and therefore will have to 
move less than half the distance, on average, to the desired cylinder. We leave 
it as an exercise to show that the average distance traveled is 1/3 of the way 
across the disk.
The time it takes the Megatron 747 to move 1/3 of the way across the disk 
is 1 + (65536/3)/4000 = 6.46 milliseconds. Our estimate of the average latency 
is thus 6.46 + 4.17 + 0.13 = 10.76 milliseconds; the three terms represent average 
seek time, average rotational latency, and transfer time, respectively. □
13.2. DISKS 567
13.2.4 Exercises for Section 13.2
E xercise 13.2.1: The Megatron 777 disk has the following characteristics:
1. There are ten surfaces, with 100,000 tracks each.
2. Tracks hold an average of 1000 sectors of 1024 bytes each.
3. 20% of each track is used for gaps.
4. The disk rotates at 10,000 rpm.
5. The time it takes the head to move n tracks is 1 + 0.0002n milliseconds. 
Answer the following questions about the Megatron 777.
a) What is the capacity of the disk?
b) If tracks are located on the outer inch of a 3.5-inch-diameter surface, what 
is the average density of bits in the sectors of a track?
c) What is the maximum seek time?
d) W hat is the maximum rotational latency?
e) If a block is 65,546 bytes (i.e., 64 sectors), what is the transfer time of a 
block?
! f) What is the average seek time?
g) W hat is the average rotational latency?
! E xercise 13.2.2: Suppose the Megatron 747 disk head is at cylinder 8192,
i.e., 1/8 of the way across the cylinders. Suppose that the next request is for a 
block on a random cylinder. Calculate the average time to read this block.
!! E xercise 13.2.3: Prove that if we move the head from a random cylinder to 
another random cylinder, the average distance we move is 1/3 of the way across 
the disk (neglecting edge effects due to the fact that the number of cylinders is 
finite).
!! E xercise 13.2.4: Exercise 13.2.3 assumes that we move from a random track 
to another random track. Suppose, however, that the number of sectors per 
track is proportional to the length (or radius) of the track, so the bit density 
is the same for all tracks. Suppose also that we need to move the head from a 
random sector to another random sector. Since the sectors tend to congregate 
at the outside of the disk, we might expect that the average head move would 
be less than 1/3 of the way across the tracks. Assuming that tracks occupy 
radii from 0.75 inches to 1.75 inches, calculate the average number of tracks the 
head travels when moving between two random sectors.
568 CHAPTER 13. SECONDARY STORAGE M ANAGEM ENT
E xercise 13.2.5: To modify a block on disk, we must read it into main memory, perform the modification, and write it back. Assume that the modification 
in main memory takes less time than it does for the disk to rotate, and that the 
disk controller postpones other requests for disk access until the block is ready 
to be written back to the disk. For the Megatron 747 disk, what is the time to 
modify a block?
13.3 Accelerating Access to Secondary Storage
Just because a disk takes an average of, say, 10 milliseconds to access a block, 
it does not follow that an application such as a database system will get the 
data it requests 10 milliseconds after the request is sent to the disk controller. 
If there is only one disk, the disk may be busy with another access for the same 
process or another process. In the worst case, a request for a disk access arrives 
more than once every 10 milliseconds, and these requests back up indefinitely. 
In that case, the scheduling latency becomes infinite.
There are several things we can do to decrease the average time a disk access 
takes, and thus improve the throughput (number of disk accesses per second that 
the system can accomodate). We begin this section by arguing that the “I/O 
model” is the right one for measuring the time database operations take. Then, 
we consider a number of techniques for speeding up typical database accesses 
to disk:
1. Place blocks that are accessed together on the same cylinder, so we can 
often avoid seek time, and possibly rotational latency as well.
2. Divide the data among several smaller disks rather than one large one. 
Having more head assemblies that can go after blocks independently can 
increase the number of block accesses per unit time.
3. “Mirror” a disk: making two or more copies of the data on different disks. 
In addition to saving the data in case one of the disks fails, this strategy, 
like dividing the data among several disks, lets us access several blocks at 
once.
4. Use a disk-scheduling algorithm, either in the operating system, in the 
DBMS, or in the disk controller, to select the order in which several 
requested blocks will be read or written.
5. Prefetch blocks to main memory in anticipation of their later use.
13.3.1 The I/O Model of Computation
Let us imagine a simple computer running a DBMS and trying to serve a 
number of users who are performing queries and database modifications. For 
the moment, assume our computer has one processor, one disk controller, and
13.3. ACCELERATING ACCESS TO SECONDARY STORAGE 569
one disk. The database itself is much too large to fit in main memory. Key parts 
of the database may be buffered in main memory, but generally, each piece of 
the database that one of the users accesses will have to be retrieved initially 
from disk. The following rule, which defines the I/O model of computation, can 
thus be assumed.
D om inance of I/O cost: The time taken to perform a disk access is much larger than the time likely to be used manipulating 
that data in main memory. Thus, the number of block accesses 
(Disk I /O ’s) is a good approximation to the time needed by the 
algorithm and should be minimized.
E xam ple 13.3: Suppose our database has a relation R and a query asks for 
the tuple of R that has a certain key value k. It is quite desirable to have 
an index on R to identify the disk block on which the tuple with key value k
appears. However it is generally unimportant whether the index tells us where 
on the block this tuple appears.
For instance, if we assume a Megatron 747 disk, it will take on the order 
of 11 milliseconds to read a 16K-byte block. In 11 milliseconds, a modern 
microprocessor can execute millions of instructions. However, searching for 
the key value k once the block is in main memory will only take thousands of 
instructions, even if the dumbest possible linear search is used. The additional 
time to perform the search in main memory will therefore be less than 1% of 
the block access time and can be neglected safely. □
13.3.2 Organizing Data by Cylinders
Since seek time represents about half the time it takes to access a block, it makes 
sense to store data that is likely to be accessed together, such as relations, on 
a single cylinder, or on as many adjacent cylinders as are needed. In fact, if we 
choose to read all the blocks on a single track or on a cylinder consecutively, 
then we can neglect all but the first seek time (to move to the cylinder) and 
the first rotational latency (to wait until the first of the blocks moves under the 
head). In that case, we can approach the theoretical transfer rate for moving 
data on or off the disk.
E xam ple 13.4: Suppose relation R requires 1024 blocks of a Megatron 747 
disk to hold its tuples. Suppose also that we need to access all the tuples of 
R; for example we may be doing a search without an index or computing a 
sum of the values of a particular attribute of R. If the blocks holding R are 
distributed around the disk at random, then we shall need an average latency 
(10.76 milliseconds — see Example 13.2) to access each, for a total of 11 seconds.
However, 1024 blocks are exactly one cylinder of the Megatron 747. We can 
access them all by performing one average seek (6.46 milliseconds), after which 
we can read the blocks in some order, one right after another. We can read all 
the blocks on a cylinder in 16 rotations of the disk, since there are 16 tracks.
570 CHAPTER 13. SECONDARY STORAGE M ANAGEM ENT
Sixteen rotations take 16 x 8.33 = 133 milliseconds. The total time to access R
is thus about 139 milliseconds, and we speed up the operation on R by a factor 
of about 80. □
13.3.3 Using M ultiple Disks
We can often improve the performance of our system if we replace one disk, with 
many heads locked together, by several disks with their independent heads. The 
arrangement was suggested in Fig. 13.4, where we showed three disks connected 
to a single controller. As long as the disk controller, bus, and main memory 
can handle n times the data-transfer rate, then n disks will have approximately 
the performance of one disk that operates n times as fast.
Thus, using several disks can increase the ability of a database system to 
handle heavy loads of disk-access requests. However, as long as the system is 
not overloaded (when requests will queue up and are delayed for a long time or 
ignored), there is no change in how long it takes to perform any single block 
access. If we have several disks, then the technique known as striping (described 
in the next example) will speed up access to large database objects — those 
that occupy a large number of blocks.
E xam ple 1 3 .5 : Suppose we have four Megatron 747 disks and want to access 
the relation R of Example 13.4 faster than the 139-millisecond time that was 
suggested for storing R on one cylinder of one disk. We can “stripe” R by 
dividing it among the four disks. The first disk can receive blocks 1 ,5 ,9 ,... of 
R, the second disk holds blocks 2 ,6 ,1 0 ,..., the third holds blocks 3 ,7 ,1 1 ,..., 
and the last disk holds blocks 4 ,8 ,1 2 ,..., as suggested by Fig. 13.5. Let us 
contrive that on each of the disks, all the blocks of R are on four tracks of a 
single cylinder.
r^i h 
f^i r~i
IZD H
HD
10
[V]
HD
Figure 13.5: Striping a relation across four disks
Then to retrieve the 256 blocks of R on one of the disks requires an average 
seek time (6.46 milliseconds) plus four rotations of the disk, one rotation for 
each track. That is 6.46 + 4 x 8.33 = 39.8 milliseconds. Of course we have to 
wait for the last of the four disks to finish, and there is a high probability that 
one will take substantially more seek time than average. However, we should 
get a speedup in the time to access R by about a factor of three on the average, 
when there are four disks. □
13.3. ACCELERATING ACCESS TO SECONDARY STORAGE 571
13.3.4 Mirroring Disks
There are situations where it makes sense to have two or more disks hold identical copies of data. The disks are said to be mirrors of each other. One important 
motivation is that the data will survive a head crash by either disk, since it is 
still readable on a mirror of the disk that crashed. Systems designed to enhance 
reliability often use pairs of disks as mirrors of each other.
If we have n disks, each holding the same data, then the rate at which we 
can read blocks goes up by a factor of n, since the disk controller can assign a 
read request to any of the n disks. In fact, the speedup could be even greater 
than n, if a clever controller chooses to read a block from the disk whose head 
is currently closest to that block. Unfortunately, the writing of disk blocks does 
not speed up at all. The reason is that the new block must be written to each 
of the n disks.
13.3.5 Disk Scheduling and the Elevator Algorithm
Another effective way to improve the throughput of a disk system is to have the 
disk controller choose which of several requests to execute first. This approach 
cannot be used if accesses have to be made in a certain sequence, but if the 
requests are from independent processes, they can all benefit, on the average, 
from allowing the scheduler to choose among them judiciously.
A simple and effective way to schedule large numbers of block requests is 
known as the elevator algorithm. We think of the disk head as making sweeps 
across the disk, from innermost to outermost cylinder and then back again, 
just as an elevator makes vertical sweeps from the bottom to top of a building 
and back again. As heads pass a cylinder, they stop if there are one or more 
requests for blocks on that cylinder. All these blocks are read or written, as 
requested. The heads then proceed in the same direction they were traveling 
until the next cylinder with blocks to access is encountered. When the heads 
reach a position where there are no requests ahead of them in their direction of 
travel, they reverse direction.
E xam ple 13.6: Suppose we are scheduling a Megatron 747 disk, which we 
recall has average seek, rotational latency, and transfer times of 6.46, 4.17, 
and 0.13, respectively (in this example, all times are in milliseconds). Suppose 
that at some time there are pending requests for block accesses at cylinders 
8000, 24,000, and 56,000. The heads are located at cylinder 8000. In addition, 
there are three more requests for block accesses that come in at later times, as 
summarized in Fig. 13.6. For instance, the request for a block from cylinder
16,000 is made at time 10 milliseconds.
We shall assume that each block access incurs time 0.13 for transfer and
4.17 for average rotational latency, i.e., we need 4.3 milliseconds plus whatever 
the seek time is for each block access. The seek time can be calculated by the 
rule for the Megatron 747 given in Example 13.2: 1 plus the number of tracks 
divided by 4000. Let us see what happens if we schedule disk accesses using
572 CHAPTER 13. SECONDARY STORAGE M ANAGEM ENT
Cylinder 
of request
First time 
available
8000 0
24000 0
56000 0
16000 10
64000 20
40000 30
Figure 13.6: Arrival times for four block-access requests
the elevator algorithm. The first request, at cylinder 8000, requires no seek, 
since the heads are already there. Thus, at time 4.3 the first access will be 
complete. The request for cylinder 16,000 has not arrived at this point, so we 
move the heads to cylinder 24,000, the next requested “stop” on our sweep to 
the highest-numbered tracks. The seek from cylinder 8000 to 24,000 takes 5 
milliseconds, so we arrive at time 9.3 and complete the access in another 4.3. 
Thus, the second access is complete at time 13.6. By this time, the request for 
cylinder 16,000 has arrived, but we passed that cylinder at time 7.3 and will 
not come back to it until the next pass.
We thus move next to cylinder 56,000, taking time 9 to seek and 4.3 for 
rotation and transfer. The third access is thus complete at time 26.9. Now, the 
request for cylinder 64,000 has arrived, so we continue outward. We require 3 
milliseconds for seek time, so this access is complete at time 26.9+3+4.3 = 34.2.
At this time, the request for cylinder 40,000 has been made, so it and the 
request at cylinder 16,000 remain. We thus sweep inward, honoring these two 
requests. Figure 13.7 summarizes the times at which requests are honored.
Cylinder 
of request
Time
completed
8000 4.3
24000 13.6
56000 26.9
64000 34.2
40000 45.5
16000 56.8
Figure 13.7: Finishing times for block accesses using the elevator algorithm
Let us compare the performance of the elevator algorithm with a more naive 
approach such as first-come-first-served. The first three requests are satisfied 
in exactly the same manner, assuming that the order of the first three requests 
was 8000, 24,000, and 56,000. However, at that point, we go to cylinder 16,000,
13.3. ACCELERATING ACCESS TO SECONDARY STORAGE 573
because that was the fourth request to arrive. The seek time is 11 for this 
request, since we travel from cylinder 56,000 to 16,000, more than half way 
across the disk. The fifth request, at cylinder 64,000, requires a seek time of 13, 
and the last, at 40,000, uses seek time 7. Figure 13.8 summarizes the activity 
caused by first-come-first-served scheduling. The difference between the two 
algorithms — 14 milliseconds — may not appear significant, but recall that 
the number of requests in this simple example is small and the algorithms were 
assumed not to deviate until the fourth of the six requests. □
Cylinder 
of request
Time
completed
8000 4.3
24000 13.6
56000 26.9
16000 42.2
64000 59.5
40000 70.8
Figure 13.8: Finishing times for block accesses using the first-come-first-served 
algorithm
13.3.6 Prefetching and Large-Scale Buffering
Our final suggestion for speeding up some secondary-memory algorithms is 
called prefetching or sometimes double buffering. In some applications we can 
predict the order in which blocks will be requested from disk. If so, then we can 
load them into main memory buffers before they are needed. One advantage to 
doing so is that we are thus better able to schedule the disk, such as by using 
the elevator algorithm, to reduce the average time needed to access a block. In 
the extreme case, where there are many access requests waiting at all times, we 
can make the seek time per request be very close to the minimum seek time, 
rather than the average seek time.
13.3.7 Exercises for Section 13.3
E xercise 13.3.1: Suppose we are scheduling I/O requests for a Megatron 747 
disk, and the requests in Fig. 13.9 are made, with the head initially at track 
32,000. At what time is each request serviced fully if:
a) We use the elevator algorithm (it is permissible to start moving in either 
direction at first).
b) We use first-come-first-served scheduling.
574 CHAPTER 13. SECONDARY STORAGE M ANAGEM ENT
Cylinder First time
of Request available
8000 0
48000 1
4000 10
40000 20
Figure 13.9: Arrival times for four block-access requests
Exercise 13.3.2: Suppose we use two Megatron 747 disks as mirrors of one 
another. However, instead of allowing reads of any block from either disk, we 
keep the head of the first disk in the inner half of the cylinders, and the head 
of the second disk in the outer half of the cylinders. Assuming read requests 
are on random tracks, and we never have to write:
a) What is the average rate at which this system can read blocks?
b) How does this rate compare with the average rate for mirrored Megatron 
747 disks with no restriction?
c) What disadvantages do you foresee for this system?
E xercise 13.3.3: Let us explore the relationship between the arrival rate of 
requests, the throughput of the elevator algorithm, and the average delay of 
requests. To simplify the problem, we shall make the following assumptions:
1. A pass of the elevator algorithm always proceeds from the innermost to 
outermost track, or vice-versa, even if there are no requests at the extreme 
cylinders.
2. When a pass starts, only those requests that are already pending will be 
honored, not requests that come in while the pass is in progress, even if 
the head passes their cylinder.2
3. There will never be two requests for blocks on the same cylinder waiting 
on one pass.
Let A be the interarrival rate, that is the time between requests for block accesses. Assume that the system is in steady state, that is, it has been accepting 
and answering requests for a long time. For a Megatron 747 disk, compute as 
a function of A:
2 T h e p urpose o f th is assu m p tio n is to avoid having to deal w ith th e fact th a t a typical pass 
of th e elevator alg o rith m goes fast a t first, as th e re w ill b e few w aiting requests w here th e 
head h as recently been, an d slows dow n as it m oves in to an a re a of th e disk w here it has not 
recently been. T h e analysis of th e way request d ensity varies during a pass is an interestin g 
exercise in its ow n right.
13.4. DISK FAILURES 575
a) The average time taken to perform one pass.
b) The number of requests serviced on one pass.
c) The average time a request waits for service.
!! E xercise 13.3.4: In Example 13.5, we saw how dividing the data to be sorted 
among four disks could allow more than one block to be read at a time. Suppose our data is divided randomly among n disks, and requests for data are also 
random. Requests must be executed in the order in which they are received 
because there are dependencies among them that must be respected (see Chapter 18, for example, for motivation for this constraint). What is the average 
throughput for such a system?
! E xercise 13.3.5: If we read k randomly chosen blocks from one cylinder, on 
the average how far around the cylinder must we go before we pass all of the 
blocks?
13.4 Disk Failures
In this section we shall consider the ways in which disks can fail and what can 
be done to mitigate these failures.
1. The most common form of failure is an intermittent failure, where an 
attempt to read or write a sector is unsuccessful, but with repeated tries 
we are able to read or write successfully.
2. A more serious form of failure is one in which a bit or bits are permanently 
corrupted, and it becomes impossible to read a sector correctly no matter 
how many times we try. This form of error is called media decay.
3. A related type of error is a write failure, where we attempt to write 
a sector, but we can neither write successfully nor can we retrieve the 
previously written sector. A possible cause is that there was a power 
outage during the writing of the sector.
4. The most serious form of disk failure is a disk crash, where the entire disk 
becomes unreadable, suddenly and permanently.
We shall discuss parity checks as a way to detect intermittent failures. We also 
discuss “stable storage,” a technique for organizing a disk so that media decays 
or failed writes do not result in permanent loss. Finally, we examine techniques 
collectively known as “RAID” for coping with disk crashes.
576 CHAPTER 13. SECONDARY STO RAG E M ANAGEM ENT
13.4.1 Interm ittent Failures
An intermittent failure occurs if we try to read a sector, but the correct content 
of that sector is not delivered to the disk controller. If the controller has a way 
to tell that the sector is good or bad (as we shall discuss in Section 13.4.2), 
then the controller can reissue the read request when bad data is read, until 
the sector is returned correctly, or some preset limit, like 100 tries, is reached.
Similarly, the controller may attempt to write a sector, but the contents of 
the sector are not what was intended. The only way to check that the write was 
correct is to let the disk go around again and read the sector. A straightforward 
way to perform the check is to read the sector and compare it with the sector 
we intended to write. However, instead of performing the complete comparison 
at the disk controller, it is simpler to read the sector and see if a good sector 
was read. If so, we assume the write was correct, and if the sector read is bad, 
then the write was apparently unsuccessful and must be repeated.
13.4.2 Checksums
How a reading operation can determine the good/bad status of a sector may 
appear mysterious at first. Yet the technique used in modern disk drives is quite 
simple: each sector has some additional bits, called the checksum, that are set 
depending on the values of the data bits stored in that sector. If, on reading, 
we find that the checksum is not proper for the data bits, then we know there 
is an error in reading. If the checkum is proper, there is still a small chance 
that the block was not read correctly, but by using many checksum bits we can 
make the probability of missing a bad read arbitrarily small.
A simple form of checksum is based on the parity of all the bits in the sector. 
If there is an odd number of l ’s among a collection of bits, we say the bits have 
odd parity and add a parity bit that is 1. Similarly, if there is an even number 
of l ’s among the bits, then we say the bits have even parity and add parity bit
0. As a result:
• The number of l ’s among a collection of bits and their parity bit is always 
even.
When we write a sector, the disk controller can compute the parity bit and 
append it to the sequence of bits written in the sector. Thus, every sector will 
have even parity.
E xam ple 1 3 .7 : If the sequence of bits in a sector were 01101000, then there 
is an odd number of l ’s, so the parity bit is 1. If we follow this sequence by its 
parity bit we have 011010001. If the given sequence of bits were 11101110, we 
have an even number of l ’s, and the parity bit is 0. The sequence followed by 
its parity bit is 111011100. Note that each of the nine-bit sequences constructed 
by adding a parity bit has even parity. □
13.4. DISK FAILURES 577
Any one-bit error in reading or writing the bits and their parity bit results 
in a sequence of bits that has odd parity, i.e., the number of l ’s is odd. It is 
easy for the disk controller to count the number of l ’s and to determine the 
presence of an error if a sector has odd parity.
Of course, more than one bit of the sector may be corrupted. If so, the 
probability is 50% that the number of 1-bits will be even, and the error will not 
be detected. We can increase our chances of detecting errors if we keep several 
parity bits. For example, we could keep eight parity bits, one for the first bit 
of every byte, one for the second bit of every byte, and so on, up to the eighth 
and last bit of every byte. Then, on a massive error, the probability is 50% 
that any one parity bit will detect an error, and the chance that none of the 
eight do so is only one in 28, or 1/256. In general, if we use n independent bits 
as a checksum, then the chance of missing an error is only 1/2". For instance, 
if we devote 4 bytes to a checksum, then there is only one chance in about four 
billion that the error will go undetected.
13.4.3 Stable Storage
While checksums will almost certainly detect the existence of a media failure 
or a failure to read or write correctly, it does not help us correct the error. 
Moreover, when writing we could find ourselves in a position where we overwrite 
the previous contents of a sector and yet cannot read the new contents correctly. 
That situation could be serious if, say, we were adding a small increment to 
an account balance and now have lost both the original balance and the new 
balance. If we could be assured that the contents of the sector contained either 
the new or old balance, then we would only have to determine whether the 
write was successful or not.
To deal with the problems above, we can implement a policy known as 
stable storage on a disk or on several disks. The general idea is that sectors 
are paired, and each pair represents one sector-contents X . We shall refer to 
the pair of sectors representing X as the “left” and “right” copies, X l and X r .
We continue to assume that the copies are written with a sufficient number of 
parity-check bits so that we can rule out the possibility that a bad sector looks 
good when the parity checks are considered. Thus, we shall assume that if the 
read function returns a good value w for either X l or X r , then w is the true 
value of X . The stable-storage writing policy is:
1. Write the value of X into X l - Check that the value has status “good”;
i.e., the parity-check bits are correct in the written copy. If not, repeat the 
write. If after a set number of write attempts, we have not successfully 
written X into X l , assume that there is-a media failure in this sector. A 
fix-up such as substituting a spare sector for X l must be adopted.
2. Repeat (1) for X r .
The stable-storage reading policy is to alternate trying to read X l and X r ,
578 CHAPTER 13. SECONDARY STORAGE M ANAGEM ENT
until a good value is returned. Only if no good value is returned after some 
large, prechosen number of tries, is X truly unreadable.
13.4.4 Error-Handling Capabilities of Stable Storage
The policies described in Section 13.4.3 are capable of compensating for several 
different kinds of errors. We shall outline them here.
1. Media failures. If, after storing X in sectors X l and X r , one of them 
undergoes a media failure and becomes permanently unreadable, we can 
always read X from the other. If both X l and X r have failed, then we 
cannot read X , but the probability of both failing is extremely small.
2. Write failure. Suppose that as we write X , there is a system failure — 
e.g., a power outage. It is possible that X will be lost in main memory, 
and also the copy of X being written at the time will be garbled. For 
example, half the sector may be written with part of the new value of X ,
while the other half remains as it was. When the system becomes available 
and we examine X l and X r , we are sure to be able to determine either 
the old or new value of X . The possible cases are:
(a) The failure occurred as we were writing X l ■ Then we shall find that 
the status of X l is “bad.” However, since we never got to write X r ,
its status will be “good” (unless there is a coincident media failure 
at X r , which is extremely unlikely). Thus, we can obtain the old 
value of X . We may also copy X r into X l to repair the damage to 
X l .
(b) The failure occurred after we wrote X l- Then we expect that X l
will have status “good,” and we may read the new value of X from 
X l - Since X r may or may not have the correct value of X , we 
should also copy X l into X r .
13.4.5 Recovery from Disk Crashes
The most serious mode of failure for disks is the “disk crash” or “head crash,” 
where data is permanently destroyed. If the data was not backed up on another 
medium, such as a tape backup system, or on a mirror disk as we discussed in 
Section 13.3.4, then there is nothing we can do to recover the data. This 
situation represents a disaster for many DBMS applications, such as banking 
and other financial applications.
Several schemes have been developed to reduce the risk of data loss by disk 
crashes. They generally involve redundancy, extending the idea of parity checks 
from Section 13.4.2 or duplicated sectors, as in Section 13.4.3. The common 
term for this class of strategies is RAID, or Redundant Arrays of Independent
Disks.
13.4. DISK FAILURES 579
The rate at which disk crashes occur is generally measured by the mean time
to failure, the time after which 50% of a population of disks can be expected to 
fail and be unrecoverable. For modern disks, the mean time to failure is about 
10 years. We shall make the convenient assumption that if the mean time to 
failure is n years, then in any given year, 1/nth of the surviving disks fail. In 
reality, there is a tendency for disks, like most electronic equipment, to fail early 
or fail late. That is, a small percentage have manufacturing defects that lead 
to their early demise, while those without such defects will survive for many 
years, until wear-and-tear causes a failure.
However, the mean time to a disk crash does not have to be the same as 
the mean time to data loss. The reason is that there are a number of schemes 
available for assuring that if one disk fails, there are others to help recover the 
data of the failed disk. In the remainder of this section, we shall study the most 
common schemes.
Each of these schemes starts with one or more disks that hold the data (we’ll 
call these the data disks) and adding one or more disks that hold information 
that is completely determined by the contents of the data disks. The latter are 
called redundant disks. When there is a disk crash of either a data disk or a 
redundant disk, the other disks can be used to restore the failed disk, and there 
is no permanent information loss.
13.4.6 Mirroring as a Redundancy Technique
The simplest scheme is to mirror each disk, as discussed in Section 13.3.4. 
We shall call one of the disks the data disk, while the other is the redundant
disk, which is which doesn’t matter in this scheme. Mirroring, as a protection 
against data loss, is often referred to as RAID level 1. It gives a mean time 
to memory loss that is much greater than the mean time to disk failure, as 
the following example illustrates. Essentially, with mirroring and the other 
redundancy schemes we discuss, the only way data can be lost is if there is a 
second disk crash while the first crash is being repaired.
E xam ple 13.8: Suppose each disk has a 10-year mean time to failure, which 
we shall take to mean that the probability of failure in any given year is 10%. 
If disks are mirrored, then when a disk fails, we have only to replace it with a 
good disk and copy the mirror disk to the new one. At the end, we have two 
disks that are mirrors of each other, and the system is restored to its former 
state.
The only thing that could go wrong is that during the copying the mirror 
disk fails. Now, both copies of at least part of the data have been lost, and 
there is no way to recover.
But how often will this sequence of events occur? Suppose that the process 
of replacing the failed disk takes 3 hours, which is 1/8 of a day, or 1/2920 of a 
year. Since we assume the average disk lasts 10 years, the probability that the 
mirror disk will fail during copying is (1/10) x (1/2920), or one in 29,200. If
580 CHAPTER 13. SECONDARY STORAGE M ANAGEM ENT
one disk fails every 10 years, then one of the two disks will fail once in 5 years 
on the average. One in every 29,200 of these failures results in data loss. Put 
another way, the mean time to a failure involving data loss is 5 x 29,200 =
146,000 years. □
13.4.7 Parity Blocks
While mirroring disks is an effective way to reduce the probability of a disk crash 
involving data loss, it uses as many redundant disks as there are data disks. 
Another approach, often called RAID level 4, uses only one redundant disk, no 
m atter how many data disks there are. We assume the disks are identical, so 
we can number the blocks on each disk from 1 to some number n. Of course, 
all the blocks on all the disks have the same number of bits; for instance, the
16,384-byte blocks of the Megatron 747 have 8 x 16,384 = 131,072 bits. In the 
redundant disk, the *th block consists of parity checks for the *th blocks of all 
the data disks. That is, the jth bits of all the ith blocks, including both the 
data disks and the redundant disk, must have an even number of l ’s among 
them, and we always choose the bit of the redundant disk to make this condition 
true.
We saw in Example 13.7 how to force the condition to be true. In the 
redundant disk, we choose bit j to be 1 if an odd number of the data disks 
have 1 in that bit, and we choose bit j of the redundant disk to be 0 if there 
are an even number of l ’s in that bit among the data disks. The term for this 
calculation is the modulo-2 sum. That is, the modulo-2 sum of bits is 0 if there 
are an even number of l ’s among those bits, and 1 if there are an odd number 
of l ’s.
E xam p le 1 3 .9 : Suppose for sake of an extremely simple example that blocks 
consist of only one byte — eight bits. Let there be three data disks, called
1, 2, and 3, and one redundant disk, called disk 4. Focus on the first block 
of all these disks. If the data disks have in their first blocks the following bit 
sequences:
disk 1: 11110000 
disk 2: 10101010 
disk 3: 00111000
then the redundant disk will have in block 1 the parity check bits:
disk 4: 01100010
Notice how in each position, an even number of the four 8-bit sequences have 
l ’s. There are two l ’s in positions 1, 2, 4, 5, and 7, four l ’s in position 3, and 
zero l ’s in positions 6 and 8. □
13.4. DISK FAILURES 581
R eading
Reading blocks from a data disk is no different from reading blocks from any 
disk. There is generally no reason to read from the redundant disk, but we 
could.
W riting
When we write a new block of a data disk, we need not only to change that 
block, but we need to change the corresponding block of the redundant disk 
so it continues to hold the parity checks for the corresponding blocks of all the 
data disks. A naive approach would read the corresponding blocks of the n data 
disks, take their modulo-2 sum, and rewrite the block of the redundant disk. 
That approach requires a write of the data block that is rewritten, the reading 
of the n — 1 other data blocks, and a write of the block of the redundant disk. 
The total is thus n + 1 disk I/O ’s.
A better approach is to look only at the old and new versions of the data 
block i being rewritten. If we take their modulo-2 sum, we know in which 
positions there is a change in the number of l ’s among the blocks numbered i
on all the disks. Since these changes are always by one, any even number of l ’s 
changes to an odd number. If we change the same positions of the redundant 
block, then the number of l ’s in each position becomes even again. We can 
perform these calculations using four disk I/O ’s:
1. Read the old value of the data block being changed.
2. Read the corresponding block of the redundant disk.
3. Write the new data block.
4. Recalculate and write the block of the redundant disk.
E xam ple 13.10: Suppose the three first blocks of the data disks are as in 
Example 13.9:
disk 1: 11110000 
disk 2: 10101010 
disk 3: 00111000
Suppose also that the block on the second disk changes from 10101010 to 
11001100. We take the modulo-2 sum of the old and new values of the block 
on disk 2, to get 01100110. That tells us we must change positions 2, 3, 6, and 
7 of the first block of the redundant disk. We read that block: 01100010. We 
replace this block by a new block that we get by changing the appropriate positions; in effect we replace the redundant block by the modulo-2 sum of itself 
and 01100110, to get 00000100. Another way to express the new redundant 
block is that it is the modulo-2 sum of the old and new versions of the block
582 CHAPTER 13. SECONDARY STORAGE M ANAGEM ENT
The Algebra of Modulo-2 Sums
It may be helpful for understanding some of the tricks used with parity 
checks to know the algebraic rules involving the modulo-2 sum operation on bit vectors. We shall denote this operation ©. As an example, 
1100 ® 1010 = 0110. Here are some useful rules about ffi:
• The commutative law: x © y = y ffi x.
• The associative law. x © (y © z) — (x © y) © z.
• The all-0 vector of the appropriate length, which we denote 0, is the 
identity for ©; that is, x ffi 0 = 0 ffi x = x.
• ffi is its own inverse: x ffi x = 0. As a useful consequence, if x ffi y = 2 , 
then we can “add” x to both sides and get y = x ffi z.
being rewritten and the old value of the redundant block. In our example, the 
first blocks of the four disks — three data disks and one redundant — have 
become:
disk 1: 11110000 
disk 2: 11001100 
disk 3: 00111000 
disk 4: 00000100
after the write to the block on the second disk and the necessary recomputation 
of the redundant block. Notice that in the blocks above, each column continues 
to have an even number of l ’s. □
Failure R ecovery
Now, let us consider what we would do if one of the disks crashed. If it is the 
redundant disk, we swap in a new disk, and recompute the redundant blocks. If 
the failed disk is one of the data disks, then we need to swap in a good disk and 
recompute its data from the other disks. The rule for recomputing any missing 
data is actually simple, and doesn’t depend on which disk, data or redundant, 
is failed. Since we know that the number of l ’s among corresponding bits of all 
disks is even, it follows that:
• The bit in any position is the modulo-2 sum of all the bits in the corresponding positions of all the other disks.
If one doubts the above rule, one has only to consider the two cases. If the 
bit in question is 1, then the number of corresponding bits in the other disks
13.4. DISK FAILURES 583
that are 1 must be odd, so their modulo-2 sum is 1. If the bit in question is 0, 
then there are an even number of l ’s among the corresponding bits of the other 
disks, and their modulo-2 sum is 0.
E xam ple 13.11: Suppose that disk 2 fails. We need to recompute each block 
of the replacement disk. Following Example 13.9, let us see how to recompute 
the first block of the second disk. We are given the corresponding blocks of the 
first and third data disks and the redundant disk, so the situation looks like:
disk 1: 11110000 
disk 2: ???????? 
disk 3: 00111000 
disk 4: 01100010
If we take the modulo-2 sum of each column, we deduce that the missing block 
is 10101010, as was initially the case in Example 13.9. □
13.4.8 An Improvement: RAID 5
The RAID level 4 strategy described in Section 13.4.7 effectively preserves data 
unless there are two almost simultaneous disk crashes. However, it suffers from 
a bottleneck defect that we can see when we re-examine the process of writing 
a new data block. Whatever scheme we use for updating the disks, we need to 
read and write the redundant disk’s block. If there are n data disks, then the 
number of disk writes to the redundant disk will be n times the average number 
of writes to any one data disk.
However, as we observed in Example 13.11, the rule for recovery is the 
same as for the data disks and redundant disks: take the modulo-2 sum of 
corresponding bits of the other disks. Thus, we do not have to treat one disk as 
the redundant disk and the others as data disks. Rather, we could treat each 
disk as the redundant disk for some of the blocks. This improvement is often 
called RAID level 5.
For instance, if there are n + 1 disks numbered 0 through n, we could treat 
the ith cylinder of disk j as redundant if j is the remainder when i is divided 
by n + 1.
E xam ple 13.12: In our running example, n = 3 so there are 4 disks. The 
first disk, numbered 0, is redundant for its cylinders numbered 4, 8, 12, and so 
on, because these are the numbers that leave remainder 0 when divided by 4. 
The disk numbered 1 is redundant for blocks numbered 1, 5, 9, and so on; disk 
2 is redundant for blocks 2, 6, 1 0 ,..., and disk 3 is redundant for 3, 7, 1 1 ,... .
As a result, the reading and writing load for each disk is the same. If all 
blocks are equally likely to be written, then for one write, each disk has a 1/4 
chance that the block is on that disk. If not, then it has a 1/3 chance that 
it will be the redundant disk for that block. Thus, each of the four disks is 
involved in 1/4 + (3/4) x (1/3) = 1/2 of the writes. □
584 CHAPTER 13. SECONDARY STORAGE M ANAGEM ENT
13.4.9 Coping W ith M ultiple Disk Crashes
There is a theory of error-correcting codes that allows us to deal with any 
number of disk crashes — data or redundant — if we use enough redundant 
disks. This strategy leads to the highest RAID “level,” RAID level 6. We 
shall give only a simple example here, where two simultaneous crashes are 
correctable, and the strategy is based on the simplest error-correcting code, 
known as a Hamming code.
In our description we focus on a system with seven disks, numbered 1 
through 7. The first four are data disks, and disks 5 through 7 are redundant. The relationship between data and redundant disks is summarized by 
the 3 x 7 matrix of 0’s and l ’s in Fig. 13.10. Notice that:
a) Every possible column of three 0’s and l ’s, except for the all-0 column, 
appears in the matrix of Fig. 13.10.
b) The columns for the redundant disks have a single 1.
c) The columns for the data disks each have at least two l ’s.
Data Redundant
Disk number 1 2 3 4 5 6 7
1 1 1 0 1 0 0
1 1 0 1 0 1 0
1 0 1 1 0 0 1
Figure 13.10: Redundancy pattern for a system that can recover from two 
simultaneous disk crashes
The meaning of each of the three rows of 0’s and l ’s is that if we look at 
the corresponding bits from all seven disks, and restrict our attention to those 
disks that have 1 in that row, then the modulo-2 sum of these bits must be 0. 
Put another way, the disks with 1 in a given row of the matrix are treated as 
if they were the entire set of disks in a RAID level 4 scheme. Thus, we can 
compute the bits of one of the redundant disks by finding the row in which that 
disk has 1, and talcing the modulo-2 sum of the corresponding bits of the other 
disks that have 1 in the same row.
For the matrix of Fig. 13.10, this rule implies:
1. The bits of disk 5 are the modulo-2 sum of the corresponding bits of disks 
1, 2, and 3.
2. The bits of disk 6 are the modulo-2 sum of the corresponding bits of disks 
1, 2, and 4.
13.4. DISK FAILURES 585
3. The bits of disk 7 are the modulo-2 sum of the corresponding bits of disks 
1, 3, and 4.
We shall see shortly that the particular choice of bits in this matrix gives us a 
simple rule by which we can recover from two simultaneous disk crashes.
R eading
We may read data from any data disk normally. The redundant disks can be 
ignored.
W riting
The idea is similar to the writing strategy outlined in Section 13.4.8, but now 
several redundant disks may be involved. To write a block of some data disk, 
we compute the modulo-2 sum of the new and old versions of that block. These 
bits are then added, in a modulo-2 sum, to the corresponding blocks of all those 
redundant disks that have 1 in a row in which the written disk also has 1.
E xam ple 13.13: Let us again assume that blocks are only eight bits long, 
and focus on the first blocks of the seven disks involved in our RAID level 6 
example. First, suppose the data and redundant first blocks are as given in 
Fig. 13.11. Notice that the block for disk 5 is the modulo-2 sum of the blocks 
for the first three disks, the sixth row is the modulo-2 sum of rows 1, 2, and 4, 
and the last row is the modulo-2 sum of rows 1, 3, and 4.
Disk Contents
1) 11110000
2) 10101010
3) 00111000
4) 01000001
5) 01100010
6) 00011011
7) 10001001
Figure 13.11: First blocks of all disks
Suppose we rewrite the first block of disk 2 to be 00001111. If we sum this 
sequence of bits modulo-2 with the sequence 10101010 that is the old value of 
this block, we get 10100101. If we look at the column for disk 2 in Fig. 13.10, 
we find that this disk has l ’s in the first two rows, but not the third. Since 
redundant disks 5 and 6 have 1 in rows 1 and 2, respectively, we must perform 
the sum modulo-2 operation on the current contents of their first blocks and 
the sequence 10100101 just calculated. That is, we flip the values of positions 1,
3, 6, and 8 of these two blocks. The resulting contents of the first blocks of all
586 CHAPTER 13. SECONDARY STORAGE MANAGEMENT
disks is shown in Fig. 13.12. Notice that the new contents continue to satisfy the 
constraints implied by Fig. 13.10: the modulo-2 sum of corresponding blocks 
that have 1 in a particular row of the matrix of Fig. 13.10 is still all 0’s. □
Disk Contents
1) 11110000
2) 00001111
3) 00111000
4) 01000001
5) 11000111
6) 10111110
7) 10001001
Figure 13.12: First blocks of all disks after rewriting disk 2 and changing the 
redundant disks
Failure R ecovery
Now, let us see how the redundancy scheme outlined above can be used to 
correct up to two simultaneous disk crashes. Let the failed disks be a and b.
Since all columns of the matrix of Fig. 13.10 are different, we must be able to 
find some row r in which the columns for a and b are different. Suppose that a
has 0 in row r, while b has 1 there.
Then we can compute the correct b by taking the modulo-2 sum of corresponding bits from all the disks other than b that have 1 in row r. Note that 
a is not among these, so none of these disks have failed. Having recomputed b,
we must recompute a, with all other disks available. Since every column of the 
matrix of Fig. 13.10 has a 1 in some row, we can use this row to recompute disk 
a by taking the modulo-2 sum of bits of those other disks with a 1 in this row.
Disk Contents
1) 11110000
2) ????????
3) 00111000
4) 01000001
5) ????????
6) 10111110
7) 10001001
Figure 13.13: Situation after disks 2 and 5 fail
13.4. DISK FAILURES 587
E xam ple 13.14: Suppose that disks 2 and 5 fail at about the same time. 
Consulting the matrix of Fig. 13.10, we find that the columns for these two 
disks differ in row 2, where disk 2 has 1 but disk 5 has 0. We may thus 
reconstruct disk 2 by taking the modulo-2 sum of corresponding bits of disks
1, 4, and 6, the other three disks with 1 in row 2. Notice that none of these 
three disks has failed. For instance, following from the situation regarding the 
first blocks in Fig. 13.12, we would initially have the data of Fig. 13.13 available 
after disks 2 and 5 failed.
If we take the modulo-2 sum of the contents of the blocks of disks 1, 4, and
6, we find that the block for disk 2 is 00001111. This block is correct as can be 
verified from Fig. 13.12. The situation is now as in Fig. 13.14.
Disk Contents
1) 11110000
2) 00001111
3) 00111000
4) 01000001
5) ????????
6) 10111110
7) 10001001
Figure 13.14: After recovering disk 2
Now, we see that disk 5’s column in Fig. 13.10 has a 1 in the first row. We 
can therefore recompute disk 5 by taking the modulo-2 sum of corresponding 
bits from disks 1, 2, and 3, the other three disks that have 1 in the first row. 
For block 1, this sum is 11000111. Again, the correctness of this calculation 
can be confirmed by Fig. 13.12. □
13.4.10 Exercises for Section 13.4
Exercise 13.4.1: Compute the parity bit for the following bit sequences:
a) 00111011.
b) 00000000.
c) 10101101.
Exercise 13.4.2: We can have two parity bits associated with a string if we 
follow the string by one bit that is a parity bit for the odd positions and a 
second that is the parity bit for the even positions. For each of the strings in 
Exercise 13.4.1, find the two bits that serve in this way.
588 CHAPTER 13. SECONDARY STORAGE MANAGEM ENT
Additional Observations About RAID Level 6
1. We can combine the ideas of RAID levels 5 and 6, by varying the 
choice of redundant disks according to the block or cylinder number. 
Doing so will avoid bottlenecks when writing; the scheme described 
in Section 13.4.9 will cause bottlenecks at the redundant disks.
2. The scheme described in Section 13.4.9 is not restricted to four data 
disks. The number of disks can be one less than any power of 2, say 
2k — 1. Of these disks, k are redundant, and the remaining 2k — k — 1 
are data disks, so the redundancy grows roughly as the logarithm of 
the number of data disks. For any k , we can construct the matrix 
corresponding to Fig. 13.10 by writing all possible columns of k 0’s 
and l ’s, except the all-O’s column. The columns with a single 1 
correspond to the redundant disks, and the columns with more than 
one 1 are the data disks.
E xercise 13.4.3: Suppose we use mirrored disks as in Example 13.8, the 
failure rate is 4% per year, and it takes 8 hours to replace a disk. What is the 
mean time to a disk failure involving loss of data?
! E xercise 13.4.4: Suppose that a disk has probability F of failing in a given 
year, and it takes H hours to replace a disk.
a) If we use mirrored disks, what is the mean time to data loss, as a function 
of F and H I
b) If we use a RAID level 4 or 5 scheme, with N disks, what is the mean 
time to data loss?
!! E xercise 13.4.5: Suppose we use three disks as a mirrored group; i.e., all 
three hold identical data. If the yearly probability of failure for one disk is F,
and it takes H hours to restore a disk, what is the mean time to data loss?
E xercise 13.4.6: Suppose we are using a RAID level 4 scheme with four data 
disks and one redundant disk. As in Example 13.9 assume blocks are a single 
byte. Give the block of the redundant disk if the corresponding blocks of the 
data disks are:
a) 01010110,11000000, 00111011, and 11111011.
b) 11110000, 11111000, 00111111, and 00000001.
13.4. DISK FAILURES 589
Error-Correcting Codes and RAID Level 6
There is a theory that guides our selection of a suitable matrix, like that 
of Fig. 13.10, to determine the content of redundant disks. A code of 
length n is a set of bit-vectors (called code words) of length n. The Hamming distance between two code words is the number of positions in which 
they differ, and the minimum distance of a code is the smallest Hamming 
distance of any two different code words.
If C is any code of length n, we can require that the corresponding 
bits on n disks have one of the sequences that are members of the code. As 
a very simple example, if we are using a disk and its mirror, then n = 2, 
and we can use the code C — {00,11}. That is, the corresponding bits 
of the two disks must be the same. For another example, the matrix of 
Fig. 13.10 defines the code consisting of the 16 bit-vectors of length 7 that 
have arbitrary values for the first four bits and have the remaining three 
bits determined by the rules for the three redundant disks.
If the minimum distance of a code is d, then disks whose corresponding 
bits are required to be a vector in the code will be able to tolerate d — 1 
simultaneous disk crashes. The reason is that, should we obscure d — 1 
positions of a code word, and there were two different ways these positions 
could be filled in to make a code word, then the two code words would have 
to differ in at most the d — 1 positions. Thus, the code could not have 
minimum distance d. As an example, the matrix of Fig. 13.10 actually 
defines the well-known Hamming code, which has minimum distance 3. 
Thus, it can handle two disk crashes.
E xercise 13.4.7: Using the same RAID level 4 scheme as in Exercise 13.4.6, 
suppose that data disk 1 has failed. Recover the block of that disk under the 
following circumstances:
a) The contents of disks 2 through 4 are 01010110,11000000, and 00111011, 
while the redundant disk holds 11111011.
b) The contents of disks 2 through 4 are 11110000, 11111000, and 00111111, 
while the redundant disk holds 00000001.
E xercise 13.4.8: Suppose the block on the first disk in Exercise 13.4.6 is 
changed to 10101010. What changes to the corresponding blocks on the other 
disks must be made?
Exercise 13.4.9: Suppose we have the RAID level 6 scheme of Example 13.13, 
and the blocks of the four data disks are 00111100, 11000111, 01010101, and 
10000100, respectively.
590 CHAPTER 13. SECONDARY STORAGE M ANAGEM ENT
a) What are the corresponding blocks of the redundant disks?
b) If the third disk’s block is rewritten to be 10000000, what steps must be 
taken to change other disks?
E xercise 13.4.10: Describe the steps taken to recover from the following failures using the RAID level 6 scheme with seven disks: (a) disks 1 and 7, (b) disks 
1 and 4, (c) disks 3 and 6.
13.5 Arranging D ata on Disk
We now turn to the m atter of how disks are used store databases. A data 
element such as a tuple or object is represented by a record, which consists of 
consecutive bytes in some disk block. Collections such as relations are usually 
represented by placing the records that represent their data elements in one or 
more blocks. It is normal for a disk block to hold only elements of one relation, 
although there are organizations where blocks hold tuples of several relations. 
In this section, we shall cover the basic layout techniques for both records and 
blocks.
13.5.1 Fixed-Length Records
The simplest sort of record consists of fixed-length fields, one for each attribute 
of the represented tuple. Many machines allow more efficient reading and writing of main memory when data begins at an address that is a multiple of 4 or 8; 
some even require us to do so. Thus, it is common to begin all fields at a multiple of 4 or 8, as appropriate. Space not used by the previous field is wasted. 
Note that, even though records are kept in secondary, not main, memory, they 
are manipulated in main memory. Thus it is necessary to lay out the record so 
it can be moved to main memory and accessed efficiently there.
Often, the record begins with a header, a fixed-length region where information about the record itself is kept. For example, we may want to keep in 
the record:
1. A pointer to the schema for the data stored in the record. For example, 
a tuple’s record could point to the schema for the relation to which the 
tuple belongs. This information helps us find the fields of the record.
2. The length of the record. This information helps us skip over records 
without consulting the schema.
3. Timestamps indicating the time the record was last modified, or last read. 
This information may be useful for implementing database transactions 
as will be discussed in Chapter 18.
13.5. ARRANGING DATA ON DISK 591
4. Pointers to the fields of the record. This information can substitute for 
schema information, and it will be seen to be important when we consider 
variable-length fields in Section 13.7.
CREATE TABLE M ovieStar(
name CHAR(30) PRIMARY KEY, 
address VARCHAR(255), 
gender CHAR(l), 
b irth d a te DATE
);
Figure 13.15: A SQL table declaration
E xam ple 13.15: Figure 13.15 repeats our running MovieStar schema. Let us 
assume all fields must start at a byte that is a multiple of four. Tuples of this 
relation have a header and the following four fields:
1. The first field is for name, and this field requires 30 bytes. If we assume 
that all fields begin at a multiple of 4, then we allocate 32 bytes for the 
name.
2. The next attribute is address. A VARCHAR attribute requires a fixedlength segment of bytes, with one more byte than the maximum length 
(for the string’s endmarker). Thus, we need 256 bytes for address.
3. Attribute gender is a single byte, holding either the character ’M’ or ’F’. 
We allocate 4 bytes, so the next field can start at a multiple of 4.
4. Attribute b irth d a te is a SQL DATE value, which is a 10-byte string. We 
shall allocate 12 bytes to its field, to keep subsequent records in the block 
aligned at multiples of 4.
. The header of the record will hold:
a) A pointer to the record schema.
b) The record length.
c) A timestamp indicating when the record was created.
We shall assume each of these items is 4 bytes long. Figure 13.16 shows the 
layout of a record for a MovieStar tuple. The length of the record is 316 bytes.
□
592 CHAPTER 13. SECONDARY STORAGE M ANAGEM ENT
to schem a 
length
tim estam p gender
nam e address birthdate
0 12 44
header
300304 316
Figure 13.16: Layout of records for tuples of the MovieStar relation
13.5.2 Packing Fixed-Length Records into Blocks
Records representing tuples of a relation are stored in blocks of the disk and 
moved into main memory (along with their entire block) when we need to 
access or update them. The layout of a block that holds records is suggested 
in Fig. 13.17.
header record 1 record 2 record n
Figure 13.17: A typical block holding records 
In addition to the records, there is a block header holding information such
as:
1. Links to one or more other blocks that are part of a network of blocks 
such as those that will be described in Chapter 14 for creating indexes to 
the tuples of a relation.
2. Information about the role played by this block in such a network.
3. Information about which relation the tuples of this block belong to.
4. A “directory” giving the offset of each record in the block.
5. Timestamp(s) indicating the time of the block’s last modification and/or 
access.
By fax the simplest case is when the block holds tuples from one relation, 
and the records for those tuples have a fixed format. In that case, following 
the header, we pack as many records as we can into the block and leave the 
remaining space unused.
E xam ple 13.16: Suppose we are storing records with the layout developed in 
Example 13.15. These records are 316 bytes long. Suppose also that we use 
4096-byte blocks. Of these bytes, say 12 will be used for a block header, leaving 
4084 bytes for data. In this space we can fit twelve records of the given 316-byte 
format, and 292 bytes of each block axe wasted space. □
13.6. REPRESENTING BLOCK AND RECORD ADDRESSES 593
13.5.3 Exercises for Section 13.5
E xercise 13.5.1: Suppose a record has the following fields in this order: A 
character string of length 15, an integer of 2 bytes, a SQL date, and a SQL time 
(no decimal point). How many bytes does the record take if:
a) Fields can start at any byte.
b) Fields must start at a byte that is a multiple of 4.
c) Fields must start at a byte that is a multiple of 8.
E xercise 13.5.2: Repeat Exercise 13.5.1 for the list of fields: a real of 8 bytes, 
a character string of length 17, a single byte, and a SQL date.
E xercise 13.5.3: Assume fields are as in Exercise 13.5.1, but records also have 
a record header consisting of two 4-byte pointers and a character. Calculate 
the record length for the three situations regarding field alignment (a) through
(c) in Exercise 13.5.1.
E xercise 13.5.4: Repeat Exercise 13.5.2 if the records also include a header 
consisting of an 8-byte pointer, and ten 2-byte integers.
13.6 Representing Block and Record Addresses
When in main memory, the address of a block is the virtual-memory address 
of its first byte, and the address of a record within that block is the virtualmemory address of the first byte of that record. However, in secondary storage, 
the block is not part of the application’s virtual-memory address space. Rather, 
a sequence of bytes describes the location of the block within the overall system 
of data accessible to the DBMS: the device ID for the disk, the cylinder number, 
and so on. A record can be identified by giving its block address and the offset 
of the first byte of the record within the block.
In this section, we shall begin with a discussion of address spaces, especially 
as they pertain to the common “client-server” architecture for DBMS’s (see 
Section 9.2.4). We then discuss the options for representing addresses, and 
finally look at “pointer swizzling,” the ways in which we can convert addresses 
in the data server’s world to the world of the client application programs.
13.6.1 Addresses in Client-Server Systems
Commonly, a database system consists of a server process that provides data 
from secondary storage to one or more client processes that are applications 
using the data. The server and client processes may be on one machine, or the 
server and the various clients can be distributed over many machines.
The client application uses a conventional “virtual” address space, typically 
32 bits, or about 4 billion different addresses. The operating system or DBMS
594 CHAPTER 13. SECONDARY STORAGE MANAGEMENT
decides which parts of the address space are currently located in main memory, 
and hardware maps the virtual address space to physical locations in main 
memory. We shall not think further of this virtual-to-physical translation, and 
shall think of the client address space as if it were main memory itself.
The server’s data lives in a database address space. The addresses of this 
space refer to blocks, and possibly to offsets within the block. There are several 
ways that addresses in this address space can be represented:
1. Physical Addresses. These are byte strings that let us determine the 
place within the secondary storage system where the block or record can 
be found. One or more bytes of the physical address are used to indicate 
each of:
(a) The host to which the storage is attached (if the database is stored 
across more than one machine),
(b) An identifier for the disk or other device on which the block is located,
(c) The number of the cylinder of the disk,
(d) The number of the track within the cylinder,
(e) The number of the block within the track, and
(f) (In some cases) the offset of the beginning of the record within the 
block.
2. Logical Addresses. Each block or record has a “logical address,” which is 
an arbitrary string of bytes of some fixed length. A map table, stored on 
disk in a known location, relates logical to physical addresses, as suggested 
in Fig. 13.18.
logical physical
Figure 13.18: A map table translates logical to physical addresses
Notice that physical addresses are long. Eight bytes is about the minimum 
we could use if we incorporate all the listed elements, and some systems use 
many more bytes. For example, imagine a database of objects that is designed 
to last for 100 years. In the future, the database may grow to encompass one
13.6. REPRESENTING BLOCK AND RECORD ADDRESSES 595
million machines, and each machine might be fast enough to create one object 
every nanosecond. This system would create around 277 objects, which requires 
a minimum of ten bytes to represent addresses. Since we would probably prefer 
to reserve some bytes to represent the host, others to represent the storage 
unit, and so on, a rational address notation would use considerably more than 
10 bytes for a system of this scale.
13.6.2 Logical and Structured Addresses
One might wonder what the purpose of logical addresses could be. All the information needed for a physical address is found in the map table, and following 
logical pointers to records requires consulting the map table and then going 
to the physical address. However, the level of indirection involved in the map 
table allows us considerable flexibility. For example, many data organizations 
require us to move records around, either within a block or from block to block. 
If we use a map table, then all pointers to the record refer to this map table, 
and all we have to do when we move or delete the record is to change the entry 
for that record in the table.
Many combinations of logical and physical addresses are possible as well, 
yielding structured address schemes. For instance, one could use a physical 
address for the block (but not the offset within the block), and add the key value 
for the record being referred to. Then, to find a record given this structured 
address, we use the physical part to reach the block containing that record, and 
we examine the records of the block to find the one with the proper key.
A similar, and very useful, combination of physical and logical addresses is 
to keep in each block an offset table that holds the offsets of the records within 
the block, as suggested in Fig. 13.19. Notice that the table grows from the front 
end of the block, while the records are placed starting at the end of the block. 
This strategy is useful when the records need not be of equal length. Then, we 
do not know in advance how many records the block will hold, and we do not 
have to allocate a fixed amount of the block header to the table initially.
offset
tab le- *"
— header — — unused —
Figure 13.19: A block with a table of offsets telling us the position of each 
record within the block
The address of a record is now the physical address of its block plus the offset
596 CHAPTER 13. SECONDARY STORAGE M ANAGEM ENT
of the entry in the block’s offset table for that record. This level of indirection 
within the block offers many of the advantages of logical addresses, without the 
need for a global map table.
• We can move the record around within the block, and all we have to do 
is change the record’s entry in the offset table; pointers to the record will 
still be able to find it.
• We can even allow the record to move to another block, if the offset table 
entries are large enough to hold a forwarding address for the record, giving 
its new location.
• Finally, we have an option, should the record be deleted, of leaving in its 
offset-table entry a tombstone, a special value that indicates the record has 
been deleted. Prior to its deletion, pointers to this record may have been 
stored at various places in the database. After record deletion, following 
a pointer to this record leads to the tombstone, whereupon the pointer 
can either be replaced by a null pointer, or the data structure otherwise 
modified to reflect the deletion of the record. Had we not left the tombstone, the pointer might lead to some new record, with surprising, and 
erroneous, results.
13.6.3 Pointer Swizzling
Often, pointers or addresses are part of records. This situation is not typical 
for records that represent tuples of a relation, but it is common for tuples 
that represent objects. Also, modern object-relational database systems allow 
attributes of pointer type (called references), so even relational systems need the 
ability to represent pointers in tuples. Finally, index structures are composed 
of blocks that usually have pointers within them. Thus, we need to study 
the management of pointers as blocks are moved between main and secondary 
memory.
As we mentioned earlier, every block, record, object, or other referenceable 
data item has two forms of address: its database address in the server’s address 
space, and a memory address if the item is currently copied in virtual memory. 
When in secondary storage, we surely must use the database address of the 
item. However, when the item is in the main memory, we can refer to the item 
by either its database address or its memory address. It is more efficient to put 
memory addresses wherever an item has a pointer, because these pointers can 
be followed using a single machine instruction.
In contrast, following a database address is much more time-consuming. We 
need a table that translates from all those database addresses that are currently 
in virtual memory to their current memory address. Such a translation table
is suggested in Fig. 13.20. It may look like the map table of Fig. 13.18 that 
translates between logical and physical addresses. However:
13.6. REPRESENTING BLOCK AND RECORD ADDRESSES 597
a) Logical and physical addresses are both representations for the database 
address. In contrast, memory addresses in the translation table are for 
copies of the corresponding object in memory.
b) All addressable items in the database have entries in the map table, while 
only those items currently in memory are mentioned in the translation 
table.
D Baddr m em -addr
Figure 13.20: The translation table turns database addresses into their equivalents in memory
To avoid the cost of translating repeatedly from database addresses to memory addresses, several techniques have been developed that are collectively 
known as pointer swizzling. The general idea is that when we move a block 
from secondary to main memory, pointers within the block may be “swizzled,” 
that is, translated from the database address space to the virtual address space. 
Thus, a pointer actually consists of:
1. A bit indicating whether the pointer is currently a database address or a 
(swizzled) memory address.
2. The database or memory pointer, as appropriate. The same space is used 
for whichever address form is present at the moment. Of course, not all 
the space may be used when the memory address is present, because it is 
typically shorter than the database address.
E xam ple 13.17: Figure 13.21 shows a simple situation in which the Block 1 
has a record with pointers to a second record on the same block and to a record 
on another block. The figure also shows what might happen when Block 1 
is copied to memory. The first pointer, which points within Block 1, can be 
swizzled so it points directly to the memory address of the target record.
However, if Block 2 is not in memory at this time, then we cannot swizzle the 
second pointer; it must remain unswizzled, pointing to the database address of 
its target. Should Block 2 be brought to memory later, it becomes theoretically 
possible to swizzle the second pointer of Block 1. Depending on the swizzling 
strategy used, there may or may not be a list of such pointers that are in
598 CHAPTER 13. SECONDARY STORAGE M ANAGEM ENT
memory, referring to Block 2; if so, then we have the option of swizzling the 
pointer at that time. □
D isk M em ory
B lock 2
Figure 13.21: Structure of a pointer when swizzling is used
A u to m a tic Sw izzling
There are several strategies we can use to determine when to swizzle pointers. If 
we use automatic swizzling, then as soon as a block is brought into memory, we 
locate all its pointers and addresses and enter them into the translation table 
if they are not already there. These pointers include both the pointers from
records in the block to elsewhere and the addresses of the block itself and/or 
its records, if these are addressable items. We need some mechanism to locate 
the pointers within the block. For example:
1. If the block holds records with a known schema, the schema will tell us 
where in the records the pointers are found.
2. If the block is used for one of the index structures we shall discuss in 
Chapter 14, then the block will hold pointers at known locations.
3. We may keep within the block header a list of where the pointers are.
When we enter into the translation table the addresses for the block just 
moved into memory, and/or its records, we know where in memory the block 
has been buffered. We may thus create the translation-table entry for these 
database addresses straightforwardly. When we insert one of these database 
addresses A into the translation table, we may find it in the table already, 
because its block is currently in memory. In this case, we replace A in the block
13.6. REPRESENTING BLOCK AND RECORD ADDRESSES 599
just moved to memory by the corresponding memory address, and we set the 
“swizzled” bit to true. On the other hand, if A is not yet in the translation 
table, then its block has not been copied into main memory. We therefore 
cannot swizzle this pointer and leave it in the block as a database pointer.
Suppose that during the use of this data, we follow a pointer P and we find 
that P is still unswizzled, i.e., in the form of a database pointer. We consult the 
translation table to see if database address P currently has a memory equivalent. 
If not, block B must be copied into a memory buffer. Once B is in memory, 
we can “swizzle” P by replacing its database form by the equivalent memory 
form.
Sw izzling on D em and
Another approach is to leave all pointers unswizzled when the block is first 
brought into memory. We enter its address, and the addresses of its pointers, 
into the translation table, along with their memory equivalents. If we follow a 
pointer P that is inside some block of memory, we swizzle it, using the same 
strategy that we followed when we found an unswizzled pointer using automatic 
swizzling.
The difference between on-demand and automatic swizzling is that the latter 
tries to get all the pointers swizzled quickly and efficiently when the block is 
loaded into memory. The possible time saved by swizzling all of a block’s 
pointers at one time must be weighed against the possibility that some swizzled 
pointers will never be followed. In that case, any time spent swizzling and 
unswizzling the pointer will be wasted.
An interesting option is to arrange that database pointers look like invalid 
memory addresses. If so, then we can allow the computer to follow any pointer 
as if it were in its memory form. If the pointer happens to be unswizzled, then 
the memory reference will cause a hardware trap. If the DBMS provides a 
function that is invoked by the trap, and this function “swizzles” the pointer 
in the manner described above, then we can follow swizzled pointers in single 
instructions, and only need to do something more time consuming when the 
pointer is unswizzled.
N o Sw izzling
Of course it is possible never to swizzle pointers. We still need the translation 
table, so the pointers may be followed in their unswizzled form. This approach 
does offer the advantage that records cannot be pinned in memory, as discussed 
in Section 13.6.5, and decisions about which form of pointer is present need not 
be made.
P rogram m er C ontrol o f Sw izzling
In some applications, it may be known by the application programmer whether 
the pointers in a block are likely to be followed. This programmer may be able
600 CHAPTER 13. SECONDARY STORAGE M ANAGEM ENT
to specify explicitly that a block loaded into memory is to have its pointers 
swizzled, or the programmer may call for the pointers to be swizzled only as 
needed. For example, if a programmer knows that a block is likely to be accessed 
heavily, such as the root block of a B-tree (discussed in Section 14.2), then the 
pointers would be swizzled. However, blocks that are loaded into memory, used 
once, and then likely dropped from memory, would not be swizzled.
13.6.4 Returning Blocks to Disk
When a block is moved from memory back to disk, any pointers within that 
block must be “unswizzled”; that is, their memory addresses must be replaced 
by the corresponding database addresses. The translation table can be used 
to associate addresses of the two types in either direction, so in principle it is 
possible to find, given a memory address, the database address to which the 
memory address is assigned.
However, we do not want each unswizzling operation to require a search of 
the entire translation table. While we have not discussed the implementation 
of this table, we might imagine that the table of Fig. 13.20 has appropriate 
indexes. If we think of the translation table as a relation, then the problem 
of finding the memory address associated with a database address x can be 
expressed as the query:
SELECT memAddr 
FROM T ranslationT able 
WHERE dbAddr = x;
For instance, a hash table using the database address as the key might be 
appropriate for an index on the dbAddr attribute; Chapter 14 suggests possible 
data structures.
If we want to support the reverse query,
SELECT dbAddr
FROM T ranslationT able
WHERE memAddr = y;
then we need to have an index on attribute memAddr as well. Again, Chapter 14 
suggests data structures suitable for such an index. Also, Section 13.6.5 talks 
about linked-list structures that in some circumstances can be used to go from 
a memory address to all main-memory pointers to that address.
13.6.5 Pinned Records and Blocks
A block in memory is said to be pinned if it cannot at the moment be written 
back to disk safely. A bit telling whether or not a block is pinned can be located 
in the header of the block. There are many reasons why a block could be pinned, 
including requirements of a recovery system as discussed in Chapter 17. Pointer 
swizzling introduces an important reason why certain blocks must be pinned.
13.6. REPRESENTING BLOCK AND RECORD ADDRESSES 601
If a block Bi has within it a swizzled pointer to some data item in block B 2,
then we must be very careful about moving block B2 back to disk and reusing 
its main-memory buffer. The reason is that, should we follow the pointer in 
B i, it will lead us to the buffer, which no longer holds B 2\ in effect, the pointer 
has become dangling. A block, like B 2, that is referred to by a swizzled pointer 
from somewhere else is therefore pinned.
When we write a block back to disk, we not only need to “unswizzle” any 
pointers in that block. We also need to make sure it is not pinned. If it is 
pinned, we must either unpin it, or let the block remain in memory, occupying 
space that could otherwise be used for some other block. To unpin a block 
that is pinned because of swizzled pointers from outside, we must “unswizzle” 
any pointers to it. Consequently, the translation table must record, for each 
database address whose data item is in memory, the places in memory where 
swizzled pointers to that item exist. Two possible approaches are:
1. Keep the list of references to a memory address as a linked list attached 
to the entry for that address in the translation table.
2. If memory addresses are significantly shorter than database addresses, we 
can create the linked list in the space used for the pointers themselves. 
That is, each space used for a database pointer is replaced by
(a) The swizzled pointer, and
(b) Another pointer that forms part of a linked list of all occurrences of 
this pointer.
Figure 13.22 suggests how two occurrences of a memory pointer y could be 
linked, starting at the entry in the translation table for database address 
x and its corresponding memory address y.
y !
/
y
Swizzled pointer
Translation table
Figure 13.22: A linked list of occurrences of a swizzled pointer
602 CHAPTER 13. SECONDARY STORAGE M ANAGEM ENT
13.6.6 Exercises for Section 13.6
Exercise 13.6.1: If we represent physical addresses for the Megatron 747 disk 
by allocating a separate byte or bytes to each of the cylinder, track within 
a cylinder, and block within a track, how many bytes do we need? Make a 
reasonable assumption about the maximum number of blocks on each track; 
recall that the Megatron 747 has a variable number of sectors/track.
Exercise 13.6.2: Repeat Exercise 13.6.1 for the Megatron 777 disk described 
in Exercise 13.2.1
Exercise 13.6.3: If we wish to represent record addresses as well as block 
addresses, we need additional bytes. Assuming we want addresses for a single 
Megatron 747 disk as in Exercise 13.6.1, how many bytes would we need for 
record addresses if we:
a) Included the number of the byte within a block as part of the physical 
address.
b) Used structured addresses for records. Assume that the stored records 
have a 4-byte integer as a key.
Exercise 13.6.4: Today, IP addresses have four bytes. Suppose that block 
addresses for a world-wide address system consist of an IP address for the host, 
a device number between 1 and 1000, and a block address on an individual 
device (assumed to be a Megatron 747 disk). How many bytes would block 
addresses require?
Exercise 13.6.5: In IP version 6, IP addresses are 16 bytes long. In addition, 
we may want to address not only blocks, but records, which may start at any 
byte of a block. However, devices will have their own IP address, so there will 
be no need to represent a device within a host, as we suggested was necessary 
in Exercise 13.6.4. How many bytes would be needed to represent addresses in 
these circumstances, again assuming devices were Megatron 747 disks?
Exercise 13.6.6: Suppose we wish to represent the addresses of blocks on a 
Megatron 747 disk logically, i.e., using identifiers of k bytes for some k. We also 
need to store on the disk itself a map table, as in Fig. 13.18, consisting of pairs 
of logical and physical addresses. The blocks used for the map table itself are 
not part of the database, and therefore do not have their own logical addresses 
in the map table. Assuming that physical addresses use the minimum possible 
number of bytes for physical addresses (as calculated in Exercise 13.6.1), and 
logical addresses likewise use the minimum possible number of bytes for logical 
addresses, how many blocks of 4096 bytes does the map table for the disk 
occupy?
13.7. VARIABLE-LENGTH DATA AND RECORDS 603
! Exercise 13.6.7: Suppose that we have 4096-byte blocks in which we store 
records of 100 bytes. The block header consists of an offset table, as in Fig. 
13.19, using 2-byte pointers to records within the block. On an average day, two 
records per block are inserted, and one record is deleted. A deleted record must 
have its pointer replaced by a “tombstone,” because there may be dangling 
pointers to it. For specificity, assume the deletion on any day always occurs 
before the insertions. If the block is initially empty, after how many days will 
there be no room to insert any more records?
Exercise 13.6.8: Suppose that if we swizzle all pointers automatically, we 
can perform the swizzling in half the time it would take to swizzle each one 
separately. If the probability that a pointer in main memory will be followed at 
least once is p, for what values of p is it more efficient to swizzle automatically 
than on demand?
! E xercise 13.6.9: Generalize Exercise 13.6.8 to include the possibility that we 
never swizzle pointers. Suppose that the important actions take the following 
times, in some arbitrary time units:
i. On-demand swizzling of a pointer: 30.
ii. Automatic swizzling of pointers: 20 per pointer.
Hi. Following a swizzled pointer: 1.
iv. Following an unswizzled pointer: 10.
Suppose that in-memory pointers are either not followed (probability 1 — p)
or are followed k times (probability p). For what values of k and p do noswizzling, automatic-swizzling, and on-demand-swizzling each offer the best 
average performance?
13.7 Variable-Length Data and Records
Until now, we have made the simplifying assumptions that records have a fixed 
schema, and that the schema is a list of fixed-length fields. However, in practice, 
we also may wish to represent:
1. Data items whose size varies. For instance, in Fig. 13.15 we considered a 
MovieStar relation that had an address field of up to 255 bytes. While 
there might be some addresses that long, the vast majority of them will 
probably be 50 bytes or less. We could save more than half the space used 
for storing MovieStar tuples if we used only as much space as the actual 
address needed.
2. Repeating fields. If we try to represent a many-many relationship in a 
record representing an object, we shall have to store references to as many 
objects as are related to the given object.
604 CHAPTER 13. SECONDARY STORAGE M ANAGEM ENT
3. Variable-format records. Sometimes we do not know in advance what the 
fields of a record will be, or how many occurrences of each field there 
will be. An important example is a record that represents an XML element, which might have no constraints at all, or might be allowed to have 
repeating subelements, optional attributes, and so on.
4. Enormous fields. Modern DBMS’s support attributes whose values are 
very large. For instance, a movie record might have a field that is a 2- 
gigabyte MPEG encoding of the movie itself, as well as more mundane 
fields such as the title of the movie.
13.7.1 Records W ith Variable-Length Fields
If one or more fields of a record have variable length, then the record must 
contain enough information to let us find any field of the record. A simple 
but effective scheme is to put all fixed-length fields ahead of the variable-length 
fields. We then place in the record header:
1. The length of the record.
2. Pointers to (i.e., offsets of) the beginnings of all the variable-length fields 
other than the first (which we know must immediately follow the fixedlength fields).
E xam p le 1 3 .18: Suppose we have movie-star records with name, address, 
gender, and birthdate. We shall assume that the gender and birthdate are 
fixed-length fields, taking 4 and 12 bytes, respectively. However, both name 
and address will be represented by character strings of whatever length is appropriate. Figure 13.23 suggests what a typical movie-star record would look 
like. Note that no pointer to the beginning of the name is needed; that field 
begins right after the fixed-length portion of the record. □
other header inform ation 
record length 
to address 
gender
birthdate address
Figure 13.23: A MovieStar record with name and address implemented as 
variable-length character strings
13.7. VARIABLE-LENGTH DATA AND RECORDS 605
Representing Null Values
Tuples often have fields that may be NULL. The record format of Fig. 13.23 
offers a convenient way to represent NULL values. If a field such as address 
is null, then we put a null pointer in the place where the pointer to an 
address goes. Then, we need no space for an address, except the place for 
the pointer. This arrangement can save space on average, even if address 
is a fixed-length field but frequently has the value NULL.
13.7.2 Records W ith Repeating Fields
A similar situation occurs if a record contains a variable number of occurrences 
of a field F, but the field itself is of fixed length. It is sufficient to group all 
occurrences of field F together and put in the record header a pointer to the 
first. We can locate all the occurrences of the field F as follows. Let the number 
of bytes devoted to one instance of field F be L. We then add to the offset for 
the field F all integer multiples of L, starting at 0, then L, 2L, 3L, and so on. 
Eventually, we reach the offset of the field following F or the end of the record, 
whereupon we stop.
E xam ple 13.19: Suppose we redesign our movie-star records to hold only 
the name and address (which are variable-length strings) and pointers to all 
the movies of the star. Figure 13.24 shows how this type of record could be 
represented. The header contains pointers to the beginning of the address field 
(we assume the name field always begins right after the header) and to the 
first of the movie pointers. The length of the record tells us how many movie 
pointers there are. □
other header inform ation 
record length 
to address
to m ovie pointers
address
pointers to movies
Figure 13.24: A record with a repeating group of references to movies
An alternative representation is to keep the record of fixed length, and put 
the variable-length portion — be it fields of variable length or fields that repeat
606 CHAPTER 13. SECONDARY STORAGE M ANAGEM ENT
an indefinite number of times — on a separate block. In the record itself we 
keep:
1. Pointers to the place where each repeating field begins, and
2. Either how many repetitions there are, or where the repetitions end.
Figure 13.25 shows the layout of a record for the problem of Example 13.19, 
but with the variable-length fields name and address, and the repeating field 
s ta rr e d ln (a set of movie references) kept on a separate block or blocks.
Figure 13.25: Storing variable-length fields separately from the record
There are advantages and disadvantages to using indirection for the variablelength components of a record:
• Keeping the record itself fixed-length allows records to be searched more 
efficiently, minimizes the overhead in block headers, and allows records to 
be moved within or among blocks with minimum effort.
• On the other hand, storing variable-length components on another block 
increases the number of disk I/O ’s needed to examine all components of 
a record.
A compromise strategy is to keep in the fixed-length portion of the record 
enough space for:
1. Some reasonable number of occurrences of the repeating fields,
13.7. VARIABLE-LENGTH DATA AND RECORDS 607
2. A pointer to a place where additional occurrences could be found, and
3. A count of how many additional occurrences there are.
If there are fewer than this number, some of the space would be unused. If there 
are more than can fit in the fixed-length portion, then the pointer to additional 
space will be nonnull, and we can find the additional occurrences by following 
this pointer.
13.7.3 Variable-Format Records
An even more complex situation occurs when records do not have a fixed 
schema. We mentioned an example: records that represent XML elements. 
For another example, medical records may contain information about many 
tests, but there are thousands of possible tests, and each patient has results for 
relatively few of them. If the outcome of each test is an attribute, we would 
prefer that the record for each tuple hold only the attributes for which the 
outcome is nonnull.
The simplest representation of variable-format records is a sequence of tagged
fields, each of which consists of the value of the field preceded by information 
about the role of this field, such as:
1. The attribute or field name,
2. The type of the field, if it is not apparent from the field name and some 
readily available schema information, and
3. The length of the field, if it is not apparent from the type.
E xam ple 13.20: Suppose movie stars may have additional attributes such 
as movies directed, former spouses, restaurants owned, and a number of other 
known but unusual pieces of information. In Fig. 13.26 we see the beginning of 
a hypothetical movie-star record using tagged fields. We suppose that singlebyte codes are used for the various possible field names and types. Appropriate 
codes are indicated on the figure, along with lengths for the two fields shown, 
both of which happen to be of type string. □
code for name
i
code for string type 
T length_________
code for restaurant owned 
| code for string type 
| y length_____________
n : 14 Clint Eastwood R 16 Hog's Breath Inri
Figure 13.26: A record with tagged fields
608 CHAPTER 13. SECONDARY STORAGE M ANAGEM ENT
13.7.4 Records That Do Not Fit in a Block
Today, DBMS’s frequently are used to manage datatypes with large values; 
often values do not fit in one block. Typical examples are video or audio “clips.” 
Often, these large values have a variable length, but even if the length is fixed 
for all values of the type, we need special techniques to represent values that are 
larger than blocks. In this section we shall consider a technique called “spanned 
records.” The management of extremely large values (megabytes or gigabytes) 
is addressed in Section 13.7.5.
Spanned records also are useful in situations where records are smaller than 
blocks, but packing whole records into blocks wastes significant amounts of 
space. For instance, the wasted space in Example 13.16 was only 7%, but if 
records are just slightly larger than half a block, the wasted space can approach 
50%. The reason is that then we can pack only one record per block.
The portion of a record that appears in one block is called a record fragment.
A record with two or more fragments is called spanned, and records that do not 
cross a block boundary are unspanned.
If records can be spanned, then every record and record fragment requires 
some extra header information:
1. Each record or fragment header must contain a bit telling whether or not 
it is a fragment.
2. If it is a fragment, then it needs bits telling whether it is the first or last 
fragment for its record.
3. If there is a next and/or previous fragment for the same record, then the 
fragment needs pointers to these other fragments.
E xam ple 13.21: Figure 13.27 suggests how records that were about 60% of a 
block in size could be stored with three records for every two blocks. The header 
for record fragment 2a contains an indicator that it is a fragment, an indicator 
that it is the first fragment for its record, and a pointer to next fragment, 2b.
Similarly, the header for 2b indicates it is the last fragment for its record and 
holds a back-pointer to the previous fragment 2a. □
13.7.5 BLOBs
Now, let us consider the representation of truly large values for records or fields 
of records. The common examples include images in various formats (e.g., GIF, 
or JPEG), movies in formats such as MPEG, or signals of all sorts: audio, radar, 
and so on. Such values are often called binary, large objects, or BLOBs. When 
a field has a BLOB as value, we must rethink at least two issues.
13.7. VARIABLE-LENGTH DATA AND RECORDS 609
block header 
record header
record 1
record
2 -a
record
2 -b record 3
block 1 block 2
Figure 13.27: Storing spanned records across blocks
Storage o f B L O B s
A BLOB must be stored on a sequence of blocks. Often we prefer that these 
blocks are allocated consecutively on a cylinder or cylinders of the disk, so the 
BLOB may be retrieved efficiently. However, it is also possible to store the 
BLOB on a linked list of blocks.
Moreover, it is possible that the BLOB needs to be retrieved so quickly 
(e.g., a movie that must be played in real time), that storing it on one disk 
does not allow us to retrieve it fast enough. Then, it is necessary to stripe the 
BLOB across several disks, that is, to alternate blocks of the BLOB among 
these disks. Thus, several blocks of the BLOB can be retrieved simultaneously, 
increasing the retrieval rate by a factor approximately equal to the number of 
disks involved in the striping.
R etrieval o f B L O B s
Our assumption that when a client wants a record, the block containing the 
record is passed from the database server to the client in its entirety may not 
hold. We may want to pass only the “small” fields of the record, and allow the 
client to request blocks of the BLOB one at a time, independently of the rest of 
the record. For instance, if the BLOB is a 2-hour movie, and the client requests 
that the movie be played, the BLOB could be shipped several blocks at a time 
to the client, at just the rate necessary to play the movie.
In many applications, it is also important that the client be able to request 
interior portions of the BLOB without having to receive the entire BLOB. 
Examples would be a request to see the 45th minute of a movie, or the ending 
of an audio clip. If the DBMS is to support such operations, then it requires a 
suitable index structure, e.g., an index by seconds on a movie BLOB.
13.7.6 Column Stores
An alternative to storing tuples as records is to store each column as a record. 
Since an entire column of a relation may occupy far more than a single block, 
these records may span many blocks, much as long files do. If we keep the
610 CHAPTER 13. SECONDARY STORAGE M ANAGEM ENT
values in each column in the same order, then we can reconstruct the relation 
from the column records. Alternatively, we can keep tuple ID’s or integers with 
each value, to tell which tuple the value belongs to.
E xam ple 13.22 : Consider the relation
The column for X can be represented by the record (a, c, e) and the column for 
Y can be represented by the record (b ,d ,f). If we want to indicate the tuple 
to which each value belongs, then we can represent the two columns by the 
records ((l,a), (2,c), (3,e)) and ((1,6), (2,d), (3 ,/)), respectively. No matter 
how many tuples the relation above had, the columns would be represented by 
variable-length records of values or repeating groups of tuple ID’s and values.
□
If we store relations by columns, it is often possible to compress data, the 
the values all have a known type. For example, an attribute gender in a relation 
might have type CHAR(l), but we would use four bytes in a tuple-based record, 
because it is more convenient to have all components of a tuple begin at word 
boundaries. However, if all we are storing is a sequence of gender values, then 
it would make sense to store the column by a sequence of bits. If we did so, we 
would compress the data by a factor of 32.
However, in order for column-based storage to make sense, it must be the 
case that most queries call for examination of all, or a large fraction of the values 
in each of several columns. Recall our discussion in Section 10.6 of “analytic” 
queries, which are the common kind of queries with the desired characteristic. 
These “OLAP” queries may benefit from organizing the data by columns.
13.7.7 Exercises for Section 13.7
Exercise 13.7.1: A patient record consists of the following fixed-length fields: 
the patient’s date of birth, social-security number, and patient ID, each 10 bytes 
long. It also has the following variable-length fields: name, address, and patient 
history. If pointers within a record require 4 bytes, and the record length is a 
4-byte integer, how many bytes, exclusive of the space needed for the variablelength fields, are needed for the record? You may assume that no alignment of 
fields is required.
E xercise 13.7.2: Suppose records are as in Exercise 13.7.1, and the variablelength fields name, address, and history each have a length that is uniformly 
distributed. For the name, the range is 10-50 bytes; for address it is 20-80 
bytes, and for history it is 0-1000 bytes. W hat is the average length of a 
patient record?
13.7. VARIABLE-LENGTH DATA AND RECORDS 611
The Merits of Data Compression
One might think that with storage so cheap, there is little advantage to 
compressing data. However, storing data in fewer disk blocks enables us 
to read and write the data faster, since we use fewer disk I/O ’s. When 
we need to read entire columns, then storage by compressed columns can 
result in significant speedups. However, if we want to read or write only 
a single tuple, then column-based storage can lose. The reason is that in 
order to decompress and find the value for the one tuple we want, we need 
to read the entire column. In contrast, tuple-based storage allows us to 
read only the block containing the tuple. An even more extreme case is 
when the data is not only compressed, but encrypted.
In order to make access of single values efficient, we must both compress and encrypt on a block-by-block basis. The most efficient compression methods generally perform better when they are allowed to compress 
large amounts of data as a group, and they do not lend themselves to 
block-based decompression. However, in special cases such as the compression of a gender column discussed in Section 13.7.6, we can in fact do 
block-by-block compression that is as good as possible.
E xercise 13.7.3: Suppose that the patient records of Exercise 13.7.1 are augmented by an additional repeating field that represents cholesterol tests. Each 
cholesterol test requires 16 bytes for a date and an integer result of the test. 
Show the layout of patient records if:
a) The repeating tests are kept with the record itself.
b) The tests are stored on a separate block, with pointers to them in the 
record.
E xercise 13.7.4: Starting with the patient records of Exercise 13.7.1, suppose 
we add fields for tests and their results. Each test consists of a test name, a 
date, and a test result. Assume that each such test requires 40 bytes. Also, 
suppose that for each patient and each test a result is stored with probability 
Pa) Assuming pointers and integers each require 4 bytes, what is the average 
number of bytes devoted to test results in a patient record, assuming that 
all test results are kept within the record itself, as a variable-length field?
b) Repeat (a), if test results are represented by pointers within the record 
to test-result fields kept elsewhere.
! c) Suppose we use a hybrid scheme, where room for k test results are kept 
within the record, and additional test results are found by following a
612 CHAPTER 13. SECONDARY STORAGE M ANAGEM ENT
pointer to another block (or chain of blocks) where those results are kept. 
As a function of p, what value of k minimizes the amount of storage used 
for test results?
!! d) The amount of space used by the repeating test-result fields is not the 
only issue. Let us suppose that the figure of merit we wish to minimize 
is the number of bytes used, plus a penalty of 10,000 if we have to store 
some results on another block (and therefore will require a disk I/O for 
many of the test-result accesses we need to do. Under this assumption, 
what is the best value of k as a function of p?
!! E xercise 13.7.5: Suppose blocks have 1000 bytes available for the storage of 
records, and we wish to store on them fixed-length records of length r, where 
500 < r < 1000. The value of r includes the record header, but a record 
fragment requires an additional 16 bytes for the fragment header. For what 
values of r can we improve space utilization by spanning records?
!! E xercise 13.7.6: An MPEG movie uses about one gigabyte per hour of play. 
If we carefully organized several movies on a Megatron 747 disk, how many 
could we deliver with only small delay (say 100 milliseconds) from one disk. 
Use the timing estimates of Example 13.2, but remember that you can choose 
how the movies are laid out on the disk.
13.8 Record M odifications
Insertions, deletions, and updates of records often create special problems. 
These problems are most severe when the records change their length, but 
they come up even when records and fields are all of fixed length.
13.8.1 Insertion
First, let us consider insertion of new records into a relation. If the records of 
a relation are kept in no particular order, we can just find a block with some 
empty space, or get a new block if there is none, and put the record there.
There is more of a problem when the tuples must be kept in some fixed 
order, such as sorted by their primary key (e.g., see Section 14.1.1). If we need 
to insert a new record, we first locate the appropriate block for that record. 
Suppose first that there is space in the block to put the new record. Since 
records must be kept in order, we may have to slide records around in the block 
to make space available at the proper point. If we need to slide records, then 
the block organization that we showed in Fig. 13.19, which we reproduce here 
as Fig. 13.28, is useful. Recall from our discussion in Section 13.6.2 that we 
may create an “offset table” in the header of each block, with pointers to the 
location of each record in the block. A pointer to a record from outside the 
block is a “structured address,” that is, the block address and the location of 
the entry for the record in the offset table.
13.8. RECORD MODIFICATIONS 613
offset
table
header — unused
/y ///////) record
record 4
I
record 3 2
record 1
Figure 13.28: An offset table lets us slide records within a block to make room 
for new records
If we can find room for the inserted record in the block at hand, then we 
simply slide the records within the block and adjust the pointers in the offset 
table. The new record is inserted into the block, and a new pointer to the record 
is added to the offset table for the block. However, there may be no room in 
the block for the new record, in which case we have to find room outside the 
block. There are two major approaches to solving this problem, as well as 
combinations of these approaches.
1. Find space on a “nearby” block. For example, if block Bi has no available 
space for a record that needs to be inserted in sorted order into that 
block, then look at the following block B 2 in the sorted order of the 
blocks. If there is room in B 2, move the highest record(s) of B i to B 2,
leave forwarding addresses (recall Section 13.6.2) and slide the records 
around on both blocks.
2. Create an overflow block. In this scheme, each block B has in its header 
a place for a pointer to an overflow block where additional records that 
theoretically belong in B can be placed. The overflow block for B can 
point to a second overflow block, and so on. Figure 13.29 suggests the 
structure. We show the pointer for overflow blocks as a nub on the block, 
although it is in fact part of the block header.
B lock B overflow block
for B
Figure 13.29: A block and its first overflow block
614 CHAPTER 13. SECONDARY STORAGE M ANAGEM ENT
13.8.2 Deletion
When we delete a record, we may be able to reclaim its space. If we use an 
offset table as in Fig. 13.28 and records can slide around the block, then we 
can compact the space in the block so there is always one unused region in the 
center, as suggested by that figure.
If we cannot slide records, we should maintain an available-space list in the 
block header. Then we shall know where, and how large, the available regions 
are, when a new record is inserted into the block. Note that the block header 
normally does not need to hold the entire available space list. It is sufficient to 
put the list head in the block header, and use the available regions themselves 
to hold the links in the list, much as we did in Fig. 13.22.
There is one additional complication involved in deletion, which we must 
remember regardless of what scheme we use for reorganizing blocks. There 
may be pointers to the deleted record, and if so, we don’t want these pointers 
to dangle or wind up pointing to a new record that is put in the place of the 
deleted record. The usual technique, which we pointed out in Section 13.6.2, is 
to place a tombstone in place of the record. This tombstone is permanent; it 
must exist until the entire database is reconstructed.
Where the tombstone is placed depends on the nature of record pointers. 
If pointers go to fixed locations from which the location of the record is found, 
then we put the tombstone in that fixed location. Here are two examples:
1. We suggested in Section 13.6.2 that if the offset-table scheme of Fig. 13.28 
were used, then the tombstone could be a null pointer in the offset table, 
since pointers to the record were really pointers to the offset table entries.
2. If we are using a map table, as in Fig. 13.18, to translate logical record 
addresses to physical addresses, then the tombstone can be a null pointer 
in place of the physical address.
If we need to replace records by tombstones, we should place the bit that serves 
as a tombstone at the very beginning of the record. Then, only this bit must 
remain where the record used to begin, and subsequent bytes can be reused for 
another record, as suggested by Fig. 13.30.
Figure 13.30: Record 1 can be replaced, but the tombstone remains; record 2 
has no tombstone and can be seen when we follow a pointer to it
13.9. SUMM ARY OF CHAPTER 13 615
13.8.3 Update
When a fixed-length record is updated, there is no effect on the storage system, 
because we know it can occupy exactly the same space it did before the update. 
However, when a variable-length record is updated, we have all the problems 
associated with both insertion and deletion, except that it is never necessary to 
create a tombstone for the old version of the record.
If the updated record is longer than the old version, then we may need 
to create more space on its block. This process may involve sliding records 
or even the creation of an overflow block. If variable-length portions of the 
record are stored on another block, as in Fig. 13.25, then we may need to move 
elements around that block or create a new block for storing variable-length 
fields. Conversely, if the record shrinks because of the update, we have the 
same opportunities as with a deletion to recover or consolidate space.
13.8.4 Exercises for Section 13.8
Exercise 13.8.1: Relational database systems have always preferred to use 
fixed-length tuples if possible. Give three reasons for this preference.
13.9 Summary of Chapter 13
♦ Memory Hierarchy: A computer system uses storage components ranging 
over many orders of magnitude in speed, capacity, and cost per bit. From 
the smallest/most expensive to largest/cheapest, they are: cache, main 
memory, secondary memory (disk), and tertiary memory.
♦ Disks/Secondary Storage: Secondary storage devices are principally magnetic disks with multigigabyte capacities. Disk units have several circular 
platters of magnetic material, with concentric tracks to store bits. Platters rotate around a central spindle. The tracks at a given radius from 
the center of a platter form a cylinder.
♦ Blocks and Sectors: Tracks are divided into sectors, which are separated 
by unmagnetized gaps. Sectors are the unit of reading and writing from 
the disk. Blocks are logical units of storage used by an application such 
as a DBMS. Blocks typically consist of several sectors.
♦ Disk Controller: The disk controller is a processor that controls one or 
more disk units. It is responsible for moving the disk heads to the proper 
cylinder to read or write a requested track. It also may schedule competing 
requests for disk access and buffers the blocks to be read or written.
♦ Disk Access Time: The latency of a disk is the time between a request to 
read or write a block, and the time the access is completed. Latency is 
caused principally by three factors: the seek time to move the heads to
the proper cylinder, the rotational latency during which the desired block 
rotates under the head, and the transfer time, while the block moves under 
the head and is read or written.
♦ Speeding Up Disk Access: There are several techniques for accessing disk 
blocks faster for some applications. They include dividing the data among 
several disks (striping), mirroring disks (maintaining several copies of the 
data, also to allow parallel access), and organizing data that will be accessed together by tracks or cylinders.
♦ Elevator Algorithm: We can also speed accesses by queueing access requests and handling them in an order that allows the heads to make one 
sweep across the disk. The heads stop to handle a request each time 
it reaches a cylinder containing one or more blocks with pending access 
requests.
♦ Disk Failure Modes: To avoid loss of data, systems must be able to handle 
errors. The principal types of disk failure are intermittent (a read or write 
error that will not reoccur if repeated), permanent (data on the disk is 
corrupted and cannot be properly read), and the disk crash, where the 
entire disk becomes unreadable.
♦ Checksums: By adding a parity check (extra bit to make the number of 
l ’s in a bit string even), intermittent failures and permanent failures can 
be detected, although not corrected.
♦ Stable Storage: By making two copies of all data and being careful about 
the order in which those copies are written, a single disk can be used to 
protect against almost all permanent failures of a single sector.
♦ RAID: These schemes allow data to survive a disk crash. RAID level 
4 adds a disk whose contents are a parity check on corresponding bits 
of all other disks, level 5 varies the disk holding the parity bit to avoid 
making the parity disk a writing bottleneck. Level 6 involves the use of 
error-correcting codes and may allow survival after several simultaneous 
disk crashes.
♦ Records: Records are composed of several fields plus a record header. The 
header contains information about the record, possibly including such 
matters as a timestamp, schema information, and a record length. If the 
record has varying-length fields, the header may also help locate those 
fields.
♦ Blocks: Records are generally stored within blocks. A block header, with 
information about that block, consumes some of the space in the block, 
with the remainder occupied by one or more records. To support insertions, deletions and modifications of records, we can put in the block 
header an offset table that has pointers to each of the records in the block.
616 CHAPTER 13. SECONDARY STORAGE M ANAGEM ENT
13.10. REFERENCES FOR CHAPTER 13 617
♦ Spanned Records: Generally, a record exists within one block. However, 
if records are longer than blocks, or we wish to make use of leftover space 
within blocks, then we can break records into two or more fragments, one 
on each block. A fragment header is then needed to link the fragments of 
a record.
♦ BLOBs: Very large values, such as images and videos, are called BLOBs 
(binary, large objects). These values must be stored across many blocks 
and may require specialized storage techniques such as reserving a cylinder 
or striping the blocks of the BLOB.
♦ Database Addresses: Data managed by a DBMS is found among several 
storage devices, typically disks. To locate blocks and records in this storage system, we can use physical addresses, which are a description of 
the device number, cylinder, track, sector(s), and possibly byte within a 
sector. We can also use logical addresses, which are arbitrary character 
strings that are translated into physical addresses by a map table.
♦ Pointer Swizzling: When disk blocks are brought to main memory, the 
database addresses need to be translated to memory addresses, if pointers 
are to be followed. The translation is called swizzling, and can either be 
done automatically, when blocks are brought to memory, or on-demand, 
when a pointer is first followed.
♦ Tombstones: When a record is deleted, pointers to it will dangle. A 
tombstone in place of (part of) the deleted record warns the system that 
the record is no longer there.
♦ Pinned Blocks: For various reasons, including the fact that a block may 
contain swizzled pointers, it may be unacceptable to copy a block from 
memory back to its place on disk. Such a block is said to be pinned. If the 
pinning is due to swizzled pointers, then they must be unswizzled before 
returning the block to disk.
13.10 References for Chapter 13
The RAID idea can be traced back to [8] on disk striping. The name and errorcorrecting capability is from [7]. The model of disk failures in Section 13.4 
appears in unpublished work of Lampson and Sturgis [5].
There are several useful surveys of disk-related material. A study of RAID 
systems is in [2]. [10] surveys algorithms suitable for the secondary storage 
model (block model) of computation. [3] is an important study of how one 
optimizes a system involving processor, memory, and disk, to perform specific 
tasks.
References [4] and [11] have more information on record and block structures. [9] discusses column stores as an alternative to the conventional record
618 CHAPTER 13. SECONDARY STORAGE M ANAGEM ENT
structures. Tombstones as a technique for dealing with deletion is from [6]. [1] 
covers data representation issues, such as addresses and swizzling in the context 
of object-oriented DBMS’s.
1. R. G. G. Cattell, Object Data Management, Addison-Wesley, Reading 
MA, 1994.
2. P. M. Chen et al., “RAID: high-performance, reliable secondary storage,” 
Computing Surveys 26:2 (1994), pp. 145-186.
3. J. N. Gray and F. Putzolo, “The five minute rule for trading memory for 
disk accesses and the 10 byte rule for trading memory for CPU time,” 
Proc. ACM SIGMOD Intl. Conf. on Management of Data, pp. 395-398, 
1987.
4. D. E. Knuth, The Art of Computer Programming, Vol. I, Fundamental
Algorithms, Third Edition, Addison-Wesley, Reading MA, 1997.
5. B. Lampson and H. Sturgis, “Crash recovery in a distributed data storage 
system,” Technical report, Xerox Palo Alto Research Center, 1976.
6. D. Lomet, “Scheme for invalidating free references,” IBM J. Research and
Development 19:1 (1975), pp. 26-35.
7. D. A. Patterson, G. A. Gibson, and R. H. Katz, “A case for redundant 
arrays of inexpensive disks,” Proc. ACM SIGMOD Intl. Conf. on Management of Data, pp. 109-116, 1988.
8. K. Salem and H. Garcia-Molina, “Disk striping,” Proc. Second Intl. Conf.
on Data Engineering, pp. 336-342, 1986.
9. M. Stonebraker et al., “C-Store: a column-oriented DBMS,” Proc. Thirtyfirst Intl. Conf. on Very Large Database Systems” (2005).
10. J. S. Vitter, “External memory algorithms,” Proc. Seventeenth Annual
ACM Symposium on Principles of Database Systems, pp. 119-128, 1998.
11. G. Wiederhold, File Organization for Database Design, McGraw-Hill, 
New York, 1987.
Chapter 14
Index Structures
It is not sufficient simply to scatter the records that represent tuples of a relation 
among various blocks. To see why, think how we would answer the simple query 
SELECT * FROM R. We would have to examine every block in the storage system 
to find the tuples of R. A better idea is to reserve some blocks, perhaps several 
whole cylinders, for R. Now, at least we can find the tuples of R without 
scanning the entire data store.
However, this organization offers little help for a query like
SELECT * FROM R WHERE a=10;
Section 8.4 introduced us to the importance of creating indexes to speed up 
queries that specify values for one or more attributes. As suggested in Fig. 14.1, 
an index is any data structure that takes the value of one or more fields and 
finds the records with that value “quickly.” In particular, an index lets us find 
a record without having to look at more than a small fraction of all possible 
records. The field(s) on whose values the index is based is called the search key, 
or just “key” if the index is understood.
value
m atching
records
Figure 14.1: An index takes a value for some field(s) and finds records with the 
matching value
619
620 CHAPTER 14. INDEX STRUCTURES
Different Kinds of “Keys”
There are many meanings of the term “key.” We used it in Section 2.3.6 
to mean the primary key of a relation. We shall also speak of “sort keys,” 
the attribute(s) on which a file of records is sorted. We just introduced 
“search keys,” the attribute(s) for which we are given values and asked to 
search, through an index, for tuples with matching values. We try to use 
the appropriate adjective — “primary,” “sort,” or “search” — when the 
meaning of “key” is unclear. However, in many cases, the three kinds of 
keys are one and the same.
In this chapter, we shall introduce the most common form of index in 
database systems: the B-tree. We shall also discuss hash tables in secondary 
storage, which is another important index structure. Finally, we consider other 
index structures that are designed to handle multidimensional data. These 
structures support queries that specify values or ranges for several attributes 
at once.
14.1 Index-Structure Basics
In this section, we introduce concepts that apply to all index structures. Storage structures consist of files, which are similar to the files used by operating 
systems. A data file may be used to store a relation, for example. The data file 
may have one or more index files. Each index file associates values of the search 
key with pointers to data-file records that have that value for the attribute(s) 
of the search key.
Indexes can be “dense,” meaning there is an entry in the index file for every 
record of the data file. They can be “sparse,” meaning that only some of the 
data records are represented in the index, often one index entry per block of 
the data file. Indexes can also be “primary” or “secondary.” A primary index 
determines the location of the records of the data file, while a secondary index 
does not. For example, it is common to create a primary index on the primary 
key of a relation and to create secondary indexes on some of the other attributes.
We conclude the section with a study of information retrieval from documents. The ideas of the section are combined to yield “inverted indexes,” 
which enable efficient retrieval of documents that contain one or more given 
keywords. This technique is essential for answering search queries on the Web, 
for instance.
14.1. INDEX-STRUCTURE BASICS 621
14.1.1 Sequential Files
A sequential file is created by sorting the tuples of a relation by their primary 
key. The tuples are then distributed among blocks, in this order.
E xam ple 14.1: Fig 14.2 shows a sequential file on the right. We imagine 
that keys are integers; we show only the key field, and we make the atypical 
assumption that there is room for only two records in one block. For instance, 
the first block of the file holds the records with keys 10 and 20. In this and 
several other examples, we use integers that are sequential multiples of 10 as 
keys, although there is surely no requirement that keys form an arithmetic 
sequence. □
Although in Example 14.1 we supposed that records were packed as tightly 
as possible into blocks, it is common to leave some space initially in each block to 
accomodate new tuples that may be added to a relation. Alternatively, we may 
accomodate new tuples with overflow blocks, as we suggested in Section 13.8.1.
14.1.2 Dense Indexes
If records Eire sorted, we can build on them a dense index, which is a sequence 
of blocks holding only the keys of the records and pointers to the records themselves; the pointers are addresses in the sense discussed in Section 13.6. The 
index blocks of the dense index maintain these keys in the same sorted order as 
in the file itself. Since keys and pointers presumably take much less space than 
complete records, we expect to use many fewer blocks for the index than for 
the file itself. The index is especially advantageous when it, but not the data 
file, can fit in main memory. Then, by using the index, we can find any record 
given its search key, with only one disk I/O per lookup.
E xam ple 14.2: Figure 14.2 suggests a dense index on a sorted file. The 
first index block contains pointers to the first four records (an atypically small 
number of pointers for one block), the second block has pointers to the next 
four, and so on. □
The dense index supports queries that ask for records with a given searchkey value. Given key value K , we search the index blocks for K , and when we 
find it, we follow the associated pointer to the record with key K . It might 
appear that we need to examine every block of the index, or half the blocks of 
the index, on average, before we find K . However, there are several factors that 
make the index-based search more efficient than it seems.
1. The number of index blocks is usually small compared with the number 
of data blocks.
2. Since keys are sorted, we can use binary search to find K . If there are n 
blocks of the index, we only look at log2 n of them.
622 CHAPTER 14. INDEX STRUCTURES
10
20
30
40
10
20
50
60
70
80
90
100
110 — —•
120 —
30
40
50
60
70
80
90
100
Index file Data file
Figure 14.2: A dense index (left) on a sequential data file (right)
3. The index may be small enough to be kept permanently in main memory 
buffers. If so, the search for key K involves only main-memory accesses, 
and there are no expensive disk I/O ’s to be performed.
14.1.3 Sparse Indexes
A sparse index typically has only one key-pointer pair per block of the data file. 
It thus uses less space than a dense index, at the expense of somewhat more 
time to find a record given its key. You can only use a sparse index if the data 
file is sorted by the search key, while a dense index can be used for any search 
key. Figure 14.3 shows a sparse index with one key-pointer per data block. The 
keys are for the first records on each data block.
E xam ple 14.3: As in Example 14.2, we assume that the data file is sorted, 
and keys are all the integers divisible by 10, up to some large number. We also 
continue to assume that four key-pointer pairs fit on an index block. Thus, the 
first sparse-index block has entries for the first keys on the first four blocks, 
which are 10, 30, 50, and 70. Continuing the assumed pattern of keys, the 
second index block has the first keys of the fifth through eighth blocks, which 
we assume are 90, 110, 130, and 150. We also show a third index block with 
first keys from the hypothetical ninth through twelfth data blocks. □
To find the record with search-key value K , we search the sparse index for 
the largest key less than or equal to K . Since the index file is sorted by key, a
14.1. INDEX-STRUCTURE BASICS 623
10
30
50
70 "■
90
110 •-'
130 ■-- 150 —
170 — -
190 —
210 — -
230 —
10
20
30
40
50
60
70
80
90
100
Figure 14.3: A sparse index on a sequential file
binary search can locate this entry. We follow the associated pointer to a data 
block. Now, we must search this block for the record with key K . Of course the 
block must have enough format information that the records and their contents 
can be identified. Any of the techniques from Sections 13.5 and 13.7 can be 
used.
14.1.4 Multiple Levels of Index
An index file can cover many blocks. Even if we use binary search to find the 
desired index entry, we still may need to do many disk I/O ’s to get to the record 
we want. By putting an index on the index, we can make the use of the first 
level of index more efficient.
Figure 14.4 extends Fig. 14.3 by adding a second index level (as before, we 
assume keys are every multiple of 10). The same idea would let us place a thirdlevel index on the second level, and so on. However, this idea has its limits, 
and we prefer the B-tree structure described in Section 14.2 over building many 
levels of index.
In this example, the first-level index is sparse, although we could have chosen 
a dense index for the first level. However, the second and higher levels must 
be sparse. The reason is that a dense index on an index would have exactly as 
many key-pointer pairs as the first-level index, and therefore would take exactly 
as much space as the first-level index.
624 CHAPTER 14. INDEX STRUCTURES
10
90
170
250 .
330 — -
410 — ■
490 — ■
570
10
30
50
70
90
110 — -
130 — ■
150 -
170 — -
190 — -
210 — -
230 —
10
20
30
40
50
60
70
80
90
100
Figure 14.4: Adding a second level of sparse index
14.1.5 Secondary Indexes
A secondary index serves the purpose of any index: it is a data structure that 
facilitates finding records given a value for one or more fields. However, the 
secondary index is distinguished from the primary index in that a secondary 
index does not determine the placement of records in the data file. Rather, the 
secondary index tells us the current locations of records; that location may have 
been decided by a primary index on some other field. An important consequence 
of the distinction between primary and secondary indexes is that:
• Secondary indexes are always dense. It makes no sense to talk of a sparse, 
secondary index. Since the secondary index does not influence location, 
we could not use it to predict the location of any record whose key was 
not mentioned in the index file explicitly.
E xam ple 14.4: Figure 14.5 shows a typical secondary index. The data file 
is shown with two records per block, as has been our standard for illustration. 
The records have only their search key shown; this attribute is integer valued, 
and as before we have taken the values to be multiples of 10. Notice that, unlike 
the data file in Fig. 14.2, here the data is not sorted by the search key.
However, the keys in the index file are sorted. The result is that the pointers 
in one index block can go to many different data blocks, instead of one or a few 
consecutive blocks. For example, to retrieve all the records with search key 20, 
we not only have to look at two index blocks, but we are sent by their pointers 
to three different data blocks. Thus, using a secondary index may result in
14.1. INDEX-STRUCTURE BASICS 625
Figure 14.5: A secondary index
many more disk I/O ’s than if we get the same number of records via a primary 
index. However, there is no help for this problem; we cannot control the order 
of tuples in the data block, because they are presumably ordered according to 
some other attribute(s). □
14.1.6 Applications of Secondary Indexes
Besides supporting additional indexes on relations that are organized as sequential files, there are some data structures where secondary indexes are needed for 
even the primary key. One of these is the “heap” structure, where the records 
of the relation are kept in no particular order.
A second common structure needing secondary indexes is the clustered file.
Suppose there are relations R and S, with a many-one relationship from the 
tuples of R to tuples of S. It may make sense to store each tuple of R with the 
tuple of S to which it is related, rather than according to the primary key of R.
An example will illustrate why this organization makes good sense in special 
situations.
E xam ple 14.5 : Consider our standard movie and studio relations:
M o v ie (title , y ea r, len g th , genre, studioName, producerC#) 
Studio(name, address, presC#)
Suppose further that the most common form of query is:
626 CHAPTER 14. INDEX STRUCTURES
SELECT t i t l e , year 
FROM Movie, Studio
WHERE presC# = zzz AND Movie. studioName = Studio.nam e;
Here, zzz represents any possible certificate number for a studio president. That 
is, given the president of a studio, we need to find all the movies made by that 
studio.
If we are convinced that the above query is typical, then instead of ordering 
Movie tuples by the primary key t i t l e and year, we can create a clustered
file structure for both relations Studio and Movie, as suggested by Fig. 14.6. 
Following each Studio tuple are all the Movie tuples for all the movies owned 
by that studio.
studio 1 studio 2 studio 3 studio 4
movies by
studio 1
movies by
studio 2
movies by
studio 3
movies by
studio 4
Figure 14.6: A clustered file with each studio clustered with the movies made 
by that studio
If we create an index for Studio with search key presC#, then whatever the 
value of zzz is, we can quickly find the tuple for the proper studio. Moreover, 
all the Movie tuples whose value of attribute studioName matches the value 
of name for that studio will follow the studio’s tuple in the clustered file. As 
a result, we can find the movies for this studio by making almost as few disk 
I/O ’s as possible. The reason is that the desired Movie tuples are packed 
almost as densely as possible onto the following blocks. However, an index on 
any attribute(s) of Movie would have to be a secondary index. □
14.1.7 Indirection in Secondary Indexes
There is some wasted space, perhaps a significant amount of wastage, in the 
structure suggested by Fig. 14.5. If a search-key value appears n times in the 
data file, then the value is written n times in the index file. It would be better 
if we could write the key value once for all the pointers to data records with 
that value.
A convenient way to avoid repeating values is to use a level of indirection, 
called buckets, between the secondary index file and the data file. As shown in 
Fig. 14.7, there is one pair for each search key K . The pointer of this pair goes 
to a position in a “bucket file,” which holds the “bucket” for K . Following this 
position, until the next position pointed to by the index, are pointers to all the 
records with search-key value K .
14.1. INDEX-STRUCTURE BASICS 627
Figure 14.7: Saving space by using indirection in a secondary index
E xam ple 14.6: For instance, let us follow the pointer from search key 50 
in the index file of Fig. 14.7 to the intermediate “bucket” file. This pointer 
happens to take us to the last pointer of one block of the bucket file. We search 
forward, to the first pointer of the next block. We stop at that point, because 
the next pointer of the index file, associated with search key 60, points to the 
next record in the bucket file. □
The scheme of Fig. 14.7 saves space as long as search-key values are larger 
than pointers, and the average key appears at least twice. However, even if not, 
there is an important advantage to using indirection with secondary indexes: 
often, we can use the pointers in the buckets to help answer queries without 
ever looking at most of the records in the data file. Specifically, when there are 
several conditions to a query, and each condition has a secondary index to help 
it, we can find the bucket pointers that satisfy all the conditions by intersecting 
sets of pointers in memory, and retrieving only the records pointed to by the 
surviving pointers. We thus save the I/O cost of retrieving records that satisfy 
some, but not all, of the conditions.1
E xam ple 14.7: Consider the usual Movie relation:
M o v ie (title , y ear, len g th , genre, studioName, producerC#)
1 W e also could use this pointer-intersection trick if we got the pointers directly from the
index, rather than from buckets.
628 CHAPTER 14. INDEX STRUCTURES
Suppose we have secondary indexes with indirect buckets on both studioName 
and yeax, and we are asked the query
SELECT t i t l e 
FROM Movie
WHERE studioName = ’D isney’ AND year = 2005; 
that is, find all the Disney movies made in 2005.
Buckets Buckets
for Movie tuples for
studio year
Studio Year
index index
Figure 14.8: Intersecting buckets in main memory
Figure 14.8 shows how we can answer this query using the indexes. Using 
the index on studioName, we find the pointers to all records for Disney movies, 
but we do not yet bring any of those records from disk to memory. Instead, 
using the index on year, we find the pointers to all the movies of 2005. We then 
intersect the two sets of pointers, getting exactly the movies that were made 
by Disney in 2005. Finally, we retrieve from disk all data blocks holding one or 
more of these movies, thus retrieving the minimum possible number of blocks.
□
14.1.8 Document Retrieval and Inverted Indexes
For many years, the information-retrieval community has dealt with the storage 
of documents and the efficient retrieval of documents with a given set of keywords. With the advent of the World-Wide Web and the feasibility of keeping
14.1. INDEX-STRUCTURE BASICS 629
all documents on-line, the retrieval of documents given keywords has become 
one of the largest database problems. While there are many kinds of queries 
that one can use to find relevant documents, the simplest and most common 
form can be seen in relational terms as follows:
• A document may be thought of as a tuple in a relation Doc. This relation 
has very many attributes, one corresponding to each possible word in a 
document. Each attribute is boolean — either the word is present in the 
document, or it is not. Thus, the relation schema may be thought of as
Doc(hasCat, hasDog, ... )
where hasCat is true if and only if the document has the word “cat” at 
least once.
• There is a secondary index on each of the attributes of Doc. However, 
we save the trouble of indexing those tuples for which the value of the 
attribute is FALSE; instead, the index leads us to only the documents for 
which the word is present. That is, the index has entries only for the 
search-key value TRUE.
• Instead of creating a separate index for each attribute (i.e., for each word), 
the indexes are combined into one, called an inverted index. This index uses indirect buckets for space efficiency, as was discussed in Section 14.1.7.
E xam ple 1 4 .8 : An inverted index is illustrated in Fig. 14.9. In place of a data 
file of records is a collection of documents, each of which may be stored on one 
or more disk blocks. The inverted index itself consists of a set of word-pointer 
pairs; the words are in effect the search key for the index. The inverted index 
is kept in a sequence of blocks, just like any of the indexes discussed so far.
The pointers refer to positions in a “bucket” file. For instance, we have 
shown in Fig. 14.9 the word “cat” with a pointer to the bucket file. That 
pointer leads us to the beginning of a list of pointers to all the documents that 
contain the word “cat.” We have shown some of these in the figure. Similarly, 
the word “dog” is shown leading to a list of pointers to all the documents with 
“dog.” □
Pointers in the bucket file can be:
1. Pointers to the document itself.
2. Pointers to an occurrence of the word. In this case, the pointer might 
be a pair consisting of the first block for the document and an integer 
indicating the number of the word in the document.
630 CHAPTER 14. INDEX STRUCTURES
Documents
Figure 14.9: An inverted index on documents
When we use “buckets” of pointers to occurrences of each word, we may 
extend the idea to include in the bucket array some information about each 
occurrence. Now, the bucket file itself becomes a collection of records with 
important structure. Early uses of the idea distinguished occurrences of a word 
in the title of a document, the abstract, and the body of text. With the growth 
of documents on the Web, especially documents using HTML, XML, or another 
markup language, we can also indicate the markings associated with words. 
For instance, we can distinguish words appearing in titles, headers, tables, or 
anchors, as well as words appearing in different fonts or sizes.
E xam ple 14.9: Figure 14.10 illustrates a bucket file that has been used to 
indicate occurrences of words in HTML documents. The first column indicates 
the type of occurrence, i.e., its marking, if any. The second and third columns 
are together the pointer to the occurrence. The third column indicates the document, and the second column gives the number of the word in the document.
We can use this data structure to answer various queries about documents 
without having to examine the documents in detail. For instance, suppose we 
want to find documents about dogs that compare them with cats. Without a 
deep understanding of the meaning of the text, we cannot answer this query 
precisely. However, we could get a good hint if we searched for documents that
a) Mention dogs in the title, and
14.1. INDEX-STRUCTURE BASICS 631
Type Position
Figure 14.10: Storing more information in the inverted index
Insertion and Deletion From Buckets
We show buckets in figures such as Fig. 14.9 as compacted arrays of appropriate size. In practice, they are records with a single field (the pointer) 
and are stored in blocks like any other collection of records. Thus, when 
we insert or delete pointers, we may use any of the techniques seen so far, 
such as leaving extra space in blocks for expansion of the file, overflow 
blocks, and possibly moving records within or among blocks. In the latter 
case, we must be careful to change the pointer from the inverted index to 
the bucket file, as we move the records it points to.
b) Mention cats in an anchor — presumably a link to a document about 
cats.
We can answer this query by intersecting pointers. That is, we follow the 
pointer associated with “cat” to find the occurrences of this word. We select 
from the bucket file the pointers to documents associated with occurrences of 
“cat” where the type is “anchor.” We then find the bucket entries for “dog” 
and select from them the document pointers associated with the type “title.” 
If we intersect these two sets of pointers, we have the documents that meet the 
conditions: they mention “dog” in the title and “cat” in an anchor. □
14.1.9 Exercises for Section 14.1
Exercise 14.1.1: Suppose blocks hold either three records, or ten key-pointer 
pairs. As a function of n, the number of records, how many blocks do we need 
to hold a data file and: (a) A dense index (b) A sparse index?
632 CHAPTER 14. INDEX STRUCTURES
More About Information Retrieval
There are a number of techniques for improving the effectiveness of retrieval of documents given keywords. While a complete treatment is beyond the scope of this book, here are two useful techniques:
1. Stemming. We remove suffixes to find the “stem” of each word, before entering its occurrence into the index. For example, plural nouns 
can be treated as their singular versions. Thus, in Example 14.8, the 
inverted index evidently uses stemming, since the search for word 
“dog” got us not only documents with “dog,” but also a document 
with the word “dogs.”
2. Stop words. The most common words, such as “the” or “and,” are 
called stop words and often are excluded from the inverted index. 
The reason is that the several hundred most common words appear in 
too many documents to make them useful as a way to find documents 
about specific subjects. Eliminating stop words also reduces the size 
of the inverted index significantly.
E xercise 14.1.2: Repeat Exercise 14.1.1 if blocks can hold up to 30 records 
or 200 key-pointer pairs, but neither data- nor index-blocks are allowed to be 
more than 80% full.
! E xercise 14.1.3: Repeat Exercise 14.1.1 if we use as many levels of index as 
is appropriate, until the final level of index has only one block.
! E xercise 14.1.4: Consider a clustered file organization like Fig. 14.6, and 
suppose that ten records, either studio records or movie records, will fit on 
one block. Also assume that the number of movies per studio is uniformly 
distributed between 1 and m. As a function of m, what is the average number 
of disk I/O ’s needed to retrieve a studio and all its movies? What would the 
number be if movies were randomly distributed over a large number of blocks?
E xercise 14.1.5: Suppose that blocks can hold either three records, ten keypointer pairs, or fifty pointers. Using the indirect-buckets scheme of Fig. 14.7:
a) If the average search-key value appears in 10 records, how many blocks 
do we need to hold 3000 records and its secondary index structure? How 
many blocks would be needed if we did not use buckets?
! b) If there are no constraints on the number of records that can have a given 
search-key value, what are the minimum and maximum number of blocks 
needed?
14.2. B-TREES 633
E xercise 14.1.6: On the assumptions of Exercise 14.1.5(a), what is the average number of disk I/O ’s to find and retrieve the ten records with a given 
search-key value, both with and without the bucket structure? Assume nothing 
is in memory to begin, but it is possible to locate index or bucket blocks without 
incurring additional I/O ’s beyond what is needed to retrieve these blocks into 
memory.
E xercise 14.1.7: Suppose we have a repository of 1000 documents, and we 
wish to build an inverted index with 10,000 words. A block can hold ten 
word-pointer pairs or 50 pointers to either a document or a position within 
a document. The distribution of words is Zipfian (see the box on “The Zipfian 
Distribution” in Section 16.4.3); the number of occurrences of the ith most 
frequent word is 100000/\/i, for i — 1 ,2 ,... , 10000.
a) What is the averge number of words per document?
b) Suppose our inverted index only records for each word all the documents 
that have that word. What is the maximum number of blocks we could 
need to hold the inverted index?
c) Suppose our inverted index holds pointers to each occurrence of each word. 
How many blocks do we need to hold the inverted index?
d) Repeat (b) if the 400 most common words (“stop” words) are not included 
in the index.
e) Repeat (c) if the 400 most common words are not included in the index.
E xercise 14.1.8: If we use an augmented inverted index, such as in Fig. 14.10, 
we can perform a number of other kinds of searches. Suggest how this index 
could be used to find:
a) Documents in which “cat” and “dog” appeared within five positions of 
each other in the same type of element (e.g., title, text, or anchor).
b) Documents in which “dog” followed “cat” separated by exactly one position.
c) Documents in which “dog” and “cat” both appear in the title.
14.2 B-Trees
While one or two levels of index are often very helpful in speeding up queries, 
there is a more general structure that is commonly used in commercial systems. 
This family of data structures is called B-trees, and the particular variant that 
is most often used is known as a B+ tree. In essence:
634 CHAPTER 14. INDEX STRUCTURES
• B-trees automatically maintain as many levels of index as is appropriate 
for the size of the file being indexed.
• B-trees manage the space on the blocks they use so that every block is 
between half used and completely full.
In the following discussion, we shall talk about “B-trees,” but the details will 
all be for the B+ tree variant. Other types of B-tree are discussed in exercises.
14.2.1 The Structure of B-trees
A B-tree organizes its blocks into a tree that is balanced, meaning that all paths 
from the root to a leaf have the same length. Typically, there are three layers in 
a B-tree: the root, an intermediate layer, and leaves, but any number of layers 
is possible. To help visualize B-trees, you may wish to look ahead at Figs. 14.11 
and 14.12, which show nodes of a B-tree, and Fig. 14.13, which shows an entire 
B-tree.
There is a parameter n associated with each B-tree index, and this parameter 
determines the layout of all blocks of the B-tree. Each block will have space for 
n search-key values and n + 1 pointers. In a sense, a B-tree block is similar to 
the index blocks introduced in Section 14.1.2, except that the B-tree block has 
an extra pointer, along with n key-pointer pairs. We pick n to be as large as 
will allow n -1-1 pointers and n keys to fit in one block.
E xam ple 14.10: Suppose our blocks are 4096 bytes. Also let keys be integers 
of 4 bytes and let pointers be 8 bytes. If there is no header information kept 
on the blocks, then we want to find the largest integer value of n such that 
4n + 8(n + 1) < 4096. That value is n = 340. □
There are several important rules about what can appear in the blocks of a 
B-tree:
• The keys in leaf nodes are copies of keys from the data file. These keys 
are distributed among the leaves in sorted order, from left to right.
• At the root, there are at least two used pointers.2 All pointers point to 
B-tree blocks at the level below.
• At a leaf, the last pointer points to the next leaf block to the right, i.e., 
to the block with the next higher keys. Among the other n pointers in 
a leaf block, at least \{n + 1)/2J of these pointers are used and point to 
data records; unused pointers are null and do not point anywhere. The 
*th pointer, if it is used, points to a record with the ith key.
technically, there is a possibility that the entire B-tree has only one pointer because it is
an index into a data file with only one record. In this case, the entire tree is a root block that
is also a leaf, and this block has only one key and one pointer. W e shall ignore this trivial
case in the descriptions that follow.
14.2. B-TREES 635
• At an interior node, all n + 1 pointers can be used to point to B-tree 
blocks at the next lower level. At least [(n + 1)/2] of them are actually 
used (but if the node is the root, then we require only that at least 2 be 
used, regardless of how large n is). If j pointers are used, then there will 
be j — 1 keys, say K \,K 2 , ■. ■ The first pointer points to a part 
of the B-tree where some of the records with keys less than K i will be 
found. The second pointer goes to that part of the tree where all records 
with keys that are at least K\, but less than will be found, and so 
on. Finally, the jth pointer gets us to the part of the B-tree where some 
of the records with keys greater than or equal to K j- i are found. Note 
that some records with keys far below K\ or far above Kj-i may not be 
reachable from this block at all, but will be reached via another block at 
the same level.
• All used pointers and their keys appear at the beginning of the block, 
with the exception of the (n + l)st pointer in a leaf, which points to the 
next leaf.
To record To record To record
with key with key with key
57 81 95
Figure 14.11: A typical leaf of a B-tree
E xam ple 14.11: Our running example of B-trees will use n = 3. That is, 
blocks have room for three keys and four pointers, which are atypically small 
numbers. Keys are integers. Figure 14.11 shows a leaf that is completely used. 
There are three keys, 57, 81, and 95. The first three pointers go to records with 
these keys. The last pointer, as is always the case with leaves, points to the 
next leaf to the right in the order of keys; it would be null if this leaf were the 
last in sequence.
A leaf is not necessarily full, but in our example with n = 3, there must 
be at least two key-pointer pairs. That is, the key 95 in Fig. 14.11 might be 
missing, and if so, the third pointer would be null.
Figure 14.12 shows a typical interior node. There are three keys, 14, 52, 
and 78. There are also four pointers in this node. The first points to a part of 
the B-tree from which we can reach only records with keys less than 14 — the 
first of the keys. The second pointer leads to all records with keys between the 
first and second keys of the B-tree block; the third pointer is for those records
636 CHAPTER 14. INDEX STRUCTURES
14 52 78
/
\
To keys To keys To keys To keys
AT <14 14 < K< 52 52 < AT< 78 K > 78
Figure 14.12: A typical interior node of a B-tree
between the second and third keys of the block, and the fourth pointer lets us 
reach some of the records with keys equal to or above the third key of the block.
As with our example leaf, it is not necessarily the case that all slots for keys 
and pointers are occupied. However, with n = 3, at least the first key and the 
first two pointers must be present in an interior node. □
E xam ple 14.12: Figure 14.13 shows an entire three-level B-tree, with n = 3, 
as in Example 14.11. We have assumed that the data file consists of records 
whose keys are all the primes from 2 to 47. Notice that at the leaves, each of 
these keys appears once, in order. All leaf blocks have two or three key-pointer 
pairs, plus a pointer to the next leaf in sequence. The keys are in sorted order 
as we look across the leaves from left to right.
The root has only two pointers, the minimum possible number, although it 
could have up to four. The one key at the root separates those keys reachable 
via the first pointer from those reachable via the second. That is, keys up to
12 could be found in the first subtree of the root, and keys 13 and up are in the 
second subtree.
14.2. B-TREES 637
If we look at the first child of the root, with key 7, we again find two pointers, 
one to keys less than 7 and the other to keys 7 and above. Note that the second 
pointer in this node gets us only to keys 7 and 11, not to all keys > 7, such as 
13.
Finally, the second child of the root has all four pointer slots in use. The 
first gets us to some of the keys less than 23, namely 13, 17, and 19. The second 
pointer gets us to all keys K such that 23 < K <31; the third pointer lets us 
reach all keys K such that 31 < K < 43, and the fourth pointer gets us to some 
of the keys > 43 (in this case, to all of them). □
14.2.2 Applications of B-trees
The B-tree is a powerful tool for building indexes. The sequence of pointers at 
the leaves of a B-tree can play the role of any of the pointer sequences coming 
out of an index file that we learned about in Section 14.1. Here are some 
examples:
1. The search key of the B-tree is the primary key for the data file, and the 
index is dense. That is, there is one key-pointer pair in a leaf for every 
record of the data file. The data file may or may not be sorted by primary 
key.
2. The data file is sorted by its primary key, and the B-tree is a sparse index 
with one key-pointer pair at a leaf for each block of the data file.
3. The data file is sorted by an attribute that is not a key, and this attribute 
is the search key for the B-tree. For each key value K that appears in the 
data file there is one key-pointer pair at a leaf. That pointer goes to the 
first of the records that have K as their sort-key value.
There are additional applications of B-tree variants that allow multiple occurrences of the search key3 at the leaves. Figure 14.14 suggests what such a 
B-tree might look like.
If we do allow duplicate occurrences of a search key, then we need to change 
slightly the definition of what the keys at interior nodes mean, which we discussed in Section 14.2.1. Now, suppose there are keys K \ ,K i ,... ,K n at an 
interior node. Then Ki will be the smallest new key that appears in the part of 
the subtree accessible from the (i + l)st pointer. By “new,” we mean that there 
are no occurrences of Ki in the portion of the tree to the left of the (i + l)st 
subtree, but at least one occurrence of Ki in that subtree. Note that in some 
situations, there will be no such key, in which case Ki can be taken to be null. 
Its associated pointer is still necessary, as it points to a significant portion of 
the tree that happens to have only one key value within it.
3 Remember that a “search key” is not necessarily a “key” in the sense of being unique.
638 CHAPTER 14. INDEX STRUCTURES
17 _
/rk
- 37 43
2 3 5 7 13 13 17 23 23 23 23 37 41 43 47
Figure 14.14: A B-tree with duplicate keys
E xam ple 14.13: Figure 14.14 shows a B-tree similar to Fig. 14.13, but with 
duplicate values. In particular, key 11 has been replaced by 13, and keys 19, 
29, and 31 have all been replaced by 23. As a result, the key at the root is 17, 
not 13. The reason is that, although 13 is the lowest key in the second subtree 
of the root, it is not a new key for that subtree, since it also appears in the first 
subtree.
We also had to make some changes to the second child of the root. The 
second key is changed to 37, since that is the first new key of the third child 
(fifth leaf from the left). Most interestingly, the first key is now null. The reason 
is that the second child (fourth leaf) has no new keys at all. Put another way, 
if we were searching for any key and reached the second child of the root, we 
would never want to start at its second child. If we are searching for 23 or 
anything lower, we want to start at its first child, where we will either find 
what we are looking for (if it is 17), or find the first of what we are looking for 
(if it is 23). Note that:
• We would not reach the second child of the root searching for 13; we would 
be directed at the root to its first child instead.
• If we are looking for any key between 24 and 36, we are directed to the 
third leaf, but when we don’t find even one occurrence of what we are 
looking for, we know not to search further right. For example, if there 
were a key 24 among the leaves, it would either be on the 4th leaf, in which 
case the null key in the second child of the root would be 24 instead, or 
it would be in the 5th leaf, in which case the key 37 at the second child 
of the root would be 24.
□
14.2. B-TREES 639
14.2.3 Lookup in B-Trees
We now revert to our original assumption that there are no duplicate keys at 
the leaves. We also suppose that the B-tree is a dense index, so every search-key 
value that appears in the data file will also appear at a leaf. These assumptions 
make the discussion of B-tree operations simpler, but is not essential for these 
operations. In particular, modifications for sparse indexes are similar to the 
changes we introduced in Section 14.1.3 for indexes on sequential files.
Suppose we have a B-tree index and we want to find a record with searchkey value K . We search for K recursively, starting at the root and ending at a 
leaf. The search procedure is:
BASIS: If we are at a leaf, look among the keys there. If the ith key is K , then 
the ith pointer will take us to the desired record.
I N DUCTION: If we are at an interior node with keys K i,K 2 , .. . ,K n, follow 
the rules given in Section 14.2.1 to decide which of the children of this node 
should next be examined. That is, there is only one child that could lead to a 
leaf with key K . If K < K \ , then it is the first child, if K \ < K < K 2 , it is the 
second child, and so on. Recursively apply the search procedure at this child.
E xam ple 14.14: Suppose we have the B-tree of Fig. 14.13, and we want to 
find a record with search key 40. We start at the root, where there is one 
key, 13. Since 13 < 40, we follow the second pointer, which leads us to the 
second-level node with keys 23, 31, and 43.
At that node, we find 31 < 40 < 43, so we follow the third pointer. We are 
thus led to the leaf with keys 31, 37, and 41. If there had been a record in the 
data file with key 40, we would have found key 40 at this leaf. Since we do not 
find 40, we conclude that there is no record with key 40 in the underlying data.
Note that had we been looking for a record with key 37, we would have 
taken exactly the same decisions, but when we got to the leaf we would find 
key 37. Since it is the second key in the leaf, we follow the second pointer, 
which will lead us to the data record with key 37. □
14.2.4 Range Queries
B-trees are useful not only for queries in which a single value of the search key 
is sought, but for queries in which a range of values are asked for. Typically, 
range queries have a term in the WHERE-clause that compares the search key 
with a value or values, using one of the comparison operators other than = or 
<>. Examples of range queries using a search-key attribute k are:
SELECT * FROM R SELECT * FROM R
WHERE R.k > 40; WHERE R.k >= 10 AND R.k <= 25;
If we want to find all keys in the range [a, 6] at the leaves of a B-tree, we do 
a lookup to find the key a. Whether or not it exists, we are led to a leaf where
640 CHAPTER 14. INDEX STRUCTURES
a could be, and we search the leaf for keys that are a or greater. Each such 
key we find has an associated pointer to one of the records whose key is in the 
desired range. As long as we do not find a key greater than b in the current 
block, we follow the pointer to the next leaf and repeat our search for keys in 
the range [a, 6],
The above search algorithm also works if b is infinite; i.e., there is only a 
lower bound and no upper bound. In that case, we search all the leaves from 
the one that would hold key a to the end of the chain of leaves. If a is — oo 
(that is, there is an upper bound on the range but no lower bound), then the 
search for “minus infinity” as a search key will always take us to the first leaf. 
The search then proceeds as above, stopping only when we pass the key b.
E xam ple 14.15: Suppose we have the B-tree of Fig. 14.13, and we Eure given 
the range (10,25) to search for. We look for key 10, which leads us to the second 
leaf. The first key is less than 10, but the second, 11, is at least 10. We follow 
its associated pointer to get the record with key 11.
Since there are no more keys in the second leaf, we follow the chain to the 
third leaf, where we find keys 13, 17, and 19. All are less than or equal to 25, 
so we follow their associated pointers and retrieve the records with these keys. 
Finally, we move to the fourth leaf, where we find key 23. But the next key 
of that leaf, 29, exceeds 25, so we are done with our search. Thus, we have 
retrieved the five records with keys 11 through 23. □
14.2.5 Insertion Into B-Trees
We see some of the advantages of B-trees over simpler multilevel indexes when 
we consider how to insert a new key into a B-tree. The corresponding record 
will be inserted into the file being indexed by the B-tree, using any of the 
methods discussed in Section 14.1; here we consider how the B-tree changes. 
The insertion is, in principle, recursive:
• We try to find a place for the new key in the appropriate leaf, and we put 
it there if there is room.
• If there is no room in the proper leaf, we split the leaf into two and divide 
the keys between the two new nodes, so each is half full or just over half 
full.
• The splitting of nodes at one level appears to the level above as if a new 
key-pointer pair needs to be inserted at that higher level. We may thus 
recursively apply this strategy to insert at the next level: if there is room, 
insert it; if not, split the parent node and continue up the tree.
• As an exception, if we try to insert into the root, and there is no room, 
then we split the root into two nodes and create a new root at the next 
higher level; the new root has the two nodes resulting from the split as 
its children. Recall that no matter how large n (the number of slots for
14.2. B-TREES 641
keys at a node) is, it is always permissible for the root to have only one 
key and two children.
When we split a node and insert it into its parent, we need to be careful how 
the keys are managed. First, suppose N is a, leaf whose capacity is n keys. Also 
suppose we are trying to insert an (n + l)st key and its associated pointer. We 
create a new node M , which will be the sibling of N , immediately to its right. 
The first |"(n + 1)/2] key-pointer pairs, in sorted order of the keys, remain with 
N , while the other key-pointer pairs move to M . Note that both nodes N and 
M are left with a sufficient number of key-pointer pairs — at least [(n + 1)/2J 
pairs.
Now, suppose N is an interior node whose capacity is n keys and n + 1 
pointers, and N has just been assigned n + 2 pointers because of a node splitting 
below. We do the following:
1. Create a new node M , which will be the sibling of N , immediately to its 
right.
2. Leave at N the first |~(n + 2)/2] pointers, in sorted order, and move to 
M the remaining [(n + 2)/2J pointers.
3. The first fri/2"| keys stay with N , while the last \n/2\ keys move to 
M . Note that there is always one key in the middle left over; it goes with 
neither N nor M . The leftover key K indicates the smallest key reachable 
via the first of M ’s children. Although this key doesn’t appear in N or 
M , it is associated with M , in the sense that it represents the smallest 
key reachable via M . Therefore K will be inserted into the parent of N
and M to divide searches between those two nodes.
E xam ple 14.16: Let us insert key 40 into the B-tree of Fig. 14.13. We find 
the proper leaf for the insertion by the lookup procedure of Section 14.2.3. As 
found in Example 14.14, the insertion goes into the fifth leaf. Since this leaf 
now has four key-pointer pairs — 31, 37, 40, and 41 — we need to split the 
leaf. Our first step is to create a new node and move the highest two keys, 40 
and 41, along with their pointers, to that node. Figure 14.15 shows this split.
Notice that although we now show the nodes on four ranks to save space, 
there are still only three levels to the tree. The seven leaves are linked by their 
last pointers, which still form a chain from left to right.
We must now insert a pointer to the new leaf (the one with keys 40 and 
41) into the node above it (the node with keys 23, 31, and 43). We must also 
associate with this pointer the key 40, which is the least key reachable through 
the new leaf. Unfortunately, the parent of the split node is already full; it has 
no room for another key or pointer. Thus, it too must be split.
We start with pointers to the last five leaves and the list of keys representing the least keys of the last four of these leaves. That is, we have pointers 
Pi, P2 , P3 , Pi, P5 to the leaves whose least keys are 13, 23, 31, 40, and 43, and
642 CHAPTER 14. INDEX STRUCTURES
Figure 14.15: Beginning the insertion of key 40
we have the key sequence 23, 31, 40, 43 to separate these pointers. The first 
three pointers and first two keys remain with the split interior node, while the 
last two pointers and last key go to the new node. The remaining key, 40, 
represents the least key accessible via the new node.
Figure 14.16 shows the completion of the insert of key 40. The root now 
has three children; the last two are the split interior node. Notice that the key 
40, which marks the lowest of the keys reachable via the second of the split 
nodes, has been installed in the root to separate the keys of the root’s second 
and third children. □
14.2.6 Deletion From B-Trees
If we are to delete a record with a given key K , we must first locate that record 
and its key-pointer pair in a leaf of the B-tree. This part of the deletion process 
is essentially a lookup, as in Section 14.2.3. We then delete the record itself 
from the data file, and we delete the key-pointer pair from the B-tree.
If the B-tree node from which a deletion occurred still has at least the 
minimum number of keys and pointers, then there is nothing more to be done.4 
However, it is possible that the node was right at the minimum occupancy 
before the deletion, so after deletion the constraint on the number of keys is
4If th e d a ta record w ith th e least key a t a leaf is deleted, th en we have th e option of raising 
th e ap p ro p riate key a t one of th e ancestors of th a t leaf, b u t th ere is no requirem ent th a t we 
do so; all searches will still go to th e ap p ro p riate leaf.
14.2. B-TREES 643
violated. We then need to do one of two things for a node N whose contents 
are subminimum; one case requires a recursive deletion up the tree:
1. If one of the adjacent siblings of node N has more than the minimum 
number of keys and pointers, then one key-pointer pair can be moved to 
N , keeping the order of keys intact. Possibly, the keys at the parent of N
must be adjusted to reflect the new situation. For instance, if the right 
sibling of N , say node M , provides an extra key and pointer, then it must 
be the smallest key that is moved from M to N . At the parent of M and 
TV, there is a key that represents the smallest key accessible via M ; that 
key must be increased to reflect the new M.
2. The hard case is when neither adjacent sibling can be used to provide 
an extra key for N . However, in that case, we have two adjacent nodes, 
N and a sibling M ; the latter has the minimum number of keys and the 
former has fewer than the minimum. Therefore, together they have no 
more keys and pointers than are allowed in a single node. We merge these 
two nodes, effectively deleting one of them. We need to adjust the keys at 
the parent, and then delete a key and pointer at the parent. If the parent 
is still full enough, then we are done. If not, then we recursively apply 
the deletion algorithm at the parent.
E xam ple 14.17: Let us begin with the original B-tree of Fig. 14.13, before the 
insertion of key 40. Suppose we delete key 7. This key is found in the second 
leaf. We delete it, its associated pointer, and the record that pointer points to.
644 CHAPTER 14. INDEX STRUCTURES
The second leaf now has only one key, and we need at least two in every 
leaf. But we are saved by the sibling to the left, the first leaf, because that 
leaf has an extra key-pointer pair. We may therefore move the highest key, 5, 
and its associated pointer to the second leaf. The resulting B-tree is shown in 
Fig. 14.17. Notice that because the lowest key in the second leaf is now 5, the 
key in the parent of the first two leaves has been changed from 7 to 5.
5 11 13 17 19 23 29 31 37 41
1
43 47
Figure 14.17: Deletion of key 7
Next, suppose we delete key 11. This deletion has the same effect on the 
second leaf; it again reduces the number of its keys below the minimum. This 
time, however, we cannot take a key from the first leaf, because the latter is 
down to the minimum number of keys. Additionally, there is no sibling to the 
right from which to take a key.5 Thus, we need to merge the second leaf with 
a sibling, namely the first leaf.
The three remaining key-pointer pairs from the first two leaves fit in one 
leaf, so we move 5 to the first leaf and delete the second leaf. The pointers 
and keys in the parent are adjusted to reflect the new situation at its children; 
specifically, the two pointers are replaced by one (to the remaining leaf) and 
the key 5 is no longer relevant and is deleted. The situation is now as shown in 
Fig. 14.18.
The deletion of a leaf has adversely affected the parent, which is the left 
child of the root. That node, as we see in Fig. 14.18, now has no keys and only 
one pointer. Thus, we try to obtain an extra key and pointer from an adjacent 
sibling. This time we have the easy case, since the other child of the root can 
afford to give up its smallest key and a pointer.
The change is shown in Fig. 14.19. The pointer to the leaf with keys 13, 17,
BN otice th a t th e leaf to th e rig h t, w ith keys 13, 17, an d 19, is n o t a sibling, because it has 
a different p aren t. W e could take a key from th a t node anyway, b u t th e n th e algorithm for 
adju stin g keys th ro u g h o u t th e tree becom es m ore com plex. W e leave th is enhancem ent as an 
exercise.
14.2. B-TREES 645
13
V—
23 31 43
\ N
13 17 19 23 29 31 37 41 43 47
Figure 14.18: Beginning the deletion of key 11
and 19 has been moved from the second child of the root to the first child. We 
have also changed some keys at the interior nodes. The key 13, which used to 
reside at the root and represented the smallest key accessible via the pointer 
that was transferred, is now needed at the first child of the root. On the other 
hand, the key 23, which used to separate the first and second children of the 
second child of the root now represents the smallest key accessible from the 
second child of the root. It therefore is placed at the root itself. □
14.2.7 Efficiency of B-Trees
B-trees allow lookup, insertion, and deletion of records using very few disk I/O ’s 
per file operation. First, we should observe that if n, the number of keys per 
block, is reasonably large, then splitting and merging of blocks will be rare 
events. Further, when such an operation is needed, it almost always is limited 
to the leaves, so only two leaves and their parent are affected. Thus, we can 
essentially neglect the disk-I/O cost of B-tree reorganizations.
However, every search for the record(s) with a given search key requires us 
to go from the root down to a leaf, to find a pointer to the record. Since we 
are only reading B-tree blocks, the number of disk I/O ’s will be the number 
of levels the B-tree has, plus the one (for lookup) or two (for insert or delete) 
disk I/O ’s needed for manipulation of the record itself. We must thus ask: 
how many levels does a B-tree have? For the typical sizes of keys, pointers, 
and blocks, three levels are sufficient for all but the largest databases. Thus, 
we shall generally take 3 as the number of levels of a B-tree. The following 
example illustrates why.
E xam ple 14.18: Recall our analysis in Example 14.10, where we determined 
that 340 key-pointer pairs could fit in one block for our example data. Suppose
646 CHAPTER 14. INDEX STRUCTURES
23
/nL
13 31 43
N
13 17 19 23 29
11
31 37 41 43 47
Figure 14.19: Completing the deletion of key 11
that the average block has an occupancy midway between the minimum and 
maximum, i.e., a typical block has 255 pointers. With a root, 255 children, 
and 2552 = 65025 leaves, we shall have among those leaves 2553, or about 16.6 
million pointers to records. That is, files with up to 16.6 million records can be 
accommodated by a 3-level B-tree. □
However, we can use even fewer than three disk I/O ’s per search through the 
B-tree. The root block of a B-tree is an excellent choice to keep permanently 
buffered in main memory. If so, then every search through a 3-level B-tree 
requires only two disk reads. In fact, under some circumstances it may make 
sense to keep second-level nodes of the B-tree buffered in main memory as well, 
reducing the B-tree search to a single disk I/O , plus whatever is necessary to 
manipulate the blocks of the data file itself.
14.2.8 Exercises for Section 14.2
Exercise 14.2.1: Suppose that blocks can hold either ten records or 99 keys 
and 100 pointers. Also assume that the average B-tree node is 70% full; i.e., it 
will have 69 keys and 70 pointers. We can use B-trees as part of several different 
structures. For each structure described below, determine (i) the total number 
of blocks needed for a 1,000,000-record file, and («) the average number of disk 
I/O ’s to retrieve a record given its search key. You may assume nothing is in 
memory initially, and the search key is the primary key for the records.
a) The data file is a sequential file, sorted on the search key, with 10 records 
per block. The B-tree is a dense index.
b) The same as (a), but the data file consists of records in no particular 
order, packed 10 to a block.
14.2. B-TREES 647
Should We Delete From B-Trees?
There are B-tree implementations that don’t fix up deletions at all. If a 
leaf has too few keys and pointers, it is allowed to remain as it is. The 
rationale is that most files grow on balance, and while there might be an 
occasional deletion that makes a leaf become subminimum, the leaf will 
probably soon grow again and attain the minimum number of key-pointer 
pairs once again.
Further, if records have pointers from outside the B-tree index, then 
we need to replace the record by a “tombstone,” and we don’t want to 
delete its pointer from the B-tree anyway. In certain circumstances, when 
it can be guaranteed that all accesses to the deleted record will go through 
the B-tree, we can even leave the tombstone in place of the pointer to the 
record at a leaf of the B-tree. Then, space for the record can be reused.
c) The same as (a), but the B-tree is a sparse index.
! d) Instead of the B-tree leaves having pointers to data records, the B-tree 
leaves hold the records themselves. A block can hold ten records, but 
on average, a leaf block is 70% full; i.e., there are seven records per leaf 
block.
e) The data file is a sequential file, and the B-tree is a sparse index, but each 
primary block of the data file has one overflow block. On average, the 
primary block is full, and the overflow block is half full. However, records 
are in no particular order within a primary block and its overflow block.
E xercise 14.2.2: Repeat Exercise 14.2.1 in the case that the query is a range 
query that is matched by 1000 records.
E xercise 14.2.3: Suppose pointers are 4 bytes long, and keys are 12 bytes 
long. How many keys and pointers will a block of 16,384 bytes have?
Exercise 14.2.4: What are the minimum numbers of keys and pointers in 
B-tree (i) interior nodes and (ii) leaves, when:
a) n = 10; i.e., a block holds 10 keys and 11 pointers.
b) n = 11; i.e., a block holds 11 keys and 12 pointers.
E xercise 14.2.5: Execute the following operations on Fig. 14.13. Describe 
the changes for operations that modify the tree.
a) Lookup the record with key 41.
b) Lookup the record with key 40.
648 CHAPTER 14. INDEX STRUCTURES
c) Lookup all records in the range 20 to 30.
d) Lookup all records with keys less than 30.
e) Lookup all records with keys greater than 30.
f) Insert a record with key 1.
g) Insert records with keys 14 through 16.
h) Delete the record with key 23.
i) Delete all the records with keys 23 and higher.
Exercise 14.2.6: When duplicate keys are allowed in a B-tree, there are some 
necessary modifications to the algorithms for lookup, insertion, and deletion 
that we described in this section. Give the changes for: (a) lookup (b) insertion
(c) deletion.
! E xercise 14.2.7: In Example 14.17 we suggested that it would be possible 
to borrow keys from a nonsibling to the right (or left) if we used a more complicated algorithm for maintaining keys at interior nodes. Describe a suitable 
algorithm that rebalances by borrowing from adjacent nodes at a level, regardless of whether they are siblings of the node that has too many or too few 
key-pointer pairs.
! Exercise 14.2.8: If we use the 3-key, 4-pointer nodes of our examples in this 
section, how many different B-trees are there when the data file has the following 
numbers of records: (a) 6 (b) 10 !! (c) 15.
! Exercise 14.2.9: Suppose we have B-tree nodes with room for three keys and 
four pointers, as in the examples of this section. Suppose also that when we 
split a leaf, we divide the pointers 2 and 2, while when we split an interior node, 
the first 3 pointers go with the first (left) node, and the last 2 pointers go with 
the second (right) node. We start with a leaf containing pointers to records 
with keys 1, 2, and 3. We then add in order, records with keys 4, 5, 6, and so 
on. At the insertion of what key will the B-tree first reach four levels?
14.3 Hash Tables
There are a number of data structures involving a hash table that are useful as 
indexes. We assume the reader has seen the hash table used as a main-memory 
data structure. In such a structure there is a hash function h that takes a search 
key (the hash key) as an argument and computes from it an integer in the range
0 to B — 1, where B is the number of buckets. A bucket array, which is an array 
indexed from 0 to B — 1, holds the headers of B linked lists, one for each bucket 
of the array. If a record has search key K , then we store the record by linking 
it to the bucket list for the bucket numbered h(K).
14.3. HASH TABLES 649
14.3.1 Secondary-Storage Hash Tables
A hash table that holds a very large number of records, so many that they must 
be kept mainly in secondary storage, differs from the main-memory version in 
small but important ways. First, the bucket array consists of blocks, rather 
than pointers to the headers of lists. Records that are hashed by the hash 
function h to a certain bucket are put in the block for that bucket. If a bucket 
has too many records, a chain of overflow blocks can be added to the bucket to 
hold more records.
We shall assume that the location of the first block for any bucket i can be 
found given i. For example, there might be a main-memory array of pointers 
to blocks, indexed by the bucket number. Another possibility is to put the first 
block for each bucket in fixed, consecutive disk locations, so we can compute 
the location of bucket i from the integer i.
0
d J
1
e J
c
b J 2
a J
3
f
Figure 14.20: A hash table
E xam ple 14.19: Figure 14.20 shows a hash table. To keep our illustrations 
manageable, we assume that a block can hold only two records, and that B = 4;
i.e., the hash function h returns values from 0 to 3. We show certain records 
populating the hash table. Keys are letters a through / in Fig. 14.20. We 
assume that h(d) = 0, h(c) - h(e) = 1, h(b) = 2, and h(a) = h (f) — 3. Thus, 
the six records are distributed into blocks as shown. □
Note that we show each block in Fig. 14.20 with a “nub” at the right end. 
This nub represents additional information in the block’s header. We shall use 
it to chain overflow blocks together, and starting in Section 14.3.5, we shall use 
it to keep other critical information about the block.
14.3.2 Insertion Into a Hash Table
When a new record with search key K must be inserted, we compute h(K ). If 
the bucket numbered h(K ) has space, then we insert the record into the block 
for this bucket, or into one of the overflow blocks on its chain if there is no room
650 CHAPTER 14. INDEX STRUCTURES
Choice of Hash Function
The hash function should “hash” the key so the resulting integer is a 
seemingly random function of the key. Thus, buckets will tend to have 
equal numbers of records, which improves the average time to access a 
record, as we shall discuss in Section 14.3.4. Also, the hash function 
should be easy to compute, since we shall compute it many times.
A common choice of hash function when keys are integers is to compute the remainder of K /B , where K is the key value and B is the number 
of buckets. Often, B is chosen to be a prime, although there are reasons 
to make B a power of 2, as we discuss starting in Section 14.3.5. For 
character-string search keys, we may treat each character as an integer, 
sum these integers, and take the remainder when the sum is divided by B.
in the first block. If none of the blocks of the chain for bucket h(K ) has room, 
we add a new overflow block to the chain and store the new record there.
E xam ple 14.20: Suppose we add to the hash table of Fig. 14.20 a record with 
key g, and h(g) = 1. Then we must add the new record to the bucket numbered
1. However, the block for that bucket already has two records. Thus, we add a 
new block and chain it to the original block for bucket 1. The record with key 
g goes in that block, as shown in Fig. 14.21. □
0
d J
1
e 4-
c
2
b J
3
a J
f
Figure 14.21: Adding an additional block to a hash-table bucket
14.3.3 Hash-Table Deletion
Deletion of the record (or records) with search key K follows the same pattern 
as insertion. We go to the bucket numbered h(K ) and search for records with 
that search key. Any that we find are deleted. If we are able to move records
14.3. HASH TABLES 651
around among blocks, then after deletion we may optionally consolidate the 
blocks of a bucket into one fewer block.6
E xam ple 14.21: Figure 14.22 shows the result of deleting the record with key 
c from the hash table of Fig. 14.21. Recall h(c) = 1, so we go to the bucket 
numbered 1 (i.e., the second bucket) and search all its blocks to find a record 
(or records if the search key were not the primary key) with key c. We find it 
in the first block of the chain for bucket 1. Since there is now room to move 
the record with key g from the second block of the chain to the first, we can do 
so and remove the second block.
0
d J
1
e J
g
b J
2
3
f J
Figure 14.22: Result of deletions from a hash table
We also show the deletion of the record with key a. For this key, we found 
our way to bucket 3, deleted it, and “consolidated” the remaining record at the 
beginning of the block. □
14.3.4 Efficiency of Hash Table Indexes
Ideally, there are enough buckets that most of them fit on one block. If so, 
then the typical lookup takes only one disk I/O , and insertion or deletion from 
the file takes only two disk I/O ’s. That number is significantly better than 
straightforward sparse or dense indexes, or B-tree indexes (although hash tables 
do not support range queries as B-trees do; see Section 14.2.4).
However, if the file grows, then we shall eventually reach a situation where 
there are many blocks in the chain for a typical bucket. If so, then we need to 
search long lists of blocks, taking at least one disk I/O per block. Thus, there 
is a good reason to try to keep the number of blocks per bucket low.
The hash tables we have examined so far are called static hash tables, because 
B, the number of buckets, never changes. However, there are several kinds of 
dynamic hash tables, where B is allowed to vary so it approximates the number
®A risk of consolidating blocks of a chain whenever possible is that an oscillation, where
we alternately insert and delete records from a bucket, will cause a block to be created or
destroyed at each step.
652 CHAPTER 14. INDEX STRUCTURES
of records divided by the number of records that can fit on a block; i.e., there 
is about one block per bucket. We shall discuss two such methods:
1. Extensible hashing in Section 14.3.5, and
2. Linear hashing in Section 14.3.7.
The first grows B by doubling it whenever it is deemed too small, and the 
second grows B by 1 each time statistics of the file suggest some growth is 
needed.
14.3.5 Extensible Hash Tables
Our first approach to dynamic hashing is called extensible hash tables. The 
major additions to the simpler static hash table structure are:
1. There is a level of indirection for the buckets. That is, an array of pointers 
to blocks represents the buckets, instead of the array holding the data 
blocks themselves.
2. The array of pointers can grow. Its length is always a power of 2, so in a 
growing step the number of buckets doubles.
3. However, there does not have to be a data block for each bucket; certain 
buckets can share a block if the total number of records in those buckets 
can fit in the block.
4. The hash function h computes for each key a sequence of k bits for some 
large k, say 32. However, the bucket numbers will at all times use some 
smaller number of bits, say i bits, from the beginning or end of this 
sequence. The bucket array will have 2* entries when * is the number of 
bits used.
E xam ple 14.22: Figure 14.23 shows a small extensible hash table. We suppose, for simplicity of the example, that k = 4; i.e., the hash function produces 
a sequence of only four bits. At the moment, only one of these bits is used, 
as indicated by * = 1 in the box above the bucket array. The bucket array 
therefore has only two entries, one for 0 and one for 1.
The bucket array entries point to two blocks. The first holds all the current 
records whose search keys hash to a bit sequence that begins with 0, and the 
second holds all those whose search keys hash to a sequence beginning with 
1. For convenience, we show the keys of records as if they were the entire bit 
sequence to which the hash function converts them. Thus, the first block holds 
a record whose key hashes to 0001, and the second holds records whose keys 
hash to 1001 and 1100. □
14.3. HASH TABLES 653
Buckets Data blocks
Figure 14.23: An extensible hash table
We should notice the number 1 appearing in the “nub” of each of the blocks 
in Fig. 14.23. This number, which would actually appear in the block header, 
indicates how many bits of the hash function’s sequence is used to determine 
membership of records in this block. In the situation of Example 14.22, there 
is only one bit considered for all blocks and records, but as we shall see, the 
number of bits considered for various blocks can differ as the hash table grows. 
That is, the bucket array size is determined by the maximum number of bits 
we are now using, but some blocks may use fewer.
14.3.6 Insertion Into Extensible Hash Tables
Insertion into an extensible hash table begins like insertion into a static hash 
table. To insert a record with search key K , we compute h(K ), take the first 
i bits of this bit sequence, and go to the entry of the bucket array indexed by 
these i bits. Note that we can determine i because it is kept as part of the data 
structure.
We follow the pointer in this entry of the bucket array and arrive at a 
block B. If there is room to put the new record in block B, we do so and we 
are done. If there is no room, then there are two possibilities, depending on 
the number j, which indicates how many bits of the hash value are used to 
determine membership in block B (recall the value of j is found in the “nub” 
of each block in figures).
1. If j < i, then nothing needs to be done to the bucket array. We:
(a) Split block B into two.
(b) Distribute records in B to the two blocks, based on the value of their 
(j + l)st bit — records whose key has 0 in that bit stay in B and 
those with 1 there go to the new block.
(c) Put j + 1 in each block’s “nub” (header) to indicate the number of 
bits used to determine membership.
(d) Adjust the pointers in the bucket array so entries that formerly 
pointed to B now point either to B or the new block, depending 
on their (j + l)st bit.
654 CHAPTER 14. INDEX STRUCTURES
Note that splitting block B may not solve the problem, since by chance 
all the records of B may go into one of the two blocks into which it was 
split. If so, we need to repeat the process on the overfull block, using the 
next higher value of j and the block that is still overfull.
2. If j — i, then we must first increment i by 1. We double the length of 
the bucket array, so it now has 2t+1 entries. Suppose w is a sequence 
of i bits indexing one of the entries in the previous bucket array. In the 
new bucket array, the entries indexed by both wO and w 1 (i.e., the two 
numbers derived from w by extending it with 0 or 1) each point to the 
same block that the w entry used to point to. That is, the two new entries 
share the block, and the block itself does not change. Membership in the 
block is still determined by whatever number of bits was previously used. 
Finally, we proceed to split block B as in case 1. Since i is now greater 
than j, that case applies.
E xam ple 14.23: Suppose we insert into the table of Fig. 14.23 a record whose 
key hashes to the sequence 1010. Since the first bit is 1, this record belongs in 
the second block. However, that block is already full, so it needs to be split. 
We find that j = i = 1 in this case, so we first need to double the bucket array, 
as shown in Fig. 14.24. We have also set i — 2 in this figure.
Figure 14.24: Now, two bits of the hash function are used
Notice that the two entries beginning with 0 each point to the block for 
records whose hashed keys begin with 0, and that block still has the integer 1 
in its “nub” to indicate that only the first bit determines membership in the 
block. However, the block for records beginning with 1 needs to be split, so we 
partition its records into those beginning 10 and those beginning 11. A 2 in 
each of these blocks indicates that two bits are used to determine membership. 
Fortunately, the split is successful; since each of the two new blocks gets at least 
one record, we do not have to split recursively.
Now suppose we insert records whose keys hash to 0000 and 0111. These 
both go in the first block of Fig. 14.24, which then overflows. Since only one bit 
is used to determine membership in this block, while i = 2, we do not have to
14.3. HASH TABLES 655
adjust the bucket array. We simply split the block, with 0000 and 0001 staying, 
and 0111 going to the new block. The entry for 01 in the bucket array is made 
to point to the new block. Again, we have been fortunate that the records did 
not all go in one of the new blocks, so we have no need to split recursively.
Figure 14.25: The hash table now uses three bits of the hash function
Now suppose a record whose key hashes to 1000 is inserted. The block for 
10 overflows. Since it already uses two bits to determine membership, it is 
time to split the bucket array again and set * = 3. Figure 14.25 shows the 
data structure at this point. Notice that the block for 10 has been split into 
blocks for 100 and 101, while the other blocks continue to use only two bits to 
determine membership. □
14.3.7 Linear Hash Tables
Extensible hash tables have some important advantages. Most significant is the 
fact that when looking for a record, we never need to search more than one data 
block. We also have to examine an entry of the bucket array, but if the bucket 
array is small enough to be kept in main memory, then there is no disk I/O 
needed to access the bucket array. However, extensible hash tables also suffer 
from some defects:
1. When the bucket array needs to be doubled in size, there is a substantial 
amount of work to be done (when i is large). This work interrupts access 
to the data file, or makes certain insertions appear to take a long time.
656 CHAPTER 14. INDEX STRUCTURES
2. When the bucket array is doubled in size, it may no longer fit in main 
memory, or may crowd out other data that we would like to hold in main 
memory. As a result, a system that was performing well might suddenly 
start using many more disk I/O ’s per operation.
3. If the number of records per block is small, then there is likely to be 
one block that needs to be split well in advance of the logical time to 
do so. For instance, if there are two records per block as in our running 
example, there might be one sequence of 20 bits that begins the keys of 
three records, even though the total number of records is much less than 
220. In that case, we would have to use i — 20 and a million-bucket array, 
even though the number of blocks holding records was much smaller than 
a million.
Another strategy, called linear hashing, grows the number of buckets more 
slowly. The principal new elements we find in linear hashing are:
• The number of buckets n is always chosen so the average number of records 
per bucket is a fixed fraction, say 80%, of the number of records that fill 
one block.
• Since blocks cannot always be split, overflow blocks are permitted, although the average number of overflow blocks per bucket will be much 
less than 1.
• The number of bits used to number the entries of the bucket array is 
["log2 n ], where n is the current number of buckets. These bits are always 
taken from the right (low-order) end of the bit sequence that is produced 
by the hash function.
• Suppose i bits of the hash function are being used to number array entries, and a record with key K is intended for bucket aia2 • • ■ a*; that is, 
a\a,2 ■ ■ • at are the last i bits of h(K). Then let a\a2 ■•■ai be m, treated 
as an i-bit binary integer. If m < n, then the bucket numbered m exists, 
and we place the record in that bucket. If n < m < 2®, then the bucket 
m does not yet exist, so we place the record in bucket m — 2*-1 , that is, 
the bucket we would get if we changed «i (which must be 1) to 0.
E xam ple 14.24: Figure 14.26 shows a linear hash table with n = 2. We 
currently are using only one bit of the hash value to determine the buckets 
of records. Following the pattern established in Example 14.22, we assume the 
hash function h produces 4 bits, and we represent records by the value produced 
by h when applied to the search key of the record.
We see in Fig. 14.26 the two buckets, each consisting of one block. The 
buckets are numbered 0 and 1. All records whose hash value ends in 0 go in 
the first bucket, and those whose hash value ends in 1 go in the second.
Also part of the structure are the parameters i (the number of bits of the 
hash function that currently are used), n (the current number of buckets), and r
14.3. HASH TABLES 657
Figure 14.26: A linear hash table
(the current number of records in the hash table). The ratio r /n will be limited 
so that the typical bucket will need about one disk block. We shall adopt the 
policy of choosing n, the number of buckets, so that there are no more than 
1.7n records in the file; i.e., r < 1.7n. That is, since blocks hold two records, 
the average occupancy of a bucket does not exceed 85% of the capacity of a 
block. □
14.3.8 Insertion Into Linear Hash Tables
When we insert a new record, we determine its bucket by the algorithm outlined 
in Section 14.3.7. We compute h(K ), where K is the key of the record, and 
we use the i bits at the end of bit sequence h(K) as the bucket number, m. If 
m < n, we put the record in bucket m, and if m > n, we put the record in 
bucket m — 2*-1 . If there is no room in the designated bucket, then we create 
an overflow block, add it to the chain for that bucket, and put the record there.
Each time we insert, we compare the current number of records r with the 
threshold ratio of r/n , and if the ratio is too high, we add the next bucket to 
the table. Note that the bucket we add bears no relationship to the bucket 
into which the insertion occurs! If the binary representation of the number of 
the bucket we add is ld2 • • • aj, then we split the bucket numbered O02 ■ ■ ■ di,
putting records into one or the other bucket, depending on their last i bits. 
Note that all these records will have hash values that end in 02 • ■ - a», and only 
the ith bit from the right end will vary.
The last important detail is what happens when n exceeds 2*. Then, i is 
incremented by 1. Technically, all the bucket numbers get an additional 0 in 
front of their bit sequences, but there is no need to make any physical change, 
since these bit sequences, interpreted as integers, remain the same.
E xam ple 14.25: We shall continue with Example 14.24 and consider what 
happens when a record whose key hashes to 0101 is inserted. Since this bit 
sequence ends in 1, the record goes into the second bucket of Fig. 14.26. There 
is room for the record, so no overflow block is created.
However, since there are now 4 records in 2 buckets, we exceed the ratio 
1.7, and we must therefore raise n to 3. Since |"log2 3] = 2, we should begin to 
think of buckets 0 and 1 as 00 and 01, but no change to the data structure is 
necessary. We add to the table the next bucket, which would have number 10. 
Then, we split the bucket 00, that bucket whose number differs from the added
658 CHAPTER 14. INDEX STRUCTURES
bucket only in the first bit. When we do the split, the record whose key hashes 
to 0000 stays in 00, since it ends with 00, while the record whose key hashes to 
1010 goes to 10 because it ends that way. The resulting hash table is shown in 
Fig. 14.27.
n=3
Figure 14.27: Adding a third bucket
Next, let us suppose we add a record whose search key hashes to 0001. 
The last two bits are 01, so we put it in this bucket, which currently exists. 
Unfortunately, the bucket’s block is full, so we add an overflow block. The three 
records are distributed among the two blocks of the bucket; we chose to keep 
them in numerical order of their hashed keys, but order is not important. Since 
the ratio of records to buckets for the table as a whole is 5/3, and this ratio is 
less than 1.7, we do not create a new bucket. The result is seen in Fig. 14.28.
Figure 14.28: Overflow blocks are used if necessary
Finally, consider the insertion of a record whose search key hashes to 0111. 
The last two bits are 11, but bucket 11 does not yet exist. We therefore redirect 
this record to bucket 01, whose number differs by having a 0 in the first bit. 
The new record fits in the overflow block of this bucket.
However, the ratio of the number of records to buckets has exceeded 1.7, so 
we must create a new bucket, numbered 11. Coincidentally, this bucket is the 
one we wanted for the new record. We split the four records in bucket 01, with 
0001 and 0101 remaining, and 0111 and 1111 going to the new bucket. Since 
bucket 01 now has only two records, we can delete the overflow block. The hash 
table is now as shown in Fig. 14.29.
Notice that the next time we insert a record into Fig. 14.29, we shall exceed
14.3. HASH TABLES 659
n=4
0000 J
0001 J
0101
1010 J
0111 J
1111
Figure 14.29: Adding a fourth bucket
the 1.7 ratio of records to buckets. Then, we shall raise n to 5 and i becomes
3. □
Lookup in a linear hash table follows the procedure we described for selecting 
the bucket in which an inserted record belongs. If the record we wish to look 
up is not in that bucket, it cannot be anywhere.
14.3.9 Exercises for Section 14.3
Exercise 14.3.1: Show what happens to the buckets in Fig. 14.20 if the following insertions and deletions occur:
i. Records g through j are inserted into buckets 0 through 3, respectively.
ii. Records a and b are deleted.
Hi. Records k through n are inserted into buckets 0 through 3, respectively.
iv. Records c and d are deleted.
Exercise 14.3.2: We did not discuss how deletions can be carried out in a 
linear or extensible hash table. The mechanics of locating the record(s) to 
be deleted should be obvious. What method would you suggest for executing 
the deletion? In particular, what are the advantages and disadvantages of 
restructuring the table if its smaller size after deletion allows for compression 
of certain blocks?
! E xercise 14.3.3: The material of this section assumes that search keys are 
unique. However, only small modifications are needed to allow the techniques 
to work for search keys with duplicates. Describe the necessary changes to 
insertion, deletion, and lookup algorithms, and suggest the major problems 
that arise when there are duplicates in each of the following kinds of hash 
tables: (a) simple (b) linear (c) extensible.
660 CHAPTER 14. INDEX STRUCTURES
! Exercise 14.3.4: Some hash functions do not work as well as theoretically 
possible. Suppose that we use the hash function on integer keys i defined by 
h(i) = i2 mod B, where B is the number of buckets.
a) What is wrong with this hash function if B — 10?
b) How good is this hash function if B = 16?
c) Are there values of B for which this hash function is useful?
Exercise 14.3.5: In an extensible hash table with n records per block, what 
is the probability that an overflowing block will have to be handled recursively;
i.e., all members of the block will go into the same one of the two blocks created 
in the split?
E xercise 14.3.6: Suppose keys are hashed to four-bit sequences, as in our 
examples of extensible and linear hashing in this section. However, also suppose 
that blocks can hold three records, rather than the two-record blocks of our 
examples. If we start with a hash table with two empty blocks (corresponding 
to 0 and 1), show the organization after we insert records with hashed keys:
a) 0000,0001,... ,1111, and the method of hashing is extensible hashing.
b) 0000,0001,... ,1111, and the method of hashing is linear hashing with a 
capacity threshold of 100%.
c) 1111,1110,..., 0000, and the method of hashing is extensible hashing.
d) 1111,1110,... , 0000, and the method of hashing is linear hashing with a 
capacity threshold of 75%.
Exercise 14.3.7: Suppose we use a linear or extensible hashing scheme, but 
there are pointers to records from outside. These pointers prevent us from moving records between blocks, as is sometimes required by these hashing methods. 
Suggest several ways that we could modify the structure to allow pointers from 
outside.
!! E xercise 14.3.8: A linear-hashing scheme with blocks that hold k records 
uses a threshold constant c, such that the current number of buckets n and 
the current number of records r are related by r = ckn. For instance, in 
Example 14.24 we used k = 2 and c = 0.85, so there were 1.7 records per 
bucket; i.e., r — 1.7n.
a) Suppose for convenience that each key occurs exactly its expected number 
of times.7 As a function of c, k, and n, how many blocks, including 
overflow blocks, are needed for the structure?
7This assumption does not mean all buckets have the same number of records, because
some buckets represent twice as many keys as others.
14.4. MULTIDIMENSIONAL INDEXES 661
b) Keys will not generally distribute equally, but rather the number of records with a given key (or suffix of a key) will be Poisson distributed. That 
is, if A is the expected number of records with a given key suffix, then 
the actual number of such records will be i with probability e~xX /i\.
Under this assumption, calculate the expected number of blocks used, as 
a function of c, k, and n.
Exercise 14.3.9: Suppose we have a file of 1,000,000 records that we want to 
hash into a table with 1000 buckets. 100 records will fit in a block, and we wish 
to keep blocks as full as possible, but not allow two buckets to share a block. 
What are the minimum and maximum number of blocks that we could need to 
store this hash table?
14.4 M ultidimensional Indexes
All the index structures discussed so far are one dimensional; that is, they 
assume a single search key, and they retrieve records that match a given searchkey value. Although the search key may involve several attributes, the onedimensional nature of indexes such as B-trees comes from the fact that values 
must be provided for all attributes of the search key, or the index is useless. So 
far in this chapter, we took advantage of a one-dimensional search-key space in 
several ways:
• Indexes on sequential files and B-trees both take advantage of having a 
single linear order for the keys.
• Hash tables require that the search key be completely known for any 
lookup. If a key consists of several fields, and even one is unknown, we 
cannot apply the hash function, but must instead search all the buckets.
In the balance of this chapter, we shall look at index structures that are suitable 
for multidimensional data. In these structures, any nonempty subset of the 
fields that form the dimensions can be given values, and some speedup will 
result.
14.4.1 Applications of Multidimensional Indexes
There are a number of applications that require us to view data as existing in a 
2-dimensional space, or sometimes in higher dimensions. Some of these applications can be supported by conventional DBMS’s, but there are also some specialized systems designed for multidimensional applications. One way in which 
these specialized systems distinguish themselves is by using data structures that 
support certain kinds of queries that are not common in SQL applications.
One important application of multidimensional indexes involves geographic 
data. A geographic information system stores objects in a (typically) twodimensional space. The objects may be points or shapes. Often, these databases
662 CHAPTER 14. INDEX STRUCTURES
are maps, where the stored objects could represent houses, roads, bridges, 
pipelines, and many other physical objects. A suggestion of such a map is 
in Fig. 14.30.
school
roadl
house2
house1
r
o
pipeline
a
d
2
0 100 
Figure 14.30: Some objects in 2-dimensional space
However, there are many other uses as well. For instance, an integratedcircuit design is a two-dimensional map of regions, often rectangles, composed 
of specific materials, called “layers.” Likewise, we can think of the windows 
and icons on a screen as a collection of objects in two-dimensional space.
The queries asked of geographic information systems are not typical of SQL 
queries, although many can be expressed in SQL with some effort. Examples 
of these types of queries are:
1. Partial match queries. We specify values for one or more dimensions and 
look for all points matching those values in those dimensions.
2. Range queries. We give ranges for one or more of the dimensions, and we 
ask for the set of points within those ranges. If shapes are represented, 
then we may ask for the shapes that are partially or wholly within the 
range. These queries generalize the one-dimensional range queries that 
we considered in Section 14.2.4.
3. Nearest-neighbor queries. We ask for the closest point to a given point. 
For instance, if points represent cities, we might want to find the city of 
over 100,000 population closest to a given small city.
4. Where-am-I queries. We are given a point and we want to know in which 
shape, if any, the point is located. A familiar example is what happens
14.4. MULTIDIMENSIONAL INDEXES 663
when you click your mouse, and the system determines which of the displayed elements you were clicking.
14.4.2 Executing Range Queries Using Conventional
Indexes
Now, let us consider to what extent one-dimensional indexes help in answering 
range queries. Suppose for simplicity that there are two dimensions, x and y.
We could put a secondary index on each of the dimensions, x and y. Using a 
B-tree for each would make it especially easy to get a range of values for each 
dimension.
Given ranges in both dimensions, we could begin by using the B-tree for x
to get pointers to all of the records in the range for x. Next, we use the B-tree 
for y to get pointers to the records for all points whose ^/-coordinate is in the 
range for y. Then, we intersect these pointers, using the idea of Section 14.1.7. 
If the pointers fit in main memory, then the total number of disk I/O ’s is the 
number of leaf nodes of each B-tree that need to be examined, plus a few I/O ’s 
for finding our way down the B-trees (see Section 14.2.7). To this amount we 
must add the disk I/O ’s needed to retrieve all the matching records, however 
many they may be.
E xam ple 14.26: Let us consider a hypothetical set of 1,000,000 points distributed randomly in a space in which both the x- and y-coordinates range from
0 to 1000. Suppose that 100 point records fit on a block, and an average B-tree 
leaf has about 200 key-pointer pairs (recall that not all slots of a B-tree block 
are necessarily occupied, at any given time). We shall assume there are B-tree 
indexes on both x and y.
Imagine we are given the range query asking for points in the square of 
side 100 surrounding the center of the space, that is, 450 < x < 550 and 
450 < y < 550. Using the B-tree for x, we can find pointers to all the records 
with x in the range; there should be about 100,000 pointers, and this number of 
pointers should fit in main memory. Similarly, we use the B-tree for y to get the 
pointers to all the records with y in the desired range; again there are about
100,000 of them. Approximately 10,000 pointers will be in the intersection 
of these two sets, and it is the records reached by the 10,000 pointers in the 
intersection that form our answer.
Now, let us estimate the number of disk I/O ’s needed to answer the range 
query. First, as we pointed out in Section 14.2.7, it is generally feasible to keep 
the root of any B-tree in main memory. Section 14.2.4 showed how to access 
the 100,000 pointers in either dimension by examining one intermediate-level 
node and all the leaves that contain the desired pointers. Since we assumed 
leaves have about 200 key-pointer pairs each, we shall have to look at about 
500 leaf blocks in each of the B-trees. When we add in one intermediate node 
per B-tree, we have a total of 1002 disk I/O ’s.
Finally, we have to retrieve the blocks containing the 10,000 desired records.
664 CHAPTER 14. INDEX STRUCTURES
If they are stored randomly, we must expect that they will be on almost 10,000 
different blocks. Since the entire file of a million records is assumed stored over
10,000 blocks, packed 100 to a block, we essentially have to look at every block 
of the data file anyway. Thus, in this example at least, conventional indexes 
have been little if any help in answering the range query. Of course, if the range 
were smaller, then constructing the intersection of the two pointer sets would 
allow us to limit the search to a fraction of the blocks in the data file. □
14.4.3 Executing Nearest-Neighbor Queries Using
Conventional Indexes
Almost any data structure we use will allow us to answer a nearest-neighbor 
query by picking a range in each dimension, asking the range query, and selecting the point closest to the target within that range. Unfortunately, there are 
two things that could go wrong:
1. There is no point within the selected range.
2. The closest point within the range might not be the closest point overall, 
as suggested by Fig. 14.31.
Possible
closer point
Figure 14.31: The point is in the range, but there could be a closer point outside 
the range
The general technique we shall use for answering nearest-neighbor queries is 
to begin by estimating a range in which the nearest point is likely to be found, 
and executing the corresponding range query. If no points are found within that 
range, we repeat with a larger range, until eventually we find at least one point. 
We then consider whether there is the possibility that a closer point exists, but 
that point is outside the range just used, as in Fig. 14.31. If so, we increase the 
range once more and retrieve all points in the larger range, to check.
14.4.4 Overview of M ultidimensional Index Structures
Most data structures for supporting queries on multidimensional data fall into 
one of two categories:
14.5. HASH STRUCTURES FOR MULTIDIMENSIONAL DATA 665
1. Hash-table-like approaches.
2. Tree-like approaches.
For each of these structures, we give up something that we have in one-dimensional index structures. With the hash-based schemes — grid files and partitioned hash functions in Section 14.5 — we no longer have the advantage that 
the answer to our query is in exactly one bucket. However, each of these schemes 
limit our search to a subset of the buckets. With the tree-based schemes, we 
give up at least one of these important properties of B-trees:
1. The balance of the tree, where all leaves are at the same level.
2. The correspondence between tree nodes and disk blocks.
3. The speed with which modifications to the data may be performed.
As we shall see in Section 14.6, trees often will be deeper in some parts than in 
others; often the deep parts correspond to regions that have many points. We 
shall also see that it is common that the information corresponding to a tree 
node is considerably smaller than what fits in one block. It is thus necessary to 
group nodes into blocks in some useful way.
14.5 Hash Structures for M ultidimensional Data
In this section we shall consider two data structures that generalize hash tables 
built using a single key. In each case, the bucket for a point is a function, of 
all the attributes or dimensions. One scheme, called the “grid file,” usually 
doesn’t “hash” values along the dimensions, but rather partitions the dimensions by sorting the values along that dimension. The other, called “partitioned 
hashing,” does “hash” the various dimensions, with each dimension contributing to the bucket number.
14.5.1 Grid Files
One of the simplest data structures that often outperforms single-dimension 
indexes for queries involving multidimensional data is the grid file. Think of 
the space of points partitioned in a grid. In each dimension, grid lines partition 
the space into stripes. Points that fall on a grid line will be considered to belong 
to the stripe for which that grid line is the lower boundary. The number of grid 
lines in different dimensions may vary, and there may be different spacings 
between adjacent grid lines, even between lines in the same dimension.
E xam ple 14.27: Let us introduce a running example for multidimensional 
indexes: “who buys gold jewelry?” Imagine a database of customers who have 
bought gold jewelry. To make things simple, we assume that the only relevant 
attributes are the customer’s age and salary. Our example database has twelve 
customers, which we can represent by the following age-salary pairs:
666 CHAPTER 14. INDEX STRUCTURES
(25,60) (45,60) (50,75) (50,100)
(50,120) (70,110) (85,140) (30,260)
(25,400) (45,350) (50,275) (60,260)
In Fig. 14.32 we see these twelve points located in a 2-dimensional space. We 
have also selected some grid lines in each dimension. For this simple example, we 
have chosen two lines in each dimension, dividing the space into nine rectangular 
regions, but there is no reason why the same number of lines must be used in 
each dimension. In general, a rectangle includes points on its lower and left 
boundaries, but not on its upper and right boundaries. For instance, the central 
rectangle in Fig. 14.32 represents points with 40 < age < 55 and 90 < salary <
225. □
500K
Salary
225K
90K
0
0 40 55 100
Age
Figure 14.32: A grid file
14.5.2 Lookup in a Grid File
Each of the regions into which a space is partitioned can be thought of as a 
bucket of a hash table, and each of the points in that region has its record 
placed in a block belonging to that bucket. If needed, overflow blocks can be 
used to increase the size of a bucket.
Instead of a one-dimensional array of buckets, as is found in conventional 
hash tables, the grid file uses an array whose number of dimensions is the same 
as for the data file. To locate the proper bucket for a point, we need to know, 
for each dimension, the list of values at which the grid lines occur. Hashing a 
point is thus somewhat different from applying a hash function to the values of 
its components. Rather, we look at each component of the point and determine 
the position of the point in the grid for that dimension. The positions of the 
point in each of the dimensions together determine the bucket.
14.5. HASH STRUCTURES FOR MULTIDIMENSIONAL DATA 667
E xam ple 14.28: Figure 14.33 shows the data of Fig. 14.32 placed in buckets. 
Since the grids in both dimensions divide the space into three regions, the 
bucket array is a 3 x 3 matrix. Two of the buckets:
1. Salary between $90K and $225K and age between 0 and 40, and
2. Salary below $90K and age above 55
are empty, and we do not show a block for that bucket. The other buckets are 
shown, with the artificially low maximum of two data points per block. In this 
simple example, no bucket has more than two members, so no overflow blocks 
are needed. □
Figure 14.33: A grid file representing the points of Fig. 14.32
14.5.3 Insertion Into Grid Files
When we insert a record into a grid file, we follow the procedure for lookup 
of the record, and we place the new record in that bucket. If there is room in 
the block for the bucket then there is nothing more to do. The problem occurs 
when there is no room in the bucket. There are two general approaches:
1. Add overflow blocks to the buckets, as needed.
668 CHAPTER 14. INDEX STRUCTURES
Accessing Buckets of a Grid File
While finding the proper coordinates for a point in a three-by-three grid 
like Fig. 14.33 is easy, we should remember that the grid file may have a 
very large number of stripes in each dimension. If so, then we must create 
an index for each dimension. The search key for an index is the set of 
partition values in that dimension.
Given a value v in some coordinate, we search for the greatest key 
value w less than or equal to v. Associated with w in that index will be 
the row or column of the matrix into which v falls. Given values in each 
dimension, we can find where in the matrix the pointer to the bucket falls. 
We may then retrieve the block with that pointer directly.
In extreme cases, the matrix is so big, that most of the buckets are 
empty and we cannot afford to store all the empty buckets. Then, we 
must treat the matrix as a relation whose attributes are the corners of 
the nonempty buckets and a final attribute representing the pointer to the 
bucket. Lookup in this relation is itself a multidimensional search, but its 
size is smaller than the size of the data file itself.
2. Reorganize the structure by adding or moving the grid lines. This approach is similar to the dynamic hashing techniques discussed in Section 14.3, but there are additional problems because the contents of buckets are linked across a dimension. That is, adding a grid line splits all the 
buckets along that line. As a result, it may not be possible to select a new 
grid line that does the best for all buckets. For instance, if one bucket is 
too big, we might not be able to choose either a dimension along which 
to split or a point at which to split, without making many empty buckets 
or leaving several very full ones.
E xam ple 14.29: Suppose someone 52 years old with an income of 8200K 
buys gold jewelry. This customer belongs in the central rectangle of Fig. 14.32. 
However, there are now three records in that bucket. We could simply add an 
overflow block. If we want to split the bucket, then we need to choose either 
the age or salary dimension, and we need to choose a new grid line to create 
the division. There are only three ways to introduce a grid line that will split 
the central bucket so two points are on one side and one on the other, which is 
the most even possible split in this case.
1. A vertical line, such as age = 51, that separates the two 50’s from the 
52. This line does nothing to split the buckets above or below, since both 
points of each of the other buckets for age 40-55 are to the left of the line 
age - 51.
14.5. HASH STRUCTURES FOR MULTIDIMENSIONAL DATA 669
2. A horizontal line that separates the point with salary = 200 from the 
other two points in the central bucket. We may as well choose a number 
like 130, which also splits the bucket to the right (that for age 55-100 and 
salary 90-225).
3. A horizontal line that separates the point with salary = 100 from the 
other two points. Again, we would be advised to pick a number like 115 
that also splits the bucket to the right.
Choice (1) is probably not advised, since it doesn’t split any other bucket; 
we are left with more empty buckets and have not reduced the size of any 
occupied buckets, except for the one we had to split. Choices (2) and (3) are 
equally good, although we might pick (2) because it puts the horizontal grid 
line at salary = 130, which is closer to midway between the upper and lower 
limits of 90 and 225 than we get with choice (3). The resulting partition into 
buckets is shown in Fig. 14.34. □
500K
Salary
225K
130K
90K
0
0 40 55 100
Age
Figure 14.34: Insertion of the point (52,200) followed by splitting of buckets
14.5.4 Performance of Grid Files
Let us consider how many disk I/O ’s a grid file requires on various types of 
queries. We have been focusing on the two-dimensional version of grid files, 
although they can be used for any number of dimensions. One major problem 
in the high-dimensional case is that the number of buckets grows exponentially 
with the number of dimensions. If large portions of a space are empty, then 
there will be many empty buckets. We can envision the problem even in two 
dimensions. Suppose that there were a high correlation between age and salary,
670 CHAPTER 14. INDEX STRUCTURES
so all points in Fig. 14.32 lay along the diagonal. Then no m atter where we 
placed the grid lines, the buckets off the diagonal would have to be empty.
However, if the data is well distributed, and the data file itself is not too 
large, then we can choose grid lines so that:
1. There are sufficiently few buckets that we can keep the bucket matrix in 
main memory, thus not incurring disk I/O to consult it, or to add rows 
or columns to the matrix when we introduce a new grid line.
2. We can also keep in memory indexes on the values of the grid lines in 
each dimension (as per the box “Accessing Buckets of a Grid File”), or 
we can avoid the indexes altogether and use main-memory binary search 
of the values defining the grid lines in each dimension.
3. The typical bucket does not have more than a few overflow blocks, so we 
do not incur too many disk I/O ’s when we search through a bucket.
Under those assumptions, here is how the grid file behaves on some important 
classes of queries.
L ookup o f Specific P oin ts
We are directed to the proper bucket, so the only disk I/O is what is necessary 
to read the bucket. If we are inserting or deleting, then an additional disk 
write is needed. Inserts that require the creation of an overflow block cause an 
additional write.
P artial-M atch Q ueries
Examples of this query would include “find all customers aged 50,” or “find all 
customers with a salary of $200K.” Now, we need to look at all the buckets in 
a row or column of the bucket matrix. The number of disk I/O ’s can be quite 
high if there are many buckets in a row or column, but only a small fraction of 
all the buckets will be accessed.
R ange Q ueries
A range query defines a rectangular region of the grid, and all points found 
in the buckets that cover that region will be answers to the query, with the 
exception of some of the points in buckets on the border of the search region. 
For example, if we want to find all customers aged 35-45 with a salary of 50-100, 
then we need to look in the four buckets in the lower left of Fig. 14.32. In this 
case, all buckets are on the border, so we may look at a good number of points 
that are not answers to the query. However, if the search region involves a large 
number of buckets, then most of them must be interior, and all their points are 
answers. For range queries, the number of disk 1/O’s may be large, as we may 
be required to examine many buckets. However, since range queries tend to
14.5. HASH STRUCTURES FOR MULTIDIMENSIONAL DATA 671
produce large answer sets, we typically will examine not too many more blocks 
than the minimum number of blocks on which the answer could be placed by 
any organization whatsoever.
N earest-N eighbor Q ueries
Given a point P, we start by searching the bucket in which that point belongs. 
If we find at least one point there, we have a candidate Q for the nearest 
neighbor. However, it is possible that there are points in adjacent buckets that 
are closer to P than Q is; the situation is like that suggested in Fig. 14.31. We 
have to consider whether the distance between P and a border of its bucket is 
less than the distance from P to Q. If there are such borders, then the adjacent 
buckets on the other side of each such border must be searched also. In fact, 
if buckets are severely rectangular — much longer in one dimension than the 
other — then it may be necessary to search even buckets that are not adjacent 
to the one containing point P.
E xam ple 14.30: Suppose we are looking in Fig. 14.32 for the point nearest 
P = (45,200). We find that (50,120) is the closest point in the bucket, at 
a distance of 80.2. No point in the lower three buckets can be this close to 
(45,200), because their salary component is at most 90, so we can omit searching 
them. However, the other five buckets must be searched, and we find that there 
are actually two equally close points: (30,260) and (60,260), at a distance of 
61.8 from P. Generally, the search for a nearest neighbor can be limited to a 
few buckets, and thus a few disk I/O ’s. However, since the buckets nearest the 
point P may be empty, we cannot easily put an upper bound on how costly the 
search is. □
14.5.5 Partitioned Hash Functions
Hash functions can take a list of values as arguments, although typically there 
is only one argument. For instance, if a is an integer-valued attribute and 6 is a 
character-string-valued attribute, then we could compute h(a, b) by adding the 
value of a to the value of the ASCII code for each character of b, dividing by 
the number of buckets, and taking the remainder.
However, such a hash table could be used only in queries that specified 
values for both a and b. A preferable option is to design the hash function 
so it produces some number of bits, say k. These k bits are divided among n
attributes, so that we produce ki bits of the hash value from the ith attribute, 
and Y^i=i ki = k- More precisely, the hash function h is actually a list of hash 
functions (h i,h i,... , hn), such that hi applies to a value for the ith attribute 
and produces a sequence of ki bits. The bucket in which to place a tuple with 
values (vi,v 2 ,-.- ,vn) for the n attributes is computed by concatenating the bit 
sequences: hi(vi)h2 (v2) ■ ■ ■ h„(vn).
E xam ple 14.31: If we have a hash table with 10-bit bucket numbers (1024 
buckets), we could devote four bits to attribute a and the remaining six bits to
672 CHAPTER 14. INDEX STRUCTURES
attribute 6. Suppose we have a tuple with o-value A and 6-value B, perhaps 
with other attributes that are not involved in the hash. If ha{A) = 0101 and 
hb(B) — 111000, then this tuple hashes to bucket 0101111000, the concatenation 
of the two bit sequences.
By partitioning the hash function this way, we get some advantage from 
knowing values for any one or more of the attributes that contribute to the 
hash function. For instance, if we are given a value A for attribute o, and we 
find that ha(A) = 0101, then we know that the only tuples with o-value A
are in the 64 buckets whose numbers are of the form 0101 • • • , where the • • • 
represents any six bits. Similarly, if we are given the 6-value B of a tuple, we 
can isolate the possible buckets of the tuple to the 16 buckets whose number 
ends in the six bits ht{B). □
E xam ple 14.32 : Suppose we have the “gold jewelry” data of Example 14.27, 
which we want to store in a partitioned hash table with eight buckets (i.e., three 
bits for bucket numbers). We assume as before that two records are all that can 
fit in one block. We shall devote one bit to the age attribute and the remaining 
two bits to the salary attribute.
Figure 14.35: A partitioned hash table
For the hash function on age, we shall take the age modulo 2; that is, a 
record with an even age will hash into a bucket whose number is of the form 
0xy for some bits x and y. A record with an odd age hashes to one of the buckets 
with a number of the form lxy. The hash function for salary will be the salary 
(in thousands) modulo 4. For example, a salary that leaves a remainder of 1 
when divided by 4, such as 57K, will be in a bucket whose number is zOl for 
some bit z.
14.5. HASH STRUCTURES FOR MULTIDIMENSIONAL DATA 673
In Fig. 14.35 we see the data from Example 14.27 placed in this hash table. 
Notice that, because we have used mostly ages and salaries divisible by 10, the 
hash function does not distribute the points too well. Two of the eight buckets 
have four records each and need overflow blocks, while three other buckets are 
empty. □
14.5.6 Comparison of Grid Files and Partitioned Hashing
The performance of the two data structures discussed in this section are quite 
different. Here are the major points of comparison.
• Partitioned hash tables are actually quite useless for nearest-neighbor 
queries or range queries. The problem is that physical distance between 
points is not reflected by the closeness of bucket numbers. Of course we 
could design the hash function on some attribute a so the smallest values 
were assigned the first bit string (all 0’s), the next values were assigned the 
next bit string (00 ■ • • 01), and so on. If we do so, then we have reinvented 
the grid file.
• A well chosen hash function will randomize the buckets into which points 
fall, and thus buckets will tend to be equally occupied. However, grid 
files, especially when the number of dimensions is large, will tend to leave 
many buckets empty or nearly so. The intuitive reason is that when there 
are many attributes, there is likely to be some correlation among at least 
some of them, so large regions of the space are left empty. For instance, 
we mentioned in Section 14.5.4 that a correlation between age and salary 
would cause most points of Fig. 14.32 to lie near the diagonal, with most of 
the rectangle empty. As a consequence, we can use fewer buckets, and/or 
have fewer overflow blocks in a partitioned hash table than in a grid file.
Thus, if we are required to support only partial match queries, where we 
specify some attributes’ values and leave the other attributes completely unspecified, then the partitioned hash function is likely to outperform the grid 
file. Conversely, if we need to do nearest-neighbor queries or range queries 
frequently, then we would prefer to use a grid file.
14.5.7 Exercises for Section 14.5
Exercise 14.5.1: In Fig. 14.36 are specifications for twelve of the thirteen 
PC’s introduced in Fig. 2.21. Suppose we wish to design an index on speed and 
hard-disk size only.
a) Choose five grid lines (total for the two dimensions), so that there are no 
more than two points in any bucket.
! b) Can you separate the points with at most two per bucket if you use only 
four grid lines? Either show how or argue that it is not possible.
674 CHAPTER 14. INDEX STRUCTURES
model speed ram hd
1001 2.66 1024 250
1002 2.10 512 250
1003 1.42 512 80
1004 2.80 1024 250
1005 3.20 512 250
1006 3.20 1024 320
1007 2.20 1024 200
1008 2.20 2048 250
1009 2.00 1024 250
1010 2.80 2048 300
1011 1.86 2048 160
1012 2.80 1024 160
Figure 14.36: Some PC’s and their characteristics
! c) Suggest a partitioned hash function that will partition these points into 
four buckets with at most four points per bucket.
! E xercise 14.5.2: Suppose we wish to place the data of Fig. 14.36 in a threedimensional grid file, based on the speed, ram, and hard-disk attributes. Suggest a partition in each dimension that will divide the data well.
Exercise 14.5.3: Choose a partitioned hash function with one bit for each of 
the three attributes speed, ram, and hard-disk that divides the data of Fig. 14.36 
well.
Exercise 14.5.4: Suppose we place the data of Fig. 14.36 in a grid file with 
dimensions for speed and ram only. The partitions are at speeds of 2.00, 2.20, 
and 2.80, and at ram of 1024 and 2048. Suppose also that only two points can 
fit in one bucket. Suggest good splits if we insert a point with speed 2.5 and 
ram 1536.
E xercise 14.5.5: Suppose we store a relation R (x,y) in a grid file. Both 
attributes have a range of values from 0 to 1000. The partitions of this grid file 
happen to be uniformly spaced; for x there are partitions every 20 units, at 20, 
40, 60, and so on, while for y the partitions are every 50 units, at 50, 100, 150, 
and so on.
a) How many buckets do we have to examine to answer the range query 
SELECT * FROM R
WHERE 310 < x AND x < 400 AND 520 < y AND y < 730;
14.6. TREE STRUCTURES FOR MULTIDIMENSIONAL DATA 675
! b) We wish to perform a nearest-neighbor query for the point (110,205). 
We begin by searching the bucket with lower-left corner at (100,200) and 
upper-right corner at (120,250), and we find that the closest point in this 
bucket is (115,220). What other buckets must be searched to verify that 
this point is the closest?
E xercise 14.5.6: Suppose we have a hash table whose buckets are numbered
0 to 2n — 1; i.e., bucket addresses are n bits long. We wish to store in the table 
a relation with two attributes x and y. A query will specify either a value for 
x or y, but never both. With probability p, it is x whose value is specified.
a) Suppose we partition the hash function so that m bits are devoted to x
and the remaining n — m bits to y. As a function of m, n, and p, what 
is the expected number of buckets that must be examined to answer a 
random query?
b) For what value of m (as a function of n and p) is the expected number of 
buckets minimized? Do not worry that this m is unlikely to be an integer.
14.6 Tree Structures for M ultidimensional Data
We shall now consider four more structures that are useful for range queries or 
nearest-neighbor queries on multidimensional data. In order, we shall consider:
1. Multiple-key indexes.
2. kd-trees.
3. Quad trees.
4. R-trees.
The first three are intended for sets of points. The R-tree is commonly used to 
represent sets of regions; it is also useful for points.
14.6.1 M ultiple-Key Indexes
Suppose we have several attributes representing dimensions of our data points, 
and we want to support range queries or nearest-neighbor queries on these 
points. A simple tree scheme for accessing these points is an index of indexes, 
or more generally a tree in which the nodes at each level are indexes for one 
attribute.
The idea is suggested in Fig. 14.37 for the case of two attributes. The 
“root of the tree” is an index for the first of the two attributes. This index 
could be any type of conventional index, such as a B-tree or a hash table. The 
index associates with each of its search-key values — i.e., values for the first 
attribute — a pointer to another index. If V is a value of the first attribute,
676 CHAPTER 14. INDEX STRUCTURES
Index on 
first attribute
Indexes on 
second attribute
Figure 14.37: Using nested indexes on different keys
then the index we reach by following key V and its pointer is an index into the 
set of points that have V for their value in the first attribute and any value for 
the second attribute.
E xam ple 14.33: Figure 14.38 shows a multiple-key index for our running 
“gold jewelry” example, where the first attribute is age, and the second attribute 
is salary. The root index, on age, is suggested at the left of Fig. 14.38. At the 
right of Fig. 14.38 are seven indexes that provide access to the points themselves. 
For example, if we follow the pointer associated with age 50 in the root index, 
we get to a smaller index where salary is the key, and the four key values in the 
index are the four salaries associated with points that have age 50: salaries 75, 
100, 120, and 275. □
In a multiple-key index, some of the second- or higher-level indexes may be 
very small. For example, Fig 14.38 has four second-level indexes with but a 
single pair. Thus, it may be appropriate to implement these indexes as simple 
tables that are packed several to a block.
14.6.2 Performance of M ultiple-Key Indexes
Let us consider how a multiple key index performs on various kinds of multidimensional queries. We shall concentrate on the case of two attributes, although 
the generalization to more than two attributes is unsurprising.
P a rtial-M a tc h Q ueries
If the first attribute is specified, then the access is quite efficient. We use the 
root index to find the one subindex that leads to the points we want. On the
14.6. TREE STRUCTURES FOR MULTIDIMENSIONAL DATA 677
Figure 14.38: Multiple-key indexes for age/salary data
other hand, if the first attribute does not have a specified value, then we must 
search every subindex, a potentially time-consuming process.
R ange Q ueries
The multiple-key index works quite well for a range query, provided the individual indexes themselves support range queries on their attribute — B-trees 
or indexed-sequential files, for instance. To answer a range query, we use the 
root index and the range of the first attribute to find all of the subindexes that 
might contain answer points. We then search each of these subindexes, using 
the range specified for the second attribute.
N earest-N eig h b o r Q ueries
These queries can be answered by a series of range queries, as described in 
Section 14.4.3.
14.6.3 kd-Trees
A kd-tree (fc-dimensional search tree) is a main-memory data structure generalizing the binary search tree to multidimensional data. We shall present the 
idea and then discuss how the idea has been adapted to the block model of 
storage. A kd-trce is a binary tree in which interior nodes have an associated 
attribute a and a value V that splits the data points into two parts: those with
678 CHAPTER 14. INDEX STRUCTURES
a-value less than V and those with o-value equal to or greater than V. The 
attributes at different levels of the tree are different, with levels rotating among 
the attributes of all dimensions.
In the classical fcrf-tree, the data points are placed at the nodes, just as in 
a binary search tree. However, we shall make two modifications in our initial 
presentation of the idea to take some limited advantage of the block model of 
storage.
1. Interior nodes will have only an attribute, a dividing value for that attribute, and pointers to left and right children.
2. Leaves will be blocks, with space for as many records as a block can hold.
C^Salaiy 8 0 ^ )
50,100 30,260 25,400
50,120 45,350
25,60 45,60
50,75
Figure 14.39: A kd-tree
E xam ple 14.34: In Fig. 14.39 is a kd-tree for the twelve points of our running 
gold-jewelry example. We use blocks that hold only two records for simplicity; 
these blocks and their contents are shown as square leaves. The interior nodes 
are ovals with an attribute — either age or salary — and a value. For instance, 
the root splits by salary, with all records in the left subtree having a salary less 
than $150K, and all records in the right subtree having a salary at least $150K.
At the second level, the split is by age. The left child of the root splits at 
age 60, so everything in its left subtree will have age less than 60 and salary 
less than $150K. Its right subtree will have age at least 60 and salary less than 
$150K. Figure 14.40 suggests how the various interior nodes split the space 
of points into leaf blocks. For example, the horizontal line at salary = 150 
represents the split at the root. The space below that line is split vertically at 
age 60, while the space above is split at age 47, corresponding to the decision 
at the right child of the root. □
14.6. TREE STRUCTURES FOR MULTIDIMENSIONAL DATA 679
500K
Salary
100
Age
Figure 14.40: The partitions implied by the tree of Fig. 14.39
14.6.4 Operations on kd -Trees
A lookup of a tuple, given values for all dimensions, proceeds as in a binary 
search tree. We make a decision which way to go at each interior node and are 
directed to a single leaf, whose block we search.
To perform an insertion, we proceed as for a lookup. We are eventually 
directed to a leaf, and if its block has room we put the new data point there. 
If there is no room, we split the block into two, and we divide its contents 
according to whatever attribute is appropriate at the level of the leaf being 
split. We create a new interior node whose children are the two new blocks, 
and we install at that interior node a splitting value that is appropriate for the 
split we have just made.8
E xam ple 14.35: Suppose someone 35 years old with a salary of S500K buys 
gold jewelry. Starting at the root, since the salary is at least $150K we go to 
the right. There, we compare the age 35 with the age 47 at the node, which 
directs us to the left. At the third level, we compare salaries again, and our 
salary is greater than the splitting value, $300K. We are thus directed to a leaf 
containing the points (25,400) and (45,350), along with the new point (35,500).
There isn’t room for three records in this block, so we must split it. The 
fourth level splits on age, so we have to pick some age that divides the records 
as evenly as possible. The median value, 35, is a good choice, so we replace the 
leaf by an interior node that splits on age = 35. To the left of this interior node 
is a leaf block with only the record (25,400), while to the right is a leaf block 
with the other two records, as shown in Fig. 14.41. □
8O ne problem th a t m ight arise is a situ atio n w here th ere are so m any p oints w ith th e sam e 
value in a given dim ension th a t th e bucket has only one value in th a t dim ension an d cannot 
be sp lit. W e can tr y sp littin g along an o th er dim ension, or we can use an overflow block.
680 CHAPTER 14. INDEX STRUCTURES
Figure 14.41: Tree after insertion of (35,500)
The more complex queries discussed in this chapter are also supported by a 
kd-tree. Here are the key ideas and synopses of the algorithms:
P artial-M atch Q ueries
If we are given values for some of the attributes, then we can go one way when 
we are at a level belonging to an attribute whose value we know. When we 
don’t know the value of the attribute at a node, we must explore both of its 
children. For example, if we ask for all points with age = 50 in the tree of 
Fig. 14.39, we must look at both children of the root, since the root splits on 
salary. However, at the left child of the root, we need go only to the left, and at 
the right child of the root we need only explore its right subtree. For example, 
if the tree is perfectly balanced and the index has two dimensions, one of which 
is specified in the search, then we would have to explore both ways at every 
other level, ultimately reaching about the square root of the total number of 
leaves.
R ange Q ueries
Sometimes, a range will allow us to move to only one child of a node, but if 
the range straddles the splitting value at the node then we must explore both 
children. For example, given the range of ages 35 to 55 and the range of salaries 
from $100K to $200K, we would explore the tree of Fig. 14.39 as follows. The 
salary range straddles the $150K at the root, so we must explore both children. 
At the left child, the range is entirely to the left, so we move to the node with 
salary $80K. Now, the range is entirely to the right, so we reach the leaf with 
records (50,100) and (50,120), both of which meet the range query. Returning 
to the right child of the root, the splitting value age = 47 tells us to look at both
14.6. TREE STRUCTURES FOR MULTIDIMENSIONAL DATA 681
subtrees. At the node with salary S300K, we can go only to the left, finding 
the point (30,260), which is actually outside the range. At the right child of 
the node for age = 47, we find two other points, both of which are outside the 
range.
14.6.5 Adapting kd-Trees to Secondary Storage
Suppose we store a file in a kd-tree with n leaves. Then the average length of a 
path from the root to a leaf will be about log2 n, as for any binary tree. If we 
store each node in a block, then as we traverse a path we must do one disk I/O 
per node. For example, if n = 1000, then we need about 10 disk I/O ’s, much 
more than the 2 or 3 disk I/O ’s that would be typical for a B-tree, even on a 
much larger file. In addition, since interior nodes of a kd-tree have relatively 
little information, most of the block would be wasted space. Two approaches 
to the twin problems of long paths and unused space are:
1. Multiway Branches at Interior Nodes. Interior nodes of a kd-tree could 
look more like B-tree nodes, with many key-pointer pairs. If we had n
keys at a node, we could split values of an attribute a into n + 1 ranges. If 
there were n-1-1 pointers, we could follow the appropriate one to a subtree 
that contained only points with attribute a in that range.
2. Group Interior Nodes Into Blocks. We could pack many interior nodes, 
each with two children, into a single block. To minimize the number of 
blocks that we must read from disk while traveling down one path, we 
are best off including in one block a node and all its descendants for some 
number of levels. That way, once we retrieve the block with this node, 
we are sure to use some additional nodes on the same block, saving disk 
I/O ’s.
14.6.6 Quad Trees
In a quad tree, each interior node corresponds to a square region in two dimensions, or to a ^-dimensional cube in k dimensions. As with the other data 
structures in this chapter, we shall consider primarily the two-dimensional case. 
If the number of points in a square is no larger than what will fit in a block, 
then we can think of this square as a leaf of the tree, and it is represented by 
the block that holds its points. If there are too many points to fit in one block, 
then we treat the square as an interior node, with children corresponding to its 
four quadrants.
E xam ple 14.36: Figure 14.42 shows the gold-jewelry data points organized 
into regions that correspond to nodes of a quad tree. For ease of calculation, we 
have restricted the usual space so salary ranges between 0 and S400K, rather 
than up to $500K as in other examples of this chapter. We continue to make 
the assumption that only two records can fit in a block.
682 CHAPTER 14. INDEX STRUCTURES
400K
Salary
• •
100
Age
Figure 14.42: Data organized in a quad tree
Figure 14.43 shows the tree explicitly. We use the compass designations for 
the quadrants and for the children of a node (e.g., SW stands for the southwest 
quadrant — the points to the left and below the center). The order of children 
is always as indicated at the root. Each interior node indicates the coordinates 
of the center of its region.
Since the entire space has 12 points, and only two will fit in one block, 
we must split the space into quadrants, which we show by the dashed line in 
Fig. 14.42. Two of the resulting quadrants — the southwest and northeast — 
have only two points. They can be represented by leaves and need not be split 
further.
Figure 14.43: A quad tree
The remaining two quadrants each have more than two points. Both are 
split into subquadrants, as suggested by the dotted lines in Fig. 14.42. Each of 
the resulting quadrants has at most two points, so no more splitting is necessary.
□
14.6. TREE STRUCTURES FOR MULTIDIMENSIONAL DATA 683
Since interior nodes of a quad tree in k dimensions have 2k children, there 
is a range of k where nodes fit conveniently into blocks. For instance, if 128, or 
27, pointers can fit in a block, then fc = 7 is a convenient number of dimensions. 
However, for the 2-dimensional case, the situation is not much better than for 
fcd-trees; an interior node has four children. Moreover, while we can choose the 
splitting point for a kd-tree node, we are constrained to pick the center of a 
quad-tree region, which may or may not divide the points in that region evenly. 
Especially when the number of dimensions is large, we expect to find many null 
pointers (corresponding to empty quadrants) in interior nodes. Of course we 
can be somewhat clever about how high-dimension nodes are represented, and 
keep only the non-null pointers and a designation of which quadrant the pointer 
represents, thus saving considerable space.
We shall not go into detail regarding the standard operations that we discussed in Section 14.6.4 for kd-trees. The algorithms for quad trees resemble 
those for kd-trees.
14.6.7 R-Trees
An R-tree (region tree) is a data structure that captures some of the spirit of 
a B-tree for multidimensional data. Recall that a B-tree node has a set of keys 
that divide a line into segments. Points along that line belong to only one 
segment, as suggested by Fig. 14.44. The B-tree thus makes it easy for us to 
find points; if we think the point is somewhere along the line represented by 
a B-tree node, we can determine a unique child of that node where the point 
could be found.
Figure 14.44: A B-tree node divides keys along a line into disjoint segments
An R-tree, on the other hand, represents data that consists of 2-dimensional, 
or higher-dimensional regions, which we call data regions. An interior node of 
an R-tree corresponds to some interior region, or just “region,” which is not 
normally a data region. In principle, the region can be of any shape, although 
in practice it is usually a rectangle or other simple shape. The R-tree node 
has, in place of keys, subregions that represent the contents of its children. The 
subregions are allowed to overlap, although it is desirable to keep the overlap 
small.
Figure 14.45 suggests a node of an R-tree that is associated with the large 
solid rectangle. The dotted rectangles represent the subregions associated with 
four of its children. Notice that the subregions do not cover the entire region, 
which is satisfactory as long as each data region that lies within the large region 
is wholly contained within one of the small regions.
684 CHAPTER 14. INDEX STRUCTURES
Figure 14.45: The region of an R-tree node and subregions of its children
14.6.8 Operations on R-Trees
A typical query for which an R-tree is useful is a “where-am-I” query, which 
specifies a point P and asks for the data region or regions in which the point lies. 
We start at the root, with which the entire region is associated. We examine 
the subregions at the root and determine which children of the root correspond 
to interior regions that contain point P. Note that there may be zero, one, or 
several such regions.
If there are zero regions, then we are done; P is not in any data region. If 
there is at least one interior region that contains P, then we must recursively 
search for P at the child corresponding to each such region. When we reach 
one or more leaves, we shall find the actual data regions, along with either the 
complete record for each data region or a pointer to that record.
When we insert a new region R into an R-tree, we start at the root and try 
to find a subregion into which R fits. If there is more than one such region, then 
we pick one, go to its corresponding child, and repeat the process there. If there 
is no subregion that contains R, then we have to expand one of the subregions. 
Which one to pick may be a difficult decision. Intuitively, we want to expand 
regions as little as possible, so we might ask which of the children’s subregions 
would have their area increased as little as possible, change the boundary of 
that region to include R, and recursively insert R at the corresponding child.
Eventually, we reach a leaf, where we insert the region R. However, if there 
is no room for R at that leaf, then we must split the leaf. How we split the 
leaf is subject to some choice. We generally want the two subregions to be as 
small as possible, yet they must, between them, cover all the data regions of 
the original leaf. Having split the leaf, we replace the region and pointer for the 
original leaf at the node above by a pair of regions and pointers corresponding 
to the two new leaves. If there is room at the parent, we are done. Otherwise, 
as in a B-tree, we recursively split nodes going up the tree.
E xam ple 14.37: Let us consider the addition of a new region to the map of 
Fig. 14.30. Suppose that leaves have room for six regions. Further suppose that 
the six regions of Fig. 14.30 are together on one leaf, whose region is represented 
by the outer (solid) rectangle in Fig. 14.46.
14.6. TREE STRUCTURES FOR MULTIDIMENSIONAL DATA 685
0 100
Figure 14.46: Splitting the set of objects
Now, suppose the local cellular phone company adds a POP (point of presence, or base station) at the position shown in Fig. 14.46. Since the seven data 
regions do not fit on one leaf, we shall split the leaf, with four in one leaf and 
three in the other. Our options are many; we have picked in Fig. 14.46 the 
division (indicated by the inner, dashed rectangles) that minimizes the overlap, 
while splitting the leaves as evenly as possible.
((0,0),(60,50))
£
8
00
©
road l road2 housel school house2 pipeline pop
Figure 14.47: An R-tree
We show in Fig. 14.47 how the two new leaves fit into the R-tree. The parent 
of these nodes has pointers to both leaves, and associated with the pointers are 
the lower-left and upper-right corners of the rectangular regions covered by each 
leaf. □
E xam ple 14.38: Suppose we inserted another house below house2, with lowerleft coordinates (70,5) and upper-right coordinates (80,15). Since this house is
686 CHAPTER 14. INDEX STRUCTURES
100
Figure 14.48: Extending a region to accommodate new data
not wholly contained within either of the leaves’ regions, we must choose which 
region to expand. If we expand the lower subregion, corresponding to the first 
leaf in Fig. 14.47, then we add 1000 square units to the region, since we extend 
it 20 units to the right. If we extend the other subregion by lowering its bottom 
by 15 units, then we add 1200 square units. We prefer the first, and the new 
regions are changed in Fig. 14.48. We also must change the description of the 
region in the top node of Fig. 14.47 from ((0,0), (60,50)) to ((0,0), (80,50)).
□
14.6.9 Exercises for Section 14.6
E xercise 14.6.1: Show a multiple-key index for the data of Fig. 14.36 if the 
indexes are on:
a) Speed, then ram.
b) Ram then hard-disk.
c) Speed, then ram, then hard-disk.
E xercise 14.6.2: Place the data of Fig. 14.36 in a fcd-tree. Assume two records 
can fit in one block. At each level, pick a separating value that divides the data 
as evenly as possible. For an order of the splitting attributes choose:
a) Speed, then ram, alternating.
b) Speed, then ram, then hard-disk, alternating.
14.6. TREE STRUCTURES FOR MULTIDIMENSIONAL DATA 687
c) Whatever attribute produces the most even split at each node.
Exercise 14.6.3: Suppose we have a relation R (x ,y ,z), where the pair of 
attributes x and y together form the key. Attribute x ranges from 1 to 100, 
and y ranges from 1 to 1000. For each x there are records with 100 different 
values of y, and for each y there are records with 10 different values of x. Note 
that there are thus 10,000 records in R. We wish to use a multiple-key index 
that will help us to answer queries of the form
SELECT z 
FROM R
WHERE x = C AND y = D;
where C and D are constants. Assume that blocks can hold ten key-pointer 
pairs, and we wish to create dense indexes at each level, perhaps with sparse 
higher-level indexes above them, so that each index starts from a single block. 
Also assume that initially all index and data blocks are on disk.
a) How many disk I/O ’s are necessary to answer a query of the above form 
if the first index is on x l
b) How many disk I/O ’s are necessary to answer a query of the above form 
if the first index is on y l
! c) Suppose you were allowed to buffer 11 blocks in memory at all times. 
Which blocks would you choose, and would you make x or y the first 
index, if you wanted to minimize the number of additional disk I/O ’s 
needed?
E xercise 14.6.4: For the structure of Exercise 14.6.3(a), how many disk I/O ’s 
cure required to answer the range query in which 20 < x < 35 and 200 < y < 350. 
Assume data is distributed uniformly; i.e., the expected number of points will 
be found within any given range.
E xercise 14.6.5: In the tree of Fig. 14.39, what new points would be directed 
to:
a) The block with point (30,260)?
b) The block with points (50,100) and (50,120)?
Exercise 14.6.6: Show a possible evolution of the tree of Fig. 14.41 if we 
insert the points (20,110) and then (40,400).
Exercise 14.6.7: We mentioned that if a kd-tree were perfectly balanced, and 
we execute a partial-match query in which one of two attributes has a value 
specified, then we wind up looking at about n out of the n leaves.
a) Explain why.
688 CHAPTER 14. INDEX STRUCTURES
b) If the tree split alternately in d dimensions, and we specified values for m
of those dimensions, what fraction of the leaves would we expect to have 
to search?
c) How does the performance of (b) compare with a partitioned hash table?
E xercise 14.6.8: Place the data of Fig. 14.36 in a quad tree with dimensions 
speed and ram. Assume the range for speed is 1.00 to 5.00, and for ram it is 
500 to 3500. No leaf of the quad tree should have more than two points.
E xercise 14.6.9: Repeat Exercise 14.6.8 with the addition of a third dimension, hard-disk, that ranges from 0 to 400.
! E xercise 14.6.10: If we are allowed to put the central point in a quadrant of a 
quad tree wherever we want, can we always divide a quadrant into subquadrants 
with an equal number of points (or as equal as possible, if the number of points 
in the quadrant is not divisible by 4)? Justify your answer.
! E xercise 14.6.11: Suppose we have a database of 1,000,000 regions, which 
may overlap. Nodes (blocks) of an R-tree can hold 100 regions and pointers. 
The region represented by any node has 100 subregions, and the overlap among 
these regions is such that the total area of the 100 subregions is 150% of the 
area of the region. If we perform a “where-am-I” query for a given point, how 
many blocks do we expect to retrieve?
14.7 Bitm ap Indexes
Let us now turn to a type of index that is rather different from those seen so 
far. We begin by imagining that records of a file have permanent numbers, 
1 ,2 ,... , n. Moreover, there is some data structure for the file that lets us find 
the ith record easily for any i. A bitmap index for a field F is a collection of 
bit-vectors of length n, one for each possible value that may appear in the field 
F. The vector for value v has 1 in position i if the ith record has v in field F,
and it has 0 there if not.
E xam ple 14.39: Suppose a file consists of records with two fields, F and G, of 
type integer and string, respectively. The current file has six records, numbered 
1 through 6, with the following values in order: (30, foo), (30, bar), (40, baz), 
(50, foo), (40, bar), (30, baz).
A bitmap index for the first field, F, would have three bit-vectors, each of 
length 6. The first, for value 30, is 110001, because the first, second, and sixth 
records have F = 30. The other two, for 40 and 50, respectively, are 001010 
and 000100.
A bitmap index for G would also have three bit-vectors, because there are 
three different strings appearing there. The three bit-vectors are:
14.7. BITM AP INDEXES 689
Value Vector
foo 100100
bax 010010
baz 001001
In each case, l ’s indicate the records in which the corresponding string appears.
□
14.7.1 Motivation for Bitmap Indexes
It might at first appear that bitmap indexes require much too much space, 
especially when there are many different values for a field, since the total number 
of bits is the product of the number of records and the number of values. For 
example, if the field is a key, and there are n records, then n2 bits are used 
among all the bit-vectors for that field. However, compression can be used to 
make the number of bits closer to n, independent of the number of different 
values, as we shall see in Section 14.7.2.
You might also suspect that there are problems managing the bitmap indexes. For example, they depend on the number of a record remaining the same 
throughout time. How do we find the ith record as the file adds and deletes 
records? Similarly, values for a field may appear or disappear. How do we find 
the bitmap for a value efficiently? These and related questions are discussed in 
Section 14.7.4.
The compensating advantage of bitmap indexes is that they allow us to 
answer partial-match queries very efficiently in many situations. In a sense they 
offer the advantages of buckets that we discussed in Example 14.7, where we 
found the Movie tuples with specified values in several attributes without first 
retrieving all the records that matched in each of the attributes. An example 
will illustrate the point.
Example 14.40: Recall Example 14.7, where we queried the Movie relation 
with the query
SELECT title FROM Movie
WHERE studioName = ’Disney’ AND yeax = 2005;
Suppose there are bitmap indexes on both attributes studioName and yeax. 
Then we can intersect the vectors for year = 2005 and studioName = ’ Disney ’; 
that is, we take the bitwise AND of these vectors, which will give us a vector 
with a 1 in position i if and only if the ith Movie tuple is for a movie made by 
Disney in 2005.
If we can retrieve tuples of Movie given their numbers, then we need to 
read only those blocks containing one or more of these tuples, just as we did in 
Example 14.7. To intersect the bit vectors, we must read them into memory, 
which requires a disk I/O for each block occupied by one of the two vectors. As 
mentioned, we shall later address both matters: accessing records given their
690 CHAPTER 14. INDEX STRUCTURES
numbers in Section 14.7.4 and making sure the bit-vectors do not occupy too 
much space in Section 14.7.2. □
Bitmap indexes can also help answer range queries. We shall consider an 
example next that both illustrates their use for range queries and shows in detail 
with short bit-vectors how the bitwise AND and OR of bit-vectors can be used 
to discover the answer to a query without looking at any records but the ones 
we want.
E xam ple 14.41: Consider the gold-jewelry data first introduced in Example 14.27. Suppose that the twelve points of that example are records numbered 
from 1 to 12 as follows:
1
5
9
For the first component, age, there are seven different values, so the bitmap 
index for age consists of the following seven vectors:
: (25,60) 2: (45,60) 3: (50,75) 4: (50,100)
: (50,120) 6: (70,110) 7: (85,140) 8: (30,260)
: (25,400) 10: (45,350) 11: (50,275) 12: (60,260)
25
50
85
100000001000 30: 000000010000 45: 010000000100 
001110000010 60: 000000000001 70: 000001000000 
000000100000
For the salary component, there are ten different values, so the salary bitmap 
index has the following ten bit-vectors:
000100000000
000000100000
000000000100
60: 110000000000 75: 001000000000 100:
110: 000001000000 120: 000010000000 140:
260: 000000010001 275: 000000000010 350:
400: 000000001000
Suppose we want to find the jewelry buyers with an age in the range 45-55 
and a salary in the range 100-200. We first find the bit-vectors for the age 
values in this range; in this example there are only two: 010000000100 and 
001110000010, for 45 and 50, respectively. If we take their bitwise OR, we have 
a new bit-vector with 1 in position i if and only if the *th record has an age in 
the desired range. This bit-vector is 011110000110.
Next, we find the bit-vectors for the salaries between 100 and 200 thousand. 
There are four, corresponding to salaries 100, 110, 120, and 140; their bitwise 
OR is 000111100000.
The last step is to take the bitwise AND of the two bit-vectors we calculated 
by OR. That is:
011110000110 AND 000111100000 = 000110000000
We thus find that only the fourth and fifth records, which are (50,100) and
(50,120), are in the desired range. □
14.7. BITM AP INDEXES 691
Binary Numbers Won’t Serve as a Run-Length
Encoding
Suppose we represented a run of i 0’s followed by a 1 with the integer i in 
binary. Then the bit-vector 000101 consists of two runs, of lengths 3 and 1, 
respectively. The binary representations of these integers are 11 and 1, so 
the run-length encoding of 000101 is 111. However, a similar calculation 
shows that the bit-vector 010001 is also encoded by 111; bit-vector 010101 
is a third vector encoded by 111. Thus, 111 cannot be decoded uniquely 
into one bit-vector.
14.7.2 Compressed Bitmaps
Suppose we have a bitmap index on field F of a file with n records, and there 
are m different values for field F that appear in the file. Then the number of 
bits in all the bit-vectors for this index is mn. If, say, blocks are 4096 bytes 
long, then we can fit 32,768 bits in one block, so the number of blocks needed 
is m n /32768. That number can be small compared to the number of blocks 
needed to hold the file itself, but the larger m is, the more space the bitmap 
index takes.
But if m is large, then l ’s in a bit-vector will be very rare; precisely, the 
probability that any bit is 1 is 1/m. If l ’s are rare, then we have an opportunity 
to encode bit-vectors so that they take much less than n bits on the average. A 
common approach is called run-length encoding, where we represent a run, that 
is, a sequence of i 0’s followed by a 1, by some suitable binary encoding of the 
integer i. We concatenate the codes for each run together, and that sequence 
of bits is the encoding of the entire bit-vector.
We might imagine that we could just represent integer i by expressing i
as a binary number. However, that simple a scheme will not do, because it 
is not possible to break a sequence of codes apart to determine uniquely the 
lengths of the runs involved (see the box on “Binary Numbers Won’t Serve as a 
Run-Length Encoding”). Thus, the encoding of integers i that represent a run 
length must be more complex than a simple binary representation.
We shall study one of many possible schemes for encoding. There are some 
better, more complex schemes that can improve on the amount of compression 
achieved here, by almost a factor of 2, but only when typical runs are very long. 
In our scheme, we first determine how many bits the binary representation of
i has. This number j, which is approximately log2 i, is represented in “unary,” 
by j — 1 l ’s and a single 0. Then, we can follow with i in binary.9
E xam ple 14.42: If i = 13, then j = 4; that is, we need 4 bits in the binary
9A ctually, except for th e case th a t j = 1 (i.e., i = 0 or i = 1), we can be sure th a t th e 
binary representation of i begins w ith 1. T hus, we can save ab o u t one b it p e r num ber if we 
om it th is 1 and use only th e rem aining j — 1 bits.
692 CHAPTER 14. INDEX STRUCTURES
representation of i. Thus, the encoding for i begins with 1110. We follow with
i in binary, or 1101. Thus, the encoding for 13 is 11101101.
The encoding for i = 1 is 01, and the encoding for i = 0 is 00. In each 
case, j = 1, so we begin with a single 0 and follow that 0 with the one bit that 
represents i. □
If we concatenate a sequence of integer codes, we can always recover the 
sequence of run lengths and therefore recover the original bit-vector. Suppose 
we have scanned some of the encoded bits, and we are now at the beginning 
of a sequence of bits that encodes some integer i. We scan forward to the first
0, to determine the value of j. That is, j equals the number of bits we must 
scan until we get to the first 0 (including that 0 in the count of bits). Once we 
know j, we look at the next j bits; i is the integer represented there in binary. 
Moreover, once we have scanned the bits representing i, we know where the 
next code for an integer begins, so we can repeat the process.
E xam p le 1 4 .4 3 : Let us decode the sequence 11101101001011. Starting at the 
beginning, we find the first 0 at the 4th bit, so j = 4. The next 4 bits are 1101, 
so we determine that the first integer is 13. We are now left with 001011 to 
decode.
Since the first bit is 0, we know the next bit represents the next integer by 
itself; this integer is 0. Thus, we have decoded the sequence 13, 0, and we must 
decode the remaining sequence 1011.
We find the first 0 in the second position, whereupon we conclude that the 
final two bits represent the last integer, 3. Our entire sequence of run-lengths 
is thus 13, 0, 3. From these numbers, we can reconstruct the actual bit-vector, 
0000000000000110001. □
Technically, every bit-vector so decoded will end in a 1, and any trailing 0’s 
will not be recovered. Since we presumably know the number of records in the 
file, the additional 0’s can be added. However, since 0 in a bit-vector indicates 
the corresponding record is not in the described set, we don’t even have to know 
the total number of records, and can ignore the trailing 0’s.
E xam p le 1 4 .4 4 : Let us convert some of the bit-vectors from Example 14.42 
to our run-length code. The vectors for the first three ages, 25, 30, and 45, 
are 100000001000, 000000010000, and 010000000100, respectively. The first of 
these has the run-length sequence (0,7). The code for 0 is 00, and the code for 
7 is 110111. Thus, the bit-vector for age 25 becomes 00110111.
Similarly, the bit-vector for age 30 has only one run, with seven 0’s. Thus, 
its code is 110111. The bit-vector for age 45 has two runs, (1,7). Since 1 has 
the code 01, and we determined that 7 has the code 110111, the code for the 
third bit-vector is 01110111. □
The compression in Example 14.44 is not great. However, we cannot see the 
true benefits when n, the number of records, is small. To appreciate the value
14.7. BITM AP INDEXES 693
of the encoding, suppose that m — n, i.e., each value for the field on which the 
bitmap index is constructed, occurs once. Notice that the code for a run of 
length i has about 21og2 i bits. If each bit-vector has a single 1, then it has a 
single run, and the length of that run cannot be longer than n. Thus, 2 log2 n 
bits is an upper bound on the length of a bit-vector’s code in this case.
Since there are n bit-vectors in the index, the total number of bits to represent the index is at most 2n log2 n. In comparison, the uncompressed bit-vectors 
for this data would require n 2 bits.
14.7.3 Operating on Run-Length-Encoded Bit-Vectors
When we need to perform bitwise AND or OR on encoded bit-vectors, we 
have little choice but to decode them and operate on the original bit-vectors. 
However, we do not have to do the decoding all at once. The compression 
scheme we have described lets us decode one run at a time, and we can thus 
determine where the next 1 is in each operand bit-vector. If we are taking the 
OR, we can produce a 1 at that position of the output, and if we are taking the 
AND we produce a 1 if and only if both operands have their next 1 at the same 
position. The algorithms involved are complex, but an example may make the 
idea adequately clear.
E xam ple 14.45: Consider the encoded bit-vectors we obtained in Example 14.44 for ages 25 and 30: 00110111 and 110111, respectively. We can decode 
their first runs easily; we find they are 0 and 7, respectively. That is, the first
1 of the bit-vector for 25 occurs in position 1, while the first 1 in the bit-vector 
for 30 occurs at position 8. We therefore generate 1 in position 1.
Next, we must decode the next run for age 25, since that bit-vector may 
produce another 1 before age 30’s bit-vector produces a 1 at position 8. However, the next run for age 25 is 7, which says that this bit-vector next produces 
a 1 at position 9. We therefore generate six 0’s and the 1 at position 8 that 
comes from the bit-vector for age 30. The 1 at position 9 from age 25’s bitvector is produced. Neither bit-vector produces any more l ’s for the output. 
We conclude that the OR of these bit-vectors is 100000011. Technically, we 
must append 000, since uncompressed bit-vectors are of length twelve in this 
example. □
14.7.4 Managing Bitmap Indexes
We have described operations on bitmap indexes without addressing three important issues:
1. When we want to find the bit-vector for a given value, or the bit-vectors 
corresponding to values in a given range, how do we find these efficiently?
2. When we have selected a set of records that answer our query, how do we 
retrieve those records efficiently?
694 CHAPTER 14. INDEX STRUCTURES
3. When the data file changes by insertion or deletion of records, how do we 
adjust the bitmap index on a given field?
F in d in g B it-V ectors
Think of each bit-vector as a record whose key is the value corresponding to 
this bit-vector (although the value itself does not appear in this “record”). 
Then any secondary index technique will take us efficiently from values to their 
bit-vectors.
We also need to store the bit-vectors somewhere. It is best to think of them 
as variable-length records, since they will generally grow as more records are 
added to the data file. The techniques of Section 13.7 are useful.
F in d in g R ecords
Now let us consider the second question: once we have determined that we need 
record k of the data file, how do we find it? Again, techniques we have seen 
already may be adapted. Think of the fcth record as having search-key value 
k (although this key does not actually appear in the record). We may then 
create a secondary index on the data file, whose search key is the number of 
the record.
H an d lin g M od ification s to th e D a ta F ile
There are two aspects to the problem of reflecting data-file modifications in a 
bitmap index.
1. Record numbers must remain fixed once assigned.
2. Changes to the data file require the bitmap index to change as well.
The consequence of point (1) is that when we delete record i, it is easiest 
to “retire” its number. Its space is replaced by a “tombstone” in the data file. 
The bitmap index must also be changed, since the bit-vector that had a 1 in 
position i must have that 1 changed to 0. Note that we can find the appropriate 
bit-vector, since we know what value record i had before deletion.
Next consider insertion of a new record. We keep track of the next available 
record number and assign it to the new record. Then, for each bitmap index, 
we must determine the value the new record has in the corresponding field and 
modify the bit-vector for that value by appending a 1 at the end. Technically, 
all the other bit-vectors in this index get a new 0 at the end, but if we are using 
a compression technique such as that of Section 14.7.2, then no change to the 
compressed values is needed.
As a special case, the new record may have a value for the indexed field 
that has not been seen before. In that case, we need a new bit-vector for 
this value, and this bit-vector and its corresponding value need to be inserted
14.8. SUM M ARY OF CHAPTER 14 695
into the secondary-index structure that is used to find a bit-vector given its 
corresponding value.
Lastly, consider a modification to a record i of the data file that changes 
the value of a field that has a bitmap index, say from value v to value w. We 
must find the bit-vector for v and change the 1 in position i to 0. If there is a 
bit-vector for value w, then we change its 0 in position i to 1. If there is not 
yet a bit-vector for w, then we create it as discussed in the paragraph above for 
the case when an insertion introduces a new value.
14.7.5 Exercises for Section 14.7
Exercise 14.7.1: For the data of Fig. 14.36, show the bitmap indexes for 
the attributes: (a) speed (b) ram (c) hd, both in (?) uncompressed form, and 
(ii) compressed form using the scheme of Section 14.7.2.
Exercise 14.7.2: Using the bitmaps of Example 14.41, find the jewelry buyers 
with an age in the range 20-40 and a salary in the range 0-100.
Exercise 14.7.3: Consider a file of 1,000,000 records, with a field F that has 
m different values.
a) As a function of m, how many bytes does the bitmap index for F have?
! b) Suppose that the records numbered from 1 to 1,000,000 are given values 
for the field F in a round-robin fashion, so each value appears every m
records. How many bytes would be consumed by a compressed index?
E xercise 14.7.4: We suggested in Section 14.7.2 that it was possible to reduce 
the number of bits taken to encode number i from the 2 log2 i that we used in 
that section until it is close to log2 i. Show how to approach that limit as closely 
as you like, as long as i is large. Hint: We used a unary encoding of the length 
of the binary encoding that we used for i. Can you encode the length of the 
code in binary?
E xercise 14.7.5: Encode, using the scheme of Section 14.7.2, the following 
bitmaps:
a) 0110000000100000100.
b) 10000010000001001101.
c) 0001000000000010000010000.
14.8 Summary of Chapter 14
♦ Sequential Files: Several simple file organizations begin by sorting the 
data file according to some sort key and placing an index on this file.
♦ Dense and Sparse Indexes: Dense indexes have a key-pointer pair for 
every record in the data file, while sparse indexes have one key-pointer 
pair for each block of the data file.
♦ Multilevel Indexes: It is sometimes useful to put an index on the index 
file itself, an index file on that, and so on. Higher levels of index must be 
sparse.
♦ Secondary Indexes: An index on a search key K can be created even if 
the data file is not sorted by K . Such an index must be dense.
♦ Inverted Indexes: The relation between documents and the words they 
contain is often represented by an index structure with word-pointer pairs. 
The pointer goes to a place in a “bucket” file where is found a list of 
pointers to places where that word occurs.
♦ B-trees: These structures are essentially multilevel indexes, with graceful 
growth capabilities. Blocks with n keys and n + 1 pointers are organized 
in a tree, with the leaves pointing to records. All nonroot blocks are 
between half-full and completely full at all times.
♦ Hash Tables: We can create hash tables out of blocks in secondary memory, much as we can create main-memory hash tables. A hash function 
maps search-key values to buckets, effectively partitioning the records of 
a data file into many small groups (the buckets). Buckets are represented 
by a block and possible overflow blocks.
♦ Extensible Hashing: This method allows the number of buckets to double 
whenever any bucket has too many records. It uses an array of pointers 
to blocks that represent the buckets. To avoid having too many blocks, 
several buckets can be represented by the same block.
♦ Linear Hashing: This method grows the number of buckets by 1 each time 
the ratio of records to buckets exceeds a threshold. Since the population 
of a single bucket cannot cause the table to expand, overflow blocks for 
buckets are needed in some situations.
♦ Queries Needing Multidimensional Indexes: The sorts of queries that 
need to be supported on multidimensional data include partial-match (all 
points with specified values in a subset of the dimensions), range queries 
(all points within a range in each dimension), nearest-neighbor (closest 
point to a given point), and where-am-I (region or regions containing a 
given point).
♦ Executing Nearest-Neighbor Queries: Many data structures allow nearestneighbor queries to be executed by performing a range query around the 
target point, and expanding the range if there is no point in that range. 
We must be careful, because finding a point within a rectangular range 
may not rule out the possibility of a closer point outside that rectangle.
696 CHAPTER 14. INDEX STRUCTURES
14.9. REFERENCES FOR CHAPTER 14 697
♦ Grid Files: The grid file slices the space of points in each of the dimensions. The grid lines can be spaced differently, and there can be different 
numbers of lines for each dimension. Grid files support range queries, 
partial-match queries, and nearest-neighbor queries well, as long as data 
is fairly uniform in distribution.
♦ Partitioned Hash Tables: A partitioned hash function constructs some 
bits of the bucket number from each dimension. They support partialmatch queries well, and are not dependent on the data being uniformly 
distributed.
♦ Multiple-Key Indexes: A simple multidimensional structure has a root 
that is an index on one attribute, leading to a collection of indexes on a 
second attribute, which can lead to indexes on a third attribute, and so 
on. They are useful for range and nearest-neighbor queries.
♦ kd- Trees: These trees are like binary search trees, but they branch on 
different attributes at different levels. They support partial-match, range, 
and nearest-neighbor queries well. Some careful packing of tree nodes into 
blocks must be done to make the structure suitable for secondary-storage 
operations.
♦ Quad Trees: The quad tree divides a multidimensional cube into quadrants, and recursively divides the quadrants the same way if they have too 
many points. They support partial-match, range, and nearest-neighbor 
queries.
♦ R-Trees: This form of tree normally represents a collection of regions by 
grouping them into a hierarchy of larger regions. It helps with where-amI queries and, if the atomic regions are actually points, will support the 
other types of queries studied in this chapter, as well.
♦ Bitmap Indexes: Multidimensional queries are supported by a form of 
index that orders the points or records and represents the positions of the 
records with a given value in an attribute by a bit vector. These indexes 
support range, nearest-neighbor, and partial-match queries.
♦ Compressed, Bitmaps: In order to save space, the bitmap indexes, which 
tend to consist of vectors with very few l ’s, are compressed by using a 
run-length encoding.
14.9 References for Chapter 14
The B-tree was the original idea of Bayer and McCreight [2]. Unlike the B+ tree 
described here, this formulation had pointers to records at the interior nodes 
as well as at the leaves. [8] is a survey of B-tree varieties.
698 CHAPTER 14. INDEX STRUCTURES
Hashing as a data structure goes back to Peterson [19]. Extensible hashing 
was developed by [9], while linear hashing is from [15]. The book by Knuth 
